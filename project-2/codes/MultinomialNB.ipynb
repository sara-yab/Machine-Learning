{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXEl2ep9cMy6",
    "outputId": "0a1f7d42-5074-42e7-d0a0-fd9a12747f16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh4Rncx_cQho",
    "outputId": "fd67a877-5f50-4e33-e3cb-48982f47c27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el1eg8tdcTVL"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuT3IHQ_cpoG"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZcGyPMzcU1c"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pah6DSKHe7xc"
   },
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "   translator = str.maketrans('', '', string.punctuation)\n",
    "   text = text.translate(translator)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tii_GGycMny5",
    "outputId": "20f61af5-8c7e-459b-8a24-46de25c9342b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there /u/LakotaPride! Welcome to /r/Trump.  [](/sp)    \n",
      " \n",
      "Thank you for posting on r/Trump Please follow all rules and guidelines. Inform the mods if you have any concerns. [](/sp) Join our live [discord](https://discord.gg/kh4Wv9DavE) chat to talk to your fellow patriots! If you have any issues please reach out.\n",
      "\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/trump) if you have any questions or concerns.*\n"
     ]
    }
   ],
   "source": [
    "print(train_x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pKmqofAmZUw"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADnjdUOU36o_"
   },
   "outputs": [],
   "source": [
    "def print_best_params(grid):\n",
    "  bestParameters = grid.best_estimator_.get_params()\n",
    "  # print(bestParameters)\n",
    "  for paramName in sorted(bestParameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (paramName, bestParameters[paramName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK-qvvi_Oa26"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MkCaFiKv0ys"
   },
   "outputs": [],
   "source": [
    "#stemmer lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUIZVtDXOHoj"
   },
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec3yRFyGOpZo",
    "outputId": "19dc2e0c-aa59-407c-d761-d5d2e7e2ef81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "The best accuracy is 88.438.\n",
      "The winning parameters are {}\n",
      "Run time: 0.5878884792327881 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training => 88.438\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"classify\", MultinomialNB())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrcelJdFxN4B",
    "outputId": "4d551c0b-a50f-4d22-9470-c49c85344185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The best accuracy is 88.438.\n",
      "The winning parameters are {'vect__binary': False, 'vect__preprocessor': None}\n",
      "Run time: 4.767054080963135 seconds\n"
     ]
    }
   ],
   "source": [
    "#removing punctuation  => not good\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "   'vect__preprocessor': [preprocess_text,remove_punctuation,None],\n",
    "    \"vect__binary\": [False,True]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"clf\", MultinomialNB())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkn0wZqG0Mqu"
   },
   "outputs": [],
   "source": [
    "train_x_punc = train_x.copy()\n",
    "\n",
    "for i in range(train_x.shape[0]):\n",
    "  train_x_punc[i]= train_x_punc[i].translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtc20jwc0VSx",
    "outputId": "3ec7a800-fe52-463d-9749-802ba45b67f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "The best accuracy is 87.185.\n",
      "The winning parameters are {}\n",
      "Run time: 0.49113988876342773 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training,train_x_punc => worse\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"classify\", MultinomialNB())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x_punc, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUZbTJ18096p",
    "outputId": "ec2bd22d-01c3-4f81-a0d2-7fcfa2c3b5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best accuracy is 90.809.\n",
      "The winning parameters are {'vect__binary': False, 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 1.4538230895996094 seconds\n",
      "\tclf: MultinomialNB()\n",
      "\tclf__alpha: 1.0\n",
      "\tclf__class_prior: None\n",
      "\tclf__fit_prior: True\n",
      "\tclf__force_alpha: 'warn'\n",
      "\tmemory: None\n",
      "\tsteps: [('vect', CountVectorizer(stop_words=['against', 'find', \"shan't\", 'i', 't', 'whence',\n",
      "                            'go', 'ten', 'she', 'somewhere', 'others',\n",
      "                            'throughout', \"don't\", 'serious', 'whereafter',\n",
      "                            'own', 'whole', 'should', 'eg', 'his', 'toward',\n",
      "                            'whether', 'wherever', 'give', 'its', 'noone', 'is',\n",
      "                            'were', \"needn't\", 'though', ...])), ('clf', MultinomialNB())]\n",
      "\tvect: CountVectorizer(stop_words=['against', 'find', \"shan't\", 'i', 't', 'whence',\n",
      "                            'go', 'ten', 'she', 'somewhere', 'others',\n",
      "                            'throughout', \"don't\", 'serious', 'whereafter',\n",
      "                            'own', 'whole', 'should', 'eg', 'his', 'toward',\n",
      "                            'whether', 'wherever', 'give', 'its', 'noone', 'is',\n",
      "                            'were', \"needn't\", 'though', ...])\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__binary: False\n",
      "\tvect__decode_error: 'strict'\n",
      "\tvect__dtype: <class 'numpy.int64'>\n",
      "\tvect__encoding: 'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__preprocessor: None\n",
      "\tvect__stop_words: ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: '(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\tverbose: False\n"
     ]
    }
   ],
   "source": [
    "#stop words => stop_words_library wins 90.809.\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk),list(stop_words_sklearn),list(stop_words_library),None]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"clf\", MultinomialNB())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x_punc, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "print_best_params(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIJiO_hG2mjV",
    "outputId": "532760e1-c0b4-42bd-b11e-0a039f9e8001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 92.479.\n",
      "The winning parameters are {'clf__alpha': 0.1, 'vect__binary': False, 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 2.2182962894439697 seconds\n"
     ]
    }
   ],
   "source": [
    "# test alpha => 92.479. , alpha = 0.1\n",
    "#selected 3\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"clf__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "le7kwDcO4jr_",
    "outputId": "2078b7c8-cf81-46c9-a5a1-52c853469138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "The best accuracy is 90.669.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'selecter__k': 5000, 'vect__binary': False, 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 12.57723093032837 seconds\n"
     ]
    }
   ],
   "source": [
    "# test selector = > decreased(90.669.)\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"clf__alpha\" : [0.001, 0.01, 0.1,0.02,0.5],\n",
    "    \"selecter__k\":[5000,3000,6000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x_punc, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gzydUK4afym",
    "outputId": "2a570ab7-5e3d-4866-bf7e-9292eb5d4efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 91.779.\n",
      "The winning parameters are {'clf__alpha': 0.01, 'normalizer__norm': 'l1', 'vect__binary': False, 'vect__stop_words': ['thick', 'thru', 'cant', 'below', 'ma', 'becomes', \"you've\", 'thus', 'fire', 'somewhere', 'latter', 'after', 'much', 'put', 'sometimes', 's', 'see', \"aren't\", 'seem', 'interest', 'if', 'elsewhere', 'over', 'less', \"won't\", 'ours', 'ain', 't', 'under', 'anyway', 'whoever', 'ourselves', 'hence', 'not', 'd', 'become', 've', 'should', 'no', 'toward', 'i', 'besides', 'therein', 'something', 'beforehand', 'out', \"shan't\", 'or', 'through', 'why', 'inc', 'upon', 'last', 'few', 'perhaps', 'one', 'found', 'themselves', 'find', 'again', 'now', 'while', 'same', 'doesn', 'who', 'with', 'formerly', 'eg', 'already', 'side', 'isn', 'don', 'which', \"you're\", 'give', 'is', 'however', 'couldnt', 'indeed', 'cry', 'nine', 'in', 'someone', 'many', 'whereby', 'before', 'further', 'the', 'whereas', 'often', 'amoungst', 'latterly', \"shouldn't\", 'they', 'meanwhile', 'our', 'twenty', 'herself', 'once', 'always', 'done', 'namely', 'against', 'wherein', 'still', 'wasn', 'etc', 'his', 'though', 'other', 'all', 'up', 'get', 'herein', 'can', \"weren't\", 'others', 'because', 'along', 'whole', 'former', 'its', \"mightn't\", 'keep', 'has', 'as', 'how', 'these', 'shouldn', 'me', 'wouldn', 'what', \"you'll\", 'seemed', 'within', 'those', 'hasn', 'every', 'hasnt', 'hundred', 'since', 'of', 'didn', \"she's\", 'via', 'here', 'per', 'otherwise', 'wherever', \"you'd\", 'whereupon', 'haven', 'never', 'anything', 'empty', 'seems', 'might', 'just', 'next', 'ltd', 'to', 'y', 'couldn', 'hadn', 'by', 'nowhere', 'among', \"mustn't\", 'seeming', 'it', 'call', 'theirs', 'each', 'behind', 'everything', 'amount', 'de', 'down', 'did', 'alone', \"don't\", 'about', 'sometime', 'an', 'also', 'will', 'weren', \"doesn't\", 'sincere', 'whither', 'whenever', 'thence', 'mostly', 'hereby', 'serious', 'twelve', 'doing', 'bill', 'ie', 'made', 'together', 'when', 'eight', 'thereafter', 'third', 'am', 'well', 'll', 'detail', \"couldn't\", 'either', 'won', 'where', 'very', 'been', 'she', 'was', 'this', 'front', 'therefore', 'sixty', 'whence', 'beyond', 'were', 'several', 'amongst', 'o', 'three', 'throughout', 're', 'into', 'he', 'shan', 'mustn', 'needn', 'own', 'do', 'anyone', 'first', 'almost', 'due', 'system', 'than', 'con', 'fifteen', 'eleven', 'enough', \"needn't\", 'mightn', 'most', 'more', 'are', 'everywhere', 'thin', 'that', 'yourselves', 'them', 'fill', 'nothing', 'having', 'at', \"didn't\", 'may', 'on', 'top', 'became', 'you', 'any', 'take', 'their', 'during', 'only', 'neither', 'whatever', 'us', 'none', 'have', 'both', 'hereupon', 'five', 'cannot', 'mill', 'although', 'co', 'from', 'somehow', 'moreover', 'onto', 'm', 'nevertheless', 'some', 'please', 'too', 'and', 'except', 'even', 'go', 'himself', 'yourself', 'hers', 'bottom', 'un', 'whether', 'another', 'around', \"haven't\", 'nor', 'such', \"hasn't\", 'beside', 'whose', 'then', 'two', 'being', 'aren', 'had', 'full', 'whom', 'ten', 'hereafter', 'could', 'there', 'else', 'rather', 'him', 'itself', 'her', 'your', 'thereupon', 'my', 'mine', 'move', 'but', 'ever', 'describe', 'show', 'afterwards', 'noone', 'six', 'thereby', 'we', 'be', \"isn't\", 'name', 'would', 'a', \"it's\", 'anywhere', 'anyhow', 'for', 'towards', \"wasn't\", 'so', 'off', 'yours', 'four', 'without', 'becoming', 'whereafter', \"that'll\", 'across', 'everyone', 'fifty', 'myself', 'yet', 'until', 'part', 'least', 'nobody', 'must', 'between', \"should've\", 'above', \"hadn't\", \"wouldn't\", 'back', 'does', 'forty']}\n",
      "Run time: 2.6477572917938232 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing normalizer , without : 92.479, with:92.199., 92.34. with norm max => no normalizer\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"clf__alpha\" : [0.001, 0.01, 0.1,0.02,0.5],\n",
    "    'normalizer__norm': ['l1','l2','max']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"normalizer\", normalizer),(\"clf\", MultinomialNB())])\n",
    "#pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuTN6ZiC6i0H",
    "outputId": "18ee575d-6ecf-445e-8456-62adc9bbf161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 89.28.\n",
      "The winning parameters are {'clf__alpha': 0.001, 'selecter__k': 3000, 'vect__binary': False, 'vect__stop_words': ['thick', 'thru', 'cant', 'below', 'ma', 'becomes', \"you've\", 'thus', 'fire', 'somewhere', 'latter', 'after', 'much', 'put', 'sometimes', 's', 'see', \"aren't\", 'seem', 'interest', 'if', 'elsewhere', 'over', 'less', \"won't\", 'ours', 'ain', 't', 'under', 'anyway', 'whoever', 'ourselves', 'hence', 'not', 'd', 'become', 've', 'should', 'no', 'toward', 'i', 'besides', 'therein', 'something', 'beforehand', 'out', \"shan't\", 'or', 'through', 'why', 'inc', 'upon', 'last', 'few', 'perhaps', 'one', 'found', 'themselves', 'find', 'again', 'now', 'while', 'same', 'doesn', 'who', 'with', 'formerly', 'eg', 'already', 'side', 'isn', 'don', 'which', \"you're\", 'give', 'is', 'however', 'couldnt', 'indeed', 'cry', 'nine', 'in', 'someone', 'many', 'whereby', 'before', 'further', 'the', 'whereas', 'often', 'amoungst', 'latterly', \"shouldn't\", 'they', 'meanwhile', 'our', 'twenty', 'herself', 'once', 'always', 'done', 'namely', 'against', 'wherein', 'still', 'wasn', 'etc', 'his', 'though', 'other', 'all', 'up', 'get', 'herein', 'can', \"weren't\", 'others', 'because', 'along', 'whole', 'former', 'its', \"mightn't\", 'keep', 'has', 'as', 'how', 'these', 'shouldn', 'me', 'wouldn', 'what', \"you'll\", 'seemed', 'within', 'those', 'hasn', 'every', 'hasnt', 'hundred', 'since', 'of', 'didn', \"she's\", 'via', 'here', 'per', 'otherwise', 'wherever', \"you'd\", 'whereupon', 'haven', 'never', 'anything', 'empty', 'seems', 'might', 'just', 'next', 'ltd', 'to', 'y', 'couldn', 'hadn', 'by', 'nowhere', 'among', \"mustn't\", 'seeming', 'it', 'call', 'theirs', 'each', 'behind', 'everything', 'amount', 'de', 'down', 'did', 'alone', \"don't\", 'about', 'sometime', 'an', 'also', 'will', 'weren', \"doesn't\", 'sincere', 'whither', 'whenever', 'thence', 'mostly', 'hereby', 'serious', 'twelve', 'doing', 'bill', 'ie', 'made', 'together', 'when', 'eight', 'thereafter', 'third', 'am', 'well', 'll', 'detail', \"couldn't\", 'either', 'won', 'where', 'very', 'been', 'she', 'was', 'this', 'front', 'therefore', 'sixty', 'whence', 'beyond', 'were', 'several', 'amongst', 'o', 'three', 'throughout', 're', 'into', 'he', 'shan', 'mustn', 'needn', 'own', 'do', 'anyone', 'first', 'almost', 'due', 'system', 'than', 'con', 'fifteen', 'eleven', 'enough', \"needn't\", 'mightn', 'most', 'more', 'are', 'everywhere', 'thin', 'that', 'yourselves', 'them', 'fill', 'nothing', 'having', 'at', \"didn't\", 'may', 'on', 'top', 'became', 'you', 'any', 'take', 'their', 'during', 'only', 'neither', 'whatever', 'us', 'none', 'have', 'both', 'hereupon', 'five', 'cannot', 'mill', 'although', 'co', 'from', 'somehow', 'moreover', 'onto', 'm', 'nevertheless', 'some', 'please', 'too', 'and', 'except', 'even', 'go', 'himself', 'yourself', 'hers', 'bottom', 'un', 'whether', 'another', 'around', \"haven't\", 'nor', 'such', \"hasn't\", 'beside', 'whose', 'then', 'two', 'being', 'aren', 'had', 'full', 'whom', 'ten', 'hereafter', 'could', 'there', 'else', 'rather', 'him', 'itself', 'her', 'your', 'thereupon', 'my', 'mine', 'move', 'but', 'ever', 'describe', 'show', 'afterwards', 'noone', 'six', 'thereby', 'we', 'be', \"isn't\", 'name', 'would', 'a', \"it's\", 'anywhere', 'anyhow', 'for', 'towards', \"wasn't\", 'so', 'off', 'yours', 'four', 'without', 'becoming', 'whereafter', \"that'll\", 'across', 'everyone', 'fifty', 'myself', 'yet', 'until', 'part', 'least', 'nobody', 'must', 'between', \"should've\", 'above', \"hadn't\", \"wouldn't\", 'back', 'does', 'forty'], 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f4c16e05460>}\n",
      "Run time: 66.79015469551086 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing lemma,stemmizer => not working\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"vect__tokenizer\": [LemmaTokenizer_word()],\n",
    "    'selecter__k':[5000,3000],\n",
    "    \"clf__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiMh3bszbewm",
    "outputId": "55e204c0-11a3-4beb-fb96-aaa4bdf22697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "The best accuracy is 93.176.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'vect__binary': False, 'vect__ngram_range': (1, 2), 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 17.149862051010132 seconds\n"
     ]
    }
   ],
   "source": [
    "#test ngram() ,best is 92.47 , 93.176. with ngram(1,2)\n",
    "#selected 1\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    'vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "    \"clf__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oseo-53bdmsd",
    "outputId": "16c2d390-0221-4d34-f003-96e6bd4adf9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "The best accuracy is 92.058.\n",
      "The winning parameters are {'clf__alpha': 0.01, 'selecter__k': 5000, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 9.161921262741089 seconds\n"
     ]
    }
   ],
   "source": [
    "#test CountVectorizer =>93.176\n",
    "#TfidfVectorizer with (1,1) ngram and selector chi2 =>92.058.\n",
    "#selected 2\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    \"clf__alpha\" : [0.01, 0.1,0.02,0.5],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "normalizer = Normalizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"normalizer\", normalizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7W_xnTpXhZv9",
    "outputId": "c0711b55-dbfc-413e-de0a-14b4d265f22f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "The best accuracy is 93.176.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'vect__binary': False, 'vect__ngram_range': (1, 2), 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 1.3021674156188965 seconds\n"
     ]
    }
   ],
   "source": [
    "#confirm 93.1\n",
    "#same as selected 1\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    'vect__ngram_range':[(1,2)],\n",
    "    \"clf__alpha\" : [0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Dir8G84EwiL",
    "outputId": "21b5ec2d-1cbb-4fb1-a484-6e1291b17f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "The best accuracy is 90.807.\n",
      "The winning parameters are {'clf__alpha': 0.02, 'selecter__k': 5000, 'selecter__score_func': <function mutual_info_classif at 0x7f4c2cdaf550>, 'vect__binary': False, 'vect__ngram_range': (1, 2), 'vect__stop_words': ['thick', 'thru', 'cant', 'below', 'ma', 'becomes', \"you've\", 'thus', 'fire', 'somewhere', 'latter', 'after', 'much', 'put', 'sometimes', 's', 'see', \"aren't\", 'seem', 'interest', 'if', 'elsewhere', 'over', 'less', \"won't\", 'ours', 'ain', 't', 'under', 'anyway', 'whoever', 'ourselves', 'hence', 'not', 'd', 'become', 've', 'should', 'no', 'toward', 'i', 'besides', 'therein', 'something', 'beforehand', 'out', \"shan't\", 'or', 'through', 'why', 'inc', 'upon', 'last', 'few', 'perhaps', 'one', 'found', 'themselves', 'find', 'again', 'now', 'while', 'same', 'doesn', 'who', 'with', 'formerly', 'eg', 'already', 'side', 'isn', 'don', 'which', \"you're\", 'give', 'is', 'however', 'couldnt', 'indeed', 'cry', 'nine', 'in', 'someone', 'many', 'whereby', 'before', 'further', 'the', 'whereas', 'often', 'amoungst', 'latterly', \"shouldn't\", 'they', 'meanwhile', 'our', 'twenty', 'herself', 'once', 'always', 'done', 'namely', 'against', 'wherein', 'still', 'wasn', 'etc', 'his', 'though', 'other', 'all', 'up', 'get', 'herein', 'can', \"weren't\", 'others', 'because', 'along', 'whole', 'former', 'its', \"mightn't\", 'keep', 'has', 'as', 'how', 'these', 'shouldn', 'me', 'wouldn', 'what', \"you'll\", 'seemed', 'within', 'those', 'hasn', 'every', 'hasnt', 'hundred', 'since', 'of', 'didn', \"she's\", 'via', 'here', 'per', 'otherwise', 'wherever', \"you'd\", 'whereupon', 'haven', 'never', 'anything', 'empty', 'seems', 'might', 'just', 'next', 'ltd', 'to', 'y', 'couldn', 'hadn', 'by', 'nowhere', 'among', \"mustn't\", 'seeming', 'it', 'call', 'theirs', 'each', 'behind', 'everything', 'amount', 'de', 'down', 'did', 'alone', \"don't\", 'about', 'sometime', 'an', 'also', 'will', 'weren', \"doesn't\", 'sincere', 'whither', 'whenever', 'thence', 'mostly', 'hereby', 'serious', 'twelve', 'doing', 'bill', 'ie', 'made', 'together', 'when', 'eight', 'thereafter', 'third', 'am', 'well', 'll', 'detail', \"couldn't\", 'either', 'won', 'where', 'very', 'been', 'she', 'was', 'this', 'front', 'therefore', 'sixty', 'whence', 'beyond', 'were', 'several', 'amongst', 'o', 'three', 'throughout', 're', 'into', 'he', 'shan', 'mustn', 'needn', 'own', 'do', 'anyone', 'first', 'almost', 'due', 'system', 'than', 'con', 'fifteen', 'eleven', 'enough', \"needn't\", 'mightn', 'most', 'more', 'are', 'everywhere', 'thin', 'that', 'yourselves', 'them', 'fill', 'nothing', 'having', 'at', \"didn't\", 'may', 'on', 'top', 'became', 'you', 'any', 'take', 'their', 'during', 'only', 'neither', 'whatever', 'us', 'none', 'have', 'both', 'hereupon', 'five', 'cannot', 'mill', 'although', 'co', 'from', 'somehow', 'moreover', 'onto', 'm', 'nevertheless', 'some', 'please', 'too', 'and', 'except', 'even', 'go', 'himself', 'yourself', 'hers', 'bottom', 'un', 'whether', 'another', 'around', \"haven't\", 'nor', 'such', \"hasn't\", 'beside', 'whose', 'then', 'two', 'being', 'aren', 'had', 'full', 'whom', 'ten', 'hereafter', 'could', 'there', 'else', 'rather', 'him', 'itself', 'her', 'your', 'thereupon', 'my', 'mine', 'move', 'but', 'ever', 'describe', 'show', 'afterwards', 'noone', 'six', 'thereby', 'we', 'be', \"isn't\", 'name', 'would', 'a', \"it's\", 'anywhere', 'anyhow', 'for', 'towards', \"wasn't\", 'so', 'off', 'yours', 'four', 'without', 'becoming', 'whereafter', \"that'll\", 'across', 'everyone', 'fifty', 'myself', 'yet', 'until', 'part', 'least', 'nobody', 'must', 'between', \"should've\", 'above', \"hadn't\", \"wouldn't\", 'back', 'does', 'forty']}\n",
      "Run time: 1804.328807592392 seconds\n"
     ]
    }
   ],
   "source": [
    "#test selector\n",
    "#[chi2, f_classif, mutual_info_classif, f_regression, mutual_info_regression]\n",
    "#fclassic : 91.225. chi2: 91.084\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    'vect__ngram_range':[(1,2)],\n",
    "    \"selecter__score_func\": [mutual_info_classif],\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"clf__alpha\" : [0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest()\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6cfMOmNkNrv",
    "outputId": "6b7523ad-01df-4340-b8e9-70b3148b84e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "The best accuracy is 92.058.\n",
      "The winning parameters are {'clf__alpha': 0.01, 'clf__fit_prior': True, 'selecter__k': 5000, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 3.5223331451416016 seconds\n"
     ]
    }
   ],
   "source": [
    "#test fit prior => does not improve\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    \"clf__alpha\" : [0.01],\n",
    "    \"clf__fit_prior\" : [True,False],\n",
    "    'selecter__k':[5000]\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "normalizer = Normalizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"normalizer\", normalizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7j9yExaajWtD",
    "outputId": "2c60933a-29be-4da9-c0bb-17c5947a16e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best accuracy is 93.176.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'vect__binary': False, 'vect__ngram_range': (1, 2), 'vect__preprocessor': None, 'vect__stop_words': ['against', 'find', \"shan't\", 'i', 't', 'whence', 'go', 'ten', 'she', 'somewhere', 'others', 'throughout', \"don't\", 'serious', 'whereafter', 'own', 'whole', 'should', 'eg', 'his', 'toward', 'whether', 'wherever', 'give', 'its', 'noone', 'is', 'were', \"needn't\", 'though', 'therein', 'afterwards', 'everywhere', \"doesn't\", 'ourselves', \"you'll\", 'found', 'isn', 'into', \"hadn't\", 'once', 'are', 'to', 'as', 'down', 'can', 'three', 'don', \"wasn't\", 'twenty', 'yourselves', 'please', 'often', 'ie', 'an', 'one', 'forty', 'within', 'didn', 'side', 'mightn', 'while', 'sometime', 'hadn', 'all', 'only', \"didn't\", 'anyone', 'becoming', 'the', 'bottom', 'from', 'almost', 'still', 'describe', 'about', 'anyway', 'd', 'may', 'six', \"that'll\", 'everything', 'take', 'back', 'for', \"isn't\", 'mostly', 'eleven', 'whoever', 'whereas', 'moreover', 'why', 'otherwise', 'thus', \"she's\", 'whose', 'if', 'therefore', 'yet', 'become', 'even', 'five', 'first', 'in', 'something', 'together', 'inc', 'further', 'fill', 'elsewhere', 'very', 'whom', 'each', 'beside', 'some', 'have', 'con', 'latterly', 'themselves', 'hereupon', \"it's\", 'third', 'upon', 'seems', 'll', 'along', 'itself', 'indeed', 'seem', 'that', 'across', 'will', 'already', 'seemed', 'least', 'becomes', 'show', 'been', \"aren't\", 'couldnt', 'mill', 'it', 'except', 'because', 'nowhere', 'by', 'empty', 'out', 'but', 'after', 'beforehand', 'thereby', 'although', 'full', \"haven't\", 'latter', 'four', 'then', 'hence', 'her', 'see', 'could', 'you', 'these', 'none', 'thereupon', 'hereafter', 'per', 'shouldn', 'how', 'thence', 'was', 'those', 'nothing', 'perhaps', 'mustn', 'hers', 'doesn', 'there', 'nine', 'ma', 'whither', 'this', 'anyhow', 'interest', 'be', 'o', 'too', 'front', 'less', 'due', 'call', 'rather', 'just', 'without', 'name', 'everyone', 'being', 'over', 'when', 'him', \"mightn't\", 's', 'amongst', 'amoungst', 'more', 'does', 'formerly', 'de', 'now', 'made', 'hundred', 'below', \"you'd\", 'through', 'anywhere', 'sincere', 'of', 'meanwhile', 'thin', 'behind', 'whenever', 'wasn', 'nor', 'until', 'among', 'so', 'yours', 'whereby', 'such', \"shouldn't\", 'sometimes', 'what', 'thru', 'much', 'same', 'must', 'again', 'a', 'am', 'off', 'never', 'ain', 'they', 'herself', 'etc', 'wouldn', 'thereafter', 'few', \"you've\", 'amount', 'namely', 'get', 'yourself', 'besides', 'my', 'than', 'alone', 'couldn', 'might', 'their', 'two', 'between', \"won't\", 'most', 'them', \"weren't\", 'herein', 'and', 'part', 'nevertheless', 'where', 'co', 'another', 'cant', 'bill', 'other', 'fire', 'several', 'did', 'no', 'up', 'cry', \"should've\", 'do', 'beyond', 'needn', 'neither', 'next', 'always', 'mine', 'put', 'wherein', 'hasn', \"couldn't\", 'onto', \"you're\", \"hasn't\", 'during', 'however', 'aren', 'thick', 'also', 'm', 'move', 'before', 'doing', 'un', 'which', 'with', 'keep', 'whereupon', 'anything', 'cannot', 'system', 'us', 'done', 'both', \"wouldn't\", 'here', 'ever', 'enough', 've', \"mustn't\", 'towards', 'having', 'either', 'hasnt', 'who', 'under', 'fifty', 'haven', 'fifteen', 'eight', 'me', 'former', 'he', 'hereby', 'became', 'or', 'top', 'any', 're', 'has', 'we', 'seeming', 'someone', 'ours', 'else', 'myself', 'above', 'since', 'had', 'our', 'your', 'not', 'would', 'many', 'around', 'detail', 'on', 'sixty', 'somehow', 'at', 'nobody', 'via', 'y', 'shan', 'twelve', 'theirs', 'last', 'ltd', 'every', 'himself', 'whatever', 'won', 'well', 'weren']}\n",
      "Run time: 4.155819654464722 seconds\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "#final test before selecting 93.17\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    'vect__preprocessor': [preprocess_text,remove_punctuation,None],\n",
    "    'vect__ngram_range':[(1,2)],\n",
    "    \"clf__alpha\" : [0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "y_pred = grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"MultinomialNB_93.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRQDKSMlRO9z",
    "outputId": "82d33bb8-826d-4910-f70a-55bbe6176e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length custom: 589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words_custom = [\n",
    "# All pronouns and associated words\n",
    "\"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\"you'll\",\"you'd\",\"you're\",\"you've\",\"yourself\",\"he\",\"he'll\",\n",
    "\"he'd\",\n",
    "\"he's\",\n",
    "\"him\",\n",
    "\"she\",\n",
    "\"she'll\",\n",
    "\"she'd\",\n",
    "\"she's\",\n",
    "\"her\",\n",
    "\"it\",\n",
    "\"it'll\",\n",
    "\"it'd\",\n",
    "\"it's\",\n",
    "\"itself\",\n",
    "\"oneself\",\n",
    "\"we\",\n",
    "\"we'll\",\n",
    "\"we'd\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"us\",\n",
    "\"ourselves\",\n",
    "\"they\",\n",
    "\"they'll\",\n",
    "\"they'd\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"them\",\n",
    "\"themselves\",\n",
    "\"everyone\",\n",
    "\"everyone's\",\n",
    "\"everybody\",\n",
    "\"everybody's\",\n",
    "\"someone\",\n",
    "\"someone's\",\n",
    "\"somebody\",\n",
    "\"somebody's\",\n",
    "\"nobody\",\n",
    "\"nobody's\",\n",
    "\"anyone\",\n",
    "\"anyone's\",\n",
    "\"everything\",\n",
    "\"everything's\",\n",
    "\"something\",\n",
    "\"something's\",\n",
    "\"nothing\",\n",
    "\"nothing's\",\n",
    "\"anything\",\n",
    "\"anything's\",\n",
    "# All determiners and associated words\n",
    "\"a\",\n",
    "\"an\",\n",
    "\"the\",\n",
    "\"this\",\n",
    "\"that\",\n",
    "\"that's\",\n",
    "\"these\",\n",
    "\"those\",\n",
    "\"my\",\n",
    "#\"mine\",   #Omitted since mine can refer to something else\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"his\",\n",
    "\"hers\",\n",
    "\"its\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"own\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"few\",\n",
    "\"much\",\n",
    "\"many\",\n",
    "\"lot\",\n",
    "\"lots\",\n",
    "\"some\",\n",
    "\"any\",\n",
    "\"enough\",\n",
    "\"all\",\n",
    "\"both\",\n",
    "\"half\",\n",
    "\"either\",\n",
    "\"neither\",\n",
    "\"each\",\n",
    "\"every\",\n",
    "\"certain\",\n",
    "\"other\",\n",
    "\"another\",\n",
    "\"such\",\n",
    "\"several\",\n",
    "\"multiple\",\n",
    "# \"what\",#Dealt with later on\n",
    "\"rather\",\n",
    "\"quite\",\n",
    "# All prepositions\n",
    "\"aboard\",\n",
    "\"about\",\n",
    "\"above\",\n",
    "\"across\",\n",
    "\"after\",\n",
    "\"against\",\n",
    "\"along\",\n",
    "\"amid\",\n",
    "\"amidst\",\n",
    "\"among\",\n",
    "\"amongst\",\n",
    "\"anti\",\n",
    "\"around\",\n",
    "\"as\",\n",
    "\"at\",\n",
    "\"away\",\n",
    "\"before\",\n",
    "\"behind\",\n",
    "\"below\",\n",
    "\"beneath\",\n",
    "\"beside\",\n",
    "\"besides\",\n",
    "\"between\",\n",
    "\"beyond\",\n",
    "\"but\",\n",
    "\"by\",\n",
    "\"concerning\",\n",
    "\"considering\",\n",
    "\"despite\",\n",
    "\"down\",\n",
    "\"during\",\n",
    "\"except\",\n",
    "\"excepting\",\n",
    "\"excluding\",\n",
    "\"far\",\n",
    "\"following\",\n",
    "\"for\",\n",
    "\"from\",\n",
    "\"here\",\n",
    "\"here's\",\n",
    "\"in\",\n",
    "\"inside\",\n",
    "\"into\",\n",
    "\"left\",\n",
    "\"like\",\n",
    "\"minus\",\n",
    "\"near\",\n",
    "\"of\",\n",
    "\"off\",\n",
    "\"on\",\n",
    "\"onto\",\n",
    "\"opposite\",\n",
    "\"out\",\n",
    "\"outside\",\n",
    "\"over\",\n",
    "\"past\",\n",
    "\"per\",\n",
    "\"plus\",\n",
    "\"regarding\",\n",
    "\"right\",\n",
    "#\"round\",   #Omitted\n",
    "#\"save\",#Omitted\n",
    "\"since\",\n",
    "\"than\",\n",
    "\"there\",\n",
    "\"there's\",\n",
    "\"through\",\n",
    "\"to\",\n",
    "\"toward\",\n",
    "\"towards\",\n",
    "\"under\",\n",
    "\"underneath\",\n",
    "\"unlike\",\n",
    "\"until\",\n",
    "\"up\",\n",
    "\"upon\",\n",
    "\"versus\",\n",
    "\"via\",\n",
    "\"with\",\n",
    "\"within\",\n",
    "\"without\",\n",
    "# Irrelevant verbs\n",
    "\"may\",\n",
    "\"might\",\n",
    "\"will\",\n",
    "\"won't\",\n",
    "\"would\",\n",
    "\"wouldn't\",\n",
    "\"can\",\n",
    "\"can't\",\n",
    "\"cannot\",\n",
    "\"could\",\n",
    "\"couldn't\",\n",
    "\"should\",\n",
    "\"shouldn't\",\n",
    "\"must\",\n",
    "\"must've\",\n",
    "\"be\",\n",
    "\"being\",\n",
    "\"been\",\n",
    "\"am\",\n",
    "\"are\",\n",
    "\"aren't\",\n",
    "\"ain't\",\n",
    "\"is\",\n",
    "\"isn't\",\n",
    "\"was\",\n",
    "\"wasn't\",\n",
    "\"were\",\n",
    "\"weren't\",\n",
    "\"do\",\n",
    "\"doing\",\n",
    "\"don't\",\n",
    "\"does\",\n",
    "\"doesn't\",\n",
    "\"did\",\n",
    "\"didn't\",\n",
    "\"done\",\n",
    "\"have\",\n",
    "\"haven't\",\n",
    "\"having\",\n",
    "\"has\",\n",
    "\"hasn't\",\n",
    "\"had\",\n",
    "\"hadn't\",\n",
    "\"get\",\n",
    "\"getting\",\n",
    "\"gets\",\n",
    "\"got\",\n",
    "\"gotten\",\n",
    "\"go\",\n",
    "\"going\",\n",
    "\"gonna\",\n",
    "\"goes\",\n",
    "\"went\",\n",
    "\"gone\",\n",
    "\"make\",\n",
    "\"making\",\n",
    "\"makes\",\n",
    "\"made\",\n",
    "\"take\",\n",
    "\"taking\",\n",
    "\"takes\",\n",
    "\"took\",\n",
    "\"taken\",\n",
    "\"need\",\n",
    "\"needing\",\n",
    "\"needs\",\n",
    "\"needed\",\n",
    "\"use\",\n",
    "\"using\",\n",
    "\"uses\",\n",
    "\"used\",\n",
    "\"want\",\n",
    "\"wanna\",\n",
    "\"wanting\",\n",
    "\"wants\",\n",
    "\"let\",\n",
    "\"lets\",\n",
    "\"letting\",\n",
    "\"let's\",\n",
    "\"suppose\",\n",
    "\"supposing\",\n",
    "\"supposes\",\n",
    "\"supposed\",\n",
    "\"seem\",\n",
    "\"seeming\",\n",
    "\"seems\",\n",
    "\"seemed\",\n",
    "\"say\",\n",
    "\"saying\",\n",
    "\"says\",\n",
    "\"said\",\n",
    "\"know\",\n",
    "\"knowing\",\n",
    "\"knows\",\n",
    "\"knew\",\n",
    "\"known\",\n",
    "\"look\",\n",
    "\"looking\",\n",
    "\"looked\",\n",
    "\"think\",\n",
    "\"thinking\",\n",
    "\"thinks\",\n",
    "\"thought\",\n",
    "\"feel\",\n",
    "\"feels\",\n",
    "\"felt\",\n",
    "\"based\",\n",
    "\"put\",\n",
    "\"puts\",\n",
    "#\"wanted\"   #Omitted since the advective is relevant\n",
    "# Question words and associated words\n",
    "\"who\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"who'd\",\n",
    "\"whoever\",\n",
    "\"whoever's\",\n",
    "\"whom\",\n",
    "\"whomever\",\n",
    "\"whomever's\",\n",
    "\"whose\",\n",
    "\"whosever\",\n",
    "\"whosever's\",\n",
    "\"when\",\n",
    "\"whenever\",\n",
    "\"which\",\n",
    "\"whichever\",\n",
    "\"where\",\n",
    "\"where's\",\n",
    "\"where'd\",\n",
    "\"wherever\",\n",
    "\"why\",\n",
    "\"why's\",\n",
    "\"why'd\",\n",
    "\"whyever\",\n",
    "\"what\",\n",
    "\"what's\",\n",
    "\"whatever\",\n",
    "\"whence\",\n",
    "\"how\",\n",
    "\"how's\",\n",
    "\"how'd\",\n",
    "\"however\",\n",
    "\"whether\",\n",
    "\"whatsoever\",\n",
    "# Connector words and irrelevant adverbs\n",
    "\"and\",\n",
    "\"or\",\n",
    "\"not\",\n",
    "\"because\",\n",
    "\"also\",\n",
    "\"always\",\n",
    "\"never\",\n",
    "\"only\",\n",
    "\"really\",\n",
    "\"very\",\n",
    "\"greatly\",\n",
    "\"extremely\",\n",
    "\"somewhat\",\n",
    "\"no\",\n",
    "\"nope\",\n",
    "\"nah\",\n",
    "\"yes\",\n",
    "\"yep\",\n",
    "\"yeh\",\n",
    "\"yeah\",\n",
    "\"maybe\",\n",
    "\"perhaps\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"less\",\n",
    "\"least\",\n",
    "\"good\",\n",
    "\"great\",\n",
    "\"well\",\n",
    "\"better\",\n",
    "\"best\",\n",
    "\"bad\",\n",
    "\"worse\",\n",
    "\"worst\",\n",
    "\"too\",\n",
    "\"thru\",\n",
    "\"though\",\n",
    "\"although\",\n",
    "\"yet\",\n",
    "\"already\",\n",
    "\"then\",\n",
    "\"even\",\n",
    "\"now\",\n",
    "\"sometimes\",\n",
    "\"still\",\n",
    "\"together\",\n",
    "\"altogether\",\n",
    "\"entirely\",\n",
    "\"fully\",\n",
    "\"entire\",\n",
    "\"whole\",\n",
    "\"completely\",\n",
    "\"utterly\",\n",
    "\"seemingly\",\n",
    "\"apparently\",\n",
    "\"clearly\",\n",
    "\"obviously\",\n",
    "\"actually\",\n",
    "\"actual\",\n",
    "\"usually\",\n",
    "\"usual\",\n",
    "\"literally\",\n",
    "\"honestly\",\n",
    "\"absolutely\",\n",
    "\"definitely\",\n",
    "\"generally\",\n",
    "\"totally\",\n",
    "\"finally\",\n",
    "\"basically\",\n",
    "\"essentially\",\n",
    "\"fundamentally\",\n",
    "\"automatically\",\n",
    "\"immediately\",\n",
    "\"necessarily\",\n",
    "\"primarily\",\n",
    "\"normally\",\n",
    "\"perfectly\",\n",
    "\"constantly\",\n",
    "\"particularly\",\n",
    "\"eventually\",\n",
    "\"hopefully\",\n",
    "\"mainly\",\n",
    "\"typically\",\n",
    "\"specifically\",\n",
    "\"differently\",\n",
    "\"appropriately\",\n",
    "\"plenty\",\n",
    "\"certainly\",\n",
    "\"unfortunately\",\n",
    "\"ultimately\",\n",
    "\"unlikely\",\n",
    "\"likely\",\n",
    "\"potentially\",\n",
    "\"fortunately\",\n",
    "\"personally\",\n",
    "\"directly\",\n",
    "\"indirectly\",\n",
    "\"nearly\",\n",
    "\"closely\",\n",
    "\"slightly\",\n",
    "\"probably\",\n",
    "\"possibly\",\n",
    "\"especially\",\n",
    "\"frequently\",\n",
    "\"often\",\n",
    "\"oftentimes\",\n",
    "\"seldom\",\n",
    "\"rarely\",\n",
    "\"sure\",\n",
    "\"while\",\n",
    "\"whilst\",\n",
    "\"able\",\n",
    "\"unable\",\n",
    "\"else\",\n",
    "\"ever\",\n",
    "\"once\",\n",
    "\"twice\",\n",
    "\"thrice\",\n",
    "\"almost\",\n",
    "\"again\",\n",
    "\"instead\",\n",
    "\"next\",\n",
    "\"previous\",\n",
    "\"unless\",\n",
    "\"somehow\",\n",
    "\"anyhow\",\n",
    "\"anywhere\",\n",
    "\"somewhere\",\n",
    "\"everywhere\",\n",
    "\"nowhere\",\n",
    "\"further\",\n",
    "\"anymore\",\n",
    "\"later\",\n",
    "\"ago\",\n",
    "\"ahead\",\n",
    "\"just\",\n",
    "\"same\",\n",
    "\"different\",\n",
    "\"big\",\n",
    "\"small\",\n",
    "\"little\",\n",
    "\"tiny\",\n",
    "\"large\",\n",
    "\"huge\",\n",
    "\"pretty\",\n",
    "\"mostly\",\n",
    "\"anyway\",\n",
    "\"anyways\",\n",
    "\"otherwise\",\n",
    "\"regardless\",\n",
    "\"throughout\",\n",
    "\"additionally\",\n",
    "\"moreover\",\n",
    "\"furthermore\",\n",
    "\"meanwhile\",\n",
    "\"afterwards\",\n",
    "# Irrelevant nouns\n",
    "\"thing\",\n",
    "\"thing's\",\n",
    "\"things\",\n",
    "\"stuff\",\n",
    "\"other's\",\n",
    "\"others\",\n",
    "\"another's\",\n",
    "\"total\",\n",
    "\"\",\n",
    "\"false\",\n",
    "\"none\",\n",
    "\"way\",\n",
    "\"kind\",\n",
    "# Lettered numbers and order\n",
    "\"zero\",\n",
    "\"zeros\",\n",
    "\"zeroes\",\n",
    "\"one\",\n",
    "\"ones\",\n",
    "\"two\",\n",
    "\"three\",\n",
    "\"four\",\n",
    "\"five\",\n",
    "\"six\", \n",
    "\"seven\",\n",
    "\"eight\",\n",
    "\"nine\",\n",
    "\"ten\",\n",
    "\"twenty\",\n",
    "\"thirty\",\n",
    "\"forty\",\n",
    "\"fifty\",\n",
    "\"sixty\",\n",
    "\"seventy\",\n",
    "\"eighty\",\n",
    "\"ninety\",\n",
    "\"hundred\",\n",
    "\"hundreds\",\n",
    "\"thousand\",\n",
    "\"thousands\",\n",
    "\"million\",\n",
    "\"millions\",\n",
    "\"first\",\n",
    "\"last\",\n",
    "\"second\",\n",
    "\"third\",\n",
    "\"fourth\",\n",
    "\"fifth\",\n",
    "\"sixth\",\n",
    "\"seventh\",\n",
    "\"eigth\",\n",
    "\"ninth\",\n",
    "\"tenth\",\n",
    "\"firstly\",\n",
    "\"secondly\",\n",
    "\"thirdly\",\n",
    "\"lastly\",\n",
    "# Greetings and slang\n",
    "\"hello\",\n",
    "\"hi\",\n",
    "\"hey\",\n",
    "\"sup\",\n",
    "\"yo\",\n",
    "\"greetings\",\n",
    "\"please\",\n",
    "\"okay\",\n",
    "\"ok\",\n",
    "\"y'all\",\n",
    "\"lol\",\n",
    "\"rofl\",\n",
    "\"thank\",\n",
    "\"thanks\",\n",
    "\"alright\",\n",
    "\"kinda\",\n",
    "\"dont\",\n",
    "\"sorry\",\n",
    "\"idk\",\n",
    "\"tldr\",\n",
    "\"tl\",\n",
    "\"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "\"tbh\",\n",
    "\"dude\",\n",
    "\"tho\",\n",
    "\"aka\",\n",
    "\"plz\",\n",
    "\"pls\",\n",
    "\"bit\",\n",
    "\"don\",\n",
    "# Miscellaneous\n",
    "\"www\",\n",
    "\"https\",\n",
    "\"http\",\n",
    "\"com\",\n",
    "\"etc\"\n",
    "\"html\",\n",
    "\"reddit\",\n",
    "\"subreddit\",\n",
    "\"subreddits\",\n",
    "\"comments\",\n",
    "\"reply\",\n",
    "\"replies\",\n",
    "\"thread\",\n",
    "\"threads\",\n",
    "\"post\",\n",
    "\"posts\",\n",
    "\"website\",\n",
    "\"websites\",\n",
    "\"web site\",\n",
    "\"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8mPdJZSRToI",
    "outputId": "49ab52c4-3b47-4f9f-e5a7-553878680750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 94.011.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n",
      "Run time: 5.304453372955322 seconds\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "#test custom dictionary => 94.01\n",
    "#selected =>4\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_custom)],\n",
    "    'vect__preprocessor': [preprocess_text,remove_punctuation,None],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    \"clf__alpha\" : [0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "y_pred = grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"MultinomialNB_without.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU1CGkQZS0Qa",
    "outputId": "3b0ae523-faf6-4faa-cb46-5d38fb32fe98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aint', 'anothers', 'anyones', 'anythings', 'arent', 'cant', 'couldnt', 'didnt', 'doesnt', 'everybodys', 'everyones', 'everythings', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'heres', 'hes', 'howd', 'hows', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'mustve', 'nobodys', 'nothings', 'shed', 'shell', 'shes', 'shouldnt', 'site', 'sites', 'somebodys', 'someones', 'somethings', 'thats', 'theres', 'theyd', 'theyll', 'theyre', 'theyve', 'wasnt', 'web', 'wed', 'werent', 'weve', 'whats', 'whered', 'wheres', 'whod', 'whoevers', 'whomevers', 'whos', 'whosevers', 'whove', 'whyd', 'whys', 'wont', 'wouldnt', 'yall', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 89.558.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__preprocessor': <function remove_punctuation at 0x7f6bd75ee790>, 'vect__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n",
      "Run time: 2.1990275382995605 seconds\n"
     ]
    }
   ],
   "source": [
    "#test custom dictionary => 94.01\n",
    "#selected =>4\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_custom)],\n",
    "    #'vect__preprocessor': [preprocess_text,remove_punctuation,None],\n",
    "    'vect__preprocessor': [remove_punctuation],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    \"clf__alpha\" : [0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gcCHhhZVKo2",
    "outputId": "685d4bee-cce6-4241-9a2a-08ab0b57a4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "y_pred_new = grid.predict(test_x)\n",
    "create_test_csv(y_pred_new,\"multi_pipeline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yo_t1tMpUmfP",
    "outputId": "d5c6f3ee-0ce2-4dcc-f288-06c8a60ea787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 94.011.\n",
      "The winning parameters are {'clf__alpha': 0.5, 'selecter__k': 5000, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n",
      "Run time: 4.01872992515564 seconds\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "#test selector => 94.011.\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_custom)],\n",
    "    'vect__preprocessor': [None],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"clf__alpha\" : [0.5,0.1],\n",
    "   #  \"normalizer__norm\": ['l2','l1']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"normalizer\",normalizer),(\"clf\", MultinomialNB())])\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "y_pred = grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"MultinomialNB_S_03032023_01.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF0lnhKpziTO"
   },
   "outputs": [],
   "source": [
    "#test the final after preprocessing\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False],\n",
    "    \"vect__stop_words\": [list(stop_words_custom)],\n",
    "    'vect__preprocessor': [],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"clf__alpha\" : [0.5,0.1],\n",
    "   #  \"normalizer__norm\": ['l2','l1']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"normalizer\",normalizer),(\"clf\", MultinomialNB())])\n",
    "pipe = Pipeline([(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\", MultinomialNB())])\n",
    "\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "y_pred = grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"MultinomialNB_S_03032023_01.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSvk9LDMX1Xm",
    "outputId": "ff409e68-dbfa-4219-ad6f-363c7105033c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5-fold cross-validation accuracy is: 0.92755\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "#now that the model is finalized , build the final model\n",
    "\n",
    "###############################################################do not use this\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "final_vectorize = CountVectorizer(stop_words = stop_words_custom, ngram_range=(1,1), binary=False)\n",
    "vec_x_train = np.asarray(final_vectorize.fit_transform(train_x).todense())\n",
    "vec_x_test = np.asarray(final_vectorize.transform(test_x).todense())\n",
    "\n",
    "#skLearnFeatureSelector = SelectKBest(chi2, k=5000)\n",
    "\n",
    "#selected_x_train = skLearnFeatureSelector.fit_transform(vec_x_train, train_y)\n",
    "#selected_x_test = skLearnFeatureSelector.transform(vec_x_test)\n",
    "\n",
    "\n",
    "\n",
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(vec_x_train, train_y)\n",
    "\n",
    "# Step 4: Evaluate the model using cross-validation\n",
    "cv_scores = cross_val_score(model, vec_x_train, train_y, cv=5)\n",
    "mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "print(f\"The 5-fold cross-validation accuracy is: {mean_cv_accuracy:.5f}\")\n",
    "\n",
    "\n",
    "y_pred = model.predict(vec_x_test)\n",
    "create_test_csv(y_pred,\"final_MultinomialNB.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvY1Dm14OKwi"
   },
   "outputs": [],
   "source": [
    "######################################################### final"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
