{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWqhQVY5y5bV",
    "outputId": "a78cc062-b765-47bc-f7b6-3d95b4d5bcbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vNucqJ1zKmz",
    "outputId": "b4c3a0f3-6299-4680-9dd3-fa2f1d9872c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "\n",
    "\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1BAklrjzUu-"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdhhtzBhzXdN"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xb-mzD0zbp9"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "\n",
    "\n",
    "train_x_initial = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x_initial = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k48fggJozeFY"
   },
   "outputs": [],
   "source": [
    "#remove punctuation from train and test\n",
    "train_x = train_x_initial.copy()\n",
    "\n",
    "for i in range(train_x.shape[0]):\n",
    "  train_x[i]= train_x[i].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "test_x = test_x_initial.copy()\n",
    "\n",
    "for i in range(test_x.shape[0]):\n",
    "  test_x[i]= test_x[i].translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8gssJYvzgkw",
    "outputId": "bf313976-7cc0-417a-eb7d-48c15e9bb6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like cars with screensas long as the UI is intuitive and phonelike Ive never driven a new Edge nor have I driven a Ford with Sync 3\n",
      "\n",
      "As far as I can tell it looks good and concise I like it\n"
     ]
    }
   ],
   "source": [
    "print(test_x[5])\n",
    "#print(test_x_initial[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7KPY1VdzjMh"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muHwcce9d6ne"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "   translator = str.maketrans('', '', string.punctuation)\n",
    "   text = text.translate(translator)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av4CiO2NzlwS"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpjDQ_dMzt1c",
    "outputId": "a696d892-fc02-476d-fc2d-72181426f056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "The best accuracy is 89.411.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__class_weight': None, 'classify__max_iter': 100, 'classify__penalty': 'l2', 'classify__solver': 'liblinear'}\n",
      "Run time: 32.65085744857788 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training without removing parameters\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l1', 'l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [0.01, 0.1, 1.0, 10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['liblinear'],   #  #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [100, 500, 1000],\n",
    "    'classify__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "model = LogisticRegression()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSXD6oBC1RD3",
    "outputId": "f324f59c-9cae-4c82-805a-00d5f54a9745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best accuracy is 93.037.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__stop_words': ['thereupon', 'latter', 'up', 'aren', 'almost', \"that'll\", 'among', 'their', 'both', 'never', 'been', 'further', 'along', 'whoever', 'inc', \"you'd\", 'again', 'eleven', 'another', 'his', 'empty', 'always', 'though', 'within', 'bill', 'put', 'needn', 'behind', 'could', 'all', 'four', 'beforehand', 'otherwise', 'well', 'upon', 'often', \"won't\", 'under', 'its', 'perhaps', 'fire', 'un', \"don't\", 'already', 'from', 'six', 'still', 'get', 'go', 'as', 'during', 'toward', 'sixty', 'not', 'isn', 'amongst', 'had', \"didn't\", 'see', 'first', 'nowhere', \"wouldn't\", 'mostly', 'thin', 'detail', 'hence', 'ours', 'against', 'being', 'seem', 'they', 'keep', 'none', 'less', 'become', 'where', 'sometimes', 'move', 'mill', 'nine', 'hereafter', 'yours', 'few', 'i', 'much', 'twenty', 'has', 'last', 'or', 'while', 'anything', 'don', 'top', 'whereas', 'something', 'hasn', 'such', 'between', 'down', 'm', 'off', 'even', 'others', 'we', 'system', 'after', 'therein', 'thereafter', 'this', 'weren', 'cant', 'two', 'which', 'thus', 'wouldn', 'fifteen', 'can', 'wasn', 'will', 'made', 'are', 'once', 'several', 'third', 'whenever', 'must', 'themselves', \"shan't\", 'front', 'about', 'a', 'amoungst', 'some', \"hadn't\", 'show', 'whether', 'formerly', 'sincere', 'the', 'with', 'more', 'side', 'there', \"you're\", 'mine', 'however', 'whereafter', 'give', 'ain', 'mustn', 'here', 'became', 'did', 'now', \"isn't\", 'shan', 'he', \"hasn't\", 'any', 'ma', 'nobody', 'didn', 'theirs', 'until', 'you', 'it', 'hereupon', 'above', 'noone', 'least', 'becomes', 'hasnt', 'other', 'them', 'were', 'someone', 'eg', \"she's\", 'con', 'take', 'haven', 'serious', 'when', 'alone', 'my', 'anyone', \"aren't\", 'fifty', 'was', 'please', \"should've\", 'nevertheless', 'de', 'doesn', 'since', \"shouldn't\", 'latterly', 'although', 'name', 'cry', 'ourselves', 'too', 'is', 'and', 'herein', 'an', 'full', 'y', 'very', 'forty', 'itself', 'me', 'before', 'd', 'do', 'moreover', 'back', 'eight', 'most', 'so', 'therefore', 'via', 'who', 'thereby', 'through', 'shouldn', 'co', 'many', 'hadn', 'whom', \"it's\", 'yourselves', 'ltd', 'due', 'somewhere', 's', 'ie', 'done', 'afterwards', 'himself', 'onto', 'call', 'beyond', 'below', 'yourself', 'everyone', \"haven't\", 'him', 'just', 'seeming', 'does', 'may', 'per', 'find', 'because', 'if', 'would', 'whence', 'cannot', 'enough', 'on', 'five', 'ever', 'across', 'herself', 'to', \"couldn't\", 'us', \"doesn't\", 'wherein', 'thence', 'in', 'describe', 'whole', 'whatever', 'everything', 'elsewhere', \"you've\", \"mustn't\", 'fill', 'your', 're', 'might', 'twelve', 'having', 'besides', 'am', 'nor', 've', \"needn't\", 'else', 'hers', 'what', 'hundred', 'beside', 'thick', 'o', 'either', 'throughout', 'only', 'anywhere', 'couldn', \"weren't\", 'those', 'interest', 'amount', 'neither', 'each', 'doing', 'bottom', 'sometime', 'next', 'without', 'meanwhile', 'seemed', 'except', 'why', 'for', 'over', \"you'll\", 'whereby', 'same', \"wasn't\", 'former', 'wherever', 't', 'around', 'mightn', 'one', 'she', 'no', 'part', 'anyhow', 'couldnt', 'own', 'three', 'rather', 'won', 'our', \"mightn't\", 'that', 'yet', 'etc', 'by', 'indeed', 'into', 'at', 'also', 'thru', 'how', 'than', 'towards', 'ten', 'myself', 'of', 'then', 'll', 'anyway', 'hereby', 'her', 'should', 'whither', 'be', 'seems', 'have', 'nothing', 'everywhere', 'whereupon', 'these', 'together', 'becoming', 'but', 'out', 'every', 'namely', 'somehow', 'found', 'whose']}\n",
      "Run time: 11.839037895202637 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words  93.037\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    #'classify__class_weight': ['balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)],\n",
    "    \"selecter__k\":[5000]\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stop_words_nltk \n",
    "#stop_words_sklearn \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enDObTdgGUfE"
   },
   "outputs": [],
   "source": [
    "#stem lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kc756XZHfv0x",
    "outputId": "0fed43ee-6c14-4171-a1c0-42c665cbaec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length custom: 589\n"
     ]
    }
   ],
   "source": [
    "stop_words_custom = [\n",
    "# All pronouns and associated words\n",
    "\"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\"you'll\",\"you'd\",\"you're\",\"you've\",\"yourself\",\"he\",\"he'll\",\n",
    "\"he'd\",\n",
    "\"he's\",\n",
    "\"him\",\n",
    "\"she\",\n",
    "\"she'll\",\n",
    "\"she'd\",\n",
    "\"she's\",\n",
    "\"her\",\n",
    "\"it\",\n",
    "\"it'll\",\n",
    "\"it'd\",\n",
    "\"it's\",\n",
    "\"itself\",\n",
    "\"oneself\",\n",
    "\"we\",\n",
    "\"we'll\",\n",
    "\"we'd\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"us\",\n",
    "\"ourselves\",\n",
    "\"they\",\n",
    "\"they'll\",\n",
    "\"they'd\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"them\",\n",
    "\"themselves\",\n",
    "\"everyone\",\n",
    "\"everyone's\",\n",
    "\"everybody\",\n",
    "\"everybody's\",\n",
    "\"someone\",\n",
    "\"someone's\",\n",
    "\"somebody\",\n",
    "\"somebody's\",\n",
    "\"nobody\",\n",
    "\"nobody's\",\n",
    "\"anyone\",\n",
    "\"anyone's\",\n",
    "\"everything\",\n",
    "\"everything's\",\n",
    "\"something\",\n",
    "\"something's\",\n",
    "\"nothing\",\n",
    "\"nothing's\",\n",
    "\"anything\",\n",
    "\"anything's\",\n",
    "# All determiners and associated words\n",
    "\"a\",\n",
    "\"an\",\n",
    "\"the\",\n",
    "\"this\",\n",
    "\"that\",\n",
    "\"that's\",\n",
    "\"these\",\n",
    "\"those\",\n",
    "\"my\",\n",
    "#\"mine\",   #Omitted since mine can refer to something else\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"his\",\n",
    "\"hers\",\n",
    "\"its\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"own\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"few\",\n",
    "\"much\",\n",
    "\"many\",\n",
    "\"lot\",\n",
    "\"lots\",\n",
    "\"some\",\n",
    "\"any\",\n",
    "\"enough\",\n",
    "\"all\",\n",
    "\"both\",\n",
    "\"half\",\n",
    "\"either\",\n",
    "\"neither\",\n",
    "\"each\",\n",
    "\"every\",\n",
    "\"certain\",\n",
    "\"other\",\n",
    "\"another\",\n",
    "\"such\",\n",
    "\"several\",\n",
    "\"multiple\",\n",
    "# \"what\",#Dealt with later on\n",
    "\"rather\",\n",
    "\"quite\",\n",
    "# All prepositions\n",
    "\"aboard\",\n",
    "\"about\",\n",
    "\"above\",\n",
    "\"across\",\n",
    "\"after\",\n",
    "\"against\",\n",
    "\"along\",\n",
    "\"amid\",\n",
    "\"amidst\",\n",
    "\"among\",\n",
    "\"amongst\",\n",
    "\"anti\",\n",
    "\"around\",\n",
    "\"as\",\n",
    "\"at\",\n",
    "\"away\",\n",
    "\"before\",\n",
    "\"behind\",\n",
    "\"below\",\n",
    "\"beneath\",\n",
    "\"beside\",\n",
    "\"besides\",\n",
    "\"between\",\n",
    "\"beyond\",\n",
    "\"but\",\n",
    "\"by\",\n",
    "\"concerning\",\n",
    "\"considering\",\n",
    "\"despite\",\n",
    "\"down\",\n",
    "\"during\",\n",
    "\"except\",\n",
    "\"excepting\",\n",
    "\"excluding\",\n",
    "\"far\",\n",
    "\"following\",\n",
    "\"for\",\n",
    "\"from\",\n",
    "\"here\",\n",
    "\"here's\",\n",
    "\"in\",\n",
    "\"inside\",\n",
    "\"into\",\n",
    "\"left\",\n",
    "\"like\",\n",
    "\"minus\",\n",
    "\"near\",\n",
    "\"of\",\n",
    "\"off\",\n",
    "\"on\",\n",
    "\"onto\",\n",
    "\"opposite\",\n",
    "\"out\",\n",
    "\"outside\",\n",
    "\"over\",\n",
    "\"past\",\n",
    "\"per\",\n",
    "\"plus\",\n",
    "\"regarding\",\n",
    "\"right\",\n",
    "#\"round\",   #Omitted\n",
    "#\"save\",#Omitted\n",
    "\"since\",\n",
    "\"than\",\n",
    "\"there\",\n",
    "\"there's\",\n",
    "\"through\",\n",
    "\"to\",\n",
    "\"toward\",\n",
    "\"towards\",\n",
    "\"under\",\n",
    "\"underneath\",\n",
    "\"unlike\",\n",
    "\"until\",\n",
    "\"up\",\n",
    "\"upon\",\n",
    "\"versus\",\n",
    "\"via\",\n",
    "\"with\",\n",
    "\"within\",\n",
    "\"without\",\n",
    "# Irrelevant verbs\n",
    "\"may\",\n",
    "\"might\",\n",
    "\"will\",\n",
    "\"won't\",\n",
    "\"would\",\n",
    "\"wouldn't\",\n",
    "\"can\",\n",
    "\"can't\",\n",
    "\"cannot\",\n",
    "\"could\",\n",
    "\"couldn't\",\n",
    "\"should\",\n",
    "\"shouldn't\",\n",
    "\"must\",\n",
    "\"must've\",\n",
    "\"be\",\n",
    "\"being\",\n",
    "\"been\",\n",
    "\"am\",\n",
    "\"are\",\n",
    "\"aren't\",\n",
    "\"ain't\",\n",
    "\"is\",\n",
    "\"isn't\",\n",
    "\"was\",\n",
    "\"wasn't\",\n",
    "\"were\",\n",
    "\"weren't\",\n",
    "\"do\",\n",
    "\"doing\",\n",
    "\"don't\",\n",
    "\"does\",\n",
    "\"doesn't\",\n",
    "\"did\",\n",
    "\"didn't\",\n",
    "\"done\",\n",
    "\"have\",\n",
    "\"haven't\",\n",
    "\"having\",\n",
    "\"has\",\n",
    "\"hasn't\",\n",
    "\"had\",\n",
    "\"hadn't\",\n",
    "\"get\",\n",
    "\"getting\",\n",
    "\"gets\",\n",
    "\"got\",\n",
    "\"gotten\",\n",
    "\"go\",\n",
    "\"going\",\n",
    "\"gonna\",\n",
    "\"goes\",\n",
    "\"went\",\n",
    "\"gone\",\n",
    "\"make\",\n",
    "\"making\",\n",
    "\"makes\",\n",
    "\"made\",\n",
    "\"take\",\n",
    "\"taking\",\n",
    "\"takes\",\n",
    "\"took\",\n",
    "\"taken\",\n",
    "\"need\",\n",
    "\"needing\",\n",
    "\"needs\",\n",
    "\"needed\",\n",
    "\"use\",\n",
    "\"using\",\n",
    "\"uses\",\n",
    "\"used\",\n",
    "\"want\",\n",
    "\"wanna\",\n",
    "\"wanting\",\n",
    "\"wants\",\n",
    "\"let\",\n",
    "\"lets\",\n",
    "\"letting\",\n",
    "\"let's\",\n",
    "\"suppose\",\n",
    "\"supposing\",\n",
    "\"supposes\",\n",
    "\"supposed\",\n",
    "\"seem\",\n",
    "\"seeming\",\n",
    "\"seems\",\n",
    "\"seemed\",\n",
    "\"say\",\n",
    "\"saying\",\n",
    "\"says\",\n",
    "\"said\",\n",
    "\"know\",\n",
    "\"knowing\",\n",
    "\"knows\",\n",
    "\"knew\",\n",
    "\"known\",\n",
    "\"look\",\n",
    "\"looking\",\n",
    "\"looked\",\n",
    "\"think\",\n",
    "\"thinking\",\n",
    "\"thinks\",\n",
    "\"thought\",\n",
    "\"feel\",\n",
    "\"feels\",\n",
    "\"felt\",\n",
    "\"based\",\n",
    "\"put\",\n",
    "\"puts\",\n",
    "#\"wanted\"   #Omitted since the advective is relevant\n",
    "# Question words and associated words\n",
    "\"who\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"who'd\",\n",
    "\"whoever\",\n",
    "\"whoever's\",\n",
    "\"whom\",\n",
    "\"whomever\",\n",
    "\"whomever's\",\n",
    "\"whose\",\n",
    "\"whosever\",\n",
    "\"whosever's\",\n",
    "\"when\",\n",
    "\"whenever\",\n",
    "\"which\",\n",
    "\"whichever\",\n",
    "\"where\",\n",
    "\"where's\",\n",
    "\"where'd\",\n",
    "\"wherever\",\n",
    "\"why\",\n",
    "\"why's\",\n",
    "\"why'd\",\n",
    "\"whyever\",\n",
    "\"what\",\n",
    "\"what's\",\n",
    "\"whatever\",\n",
    "\"whence\",\n",
    "\"how\",\n",
    "\"how's\",\n",
    "\"how'd\",\n",
    "\"however\",\n",
    "\"whether\",\n",
    "\"whatsoever\",\n",
    "# Connector words and irrelevant adverbs\n",
    "\"and\",\n",
    "\"or\",\n",
    "\"not\",\n",
    "\"because\",\n",
    "\"also\",\n",
    "\"always\",\n",
    "\"never\",\n",
    "\"only\",\n",
    "\"really\",\n",
    "\"very\",\n",
    "\"greatly\",\n",
    "\"extremely\",\n",
    "\"somewhat\",\n",
    "\"no\",\n",
    "\"nope\",\n",
    "\"nah\",\n",
    "\"yes\",\n",
    "\"yep\",\n",
    "\"yeh\",\n",
    "\"yeah\",\n",
    "\"maybe\",\n",
    "\"perhaps\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"less\",\n",
    "\"least\",\n",
    "\"good\",\n",
    "\"great\",\n",
    "\"well\",\n",
    "\"better\",\n",
    "\"best\",\n",
    "\"bad\",\n",
    "\"worse\",\n",
    "\"worst\",\n",
    "\"too\",\n",
    "\"thru\",\n",
    "\"though\",\n",
    "\"although\",\n",
    "\"yet\",\n",
    "\"already\",\n",
    "\"then\",\n",
    "\"even\",\n",
    "\"now\",\n",
    "\"sometimes\",\n",
    "\"still\",\n",
    "\"together\",\n",
    "\"altogether\",\n",
    "\"entirely\",\n",
    "\"fully\",\n",
    "\"entire\",\n",
    "\"whole\",\n",
    "\"completely\",\n",
    "\"utterly\",\n",
    "\"seemingly\",\n",
    "\"apparently\",\n",
    "\"clearly\",\n",
    "\"obviously\",\n",
    "\"actually\",\n",
    "\"actual\",\n",
    "\"usually\",\n",
    "\"usual\",\n",
    "\"literally\",\n",
    "\"honestly\",\n",
    "\"absolutely\",\n",
    "\"definitely\",\n",
    "\"generally\",\n",
    "\"totally\",\n",
    "\"finally\",\n",
    "\"basically\",\n",
    "\"essentially\",\n",
    "\"fundamentally\",\n",
    "\"automatically\",\n",
    "\"immediately\",\n",
    "\"necessarily\",\n",
    "\"primarily\",\n",
    "\"normally\",\n",
    "\"perfectly\",\n",
    "\"constantly\",\n",
    "\"particularly\",\n",
    "\"eventually\",\n",
    "\"hopefully\",\n",
    "\"mainly\",\n",
    "\"typically\",\n",
    "\"specifically\",\n",
    "\"differently\",\n",
    "\"appropriately\",\n",
    "\"plenty\",\n",
    "\"certainly\",\n",
    "\"unfortunately\",\n",
    "\"ultimately\",\n",
    "\"unlikely\",\n",
    "\"likely\",\n",
    "\"potentially\",\n",
    "\"fortunately\",\n",
    "\"personally\",\n",
    "\"directly\",\n",
    "\"indirectly\",\n",
    "\"nearly\",\n",
    "\"closely\",\n",
    "\"slightly\",\n",
    "\"probably\",\n",
    "\"possibly\",\n",
    "\"especially\",\n",
    "\"frequently\",\n",
    "\"often\",\n",
    "\"oftentimes\",\n",
    "\"seldom\",\n",
    "\"rarely\",\n",
    "\"sure\",\n",
    "\"while\",\n",
    "\"whilst\",\n",
    "\"able\",\n",
    "\"unable\",\n",
    "\"else\",\n",
    "\"ever\",\n",
    "\"once\",\n",
    "\"twice\",\n",
    "\"thrice\",\n",
    "\"almost\",\n",
    "\"again\",\n",
    "\"instead\",\n",
    "\"next\",\n",
    "\"previous\",\n",
    "\"unless\",\n",
    "\"somehow\",\n",
    "\"anyhow\",\n",
    "\"anywhere\",\n",
    "\"somewhere\",\n",
    "\"everywhere\",\n",
    "\"nowhere\",\n",
    "\"further\",\n",
    "\"anymore\",\n",
    "\"later\",\n",
    "\"ago\",\n",
    "\"ahead\",\n",
    "\"just\",\n",
    "\"same\",\n",
    "\"different\",\n",
    "\"big\",\n",
    "\"small\",\n",
    "\"little\",\n",
    "\"tiny\",\n",
    "\"large\",\n",
    "\"huge\",\n",
    "\"pretty\",\n",
    "\"mostly\",\n",
    "\"anyway\",\n",
    "\"anyways\",\n",
    "\"otherwise\",\n",
    "\"regardless\",\n",
    "\"throughout\",\n",
    "\"additionally\",\n",
    "\"moreover\",\n",
    "\"furthermore\",\n",
    "\"meanwhile\",\n",
    "\"afterwards\",\n",
    "# Irrelevant nouns\n",
    "\"thing\",\n",
    "\"thing's\",\n",
    "\"things\",\n",
    "\"stuff\",\n",
    "\"other's\",\n",
    "\"others\",\n",
    "\"another's\",\n",
    "\"total\",\n",
    "\"\",\n",
    "\"false\",\n",
    "\"none\",\n",
    "\"way\",\n",
    "\"kind\",\n",
    "# Lettered numbers and order\n",
    "\"zero\",\n",
    "\"zeros\",\n",
    "\"zeroes\",\n",
    "\"one\",\n",
    "\"ones\",\n",
    "\"two\",\n",
    "\"three\",\n",
    "\"four\",\n",
    "\"five\",\n",
    "\"six\", \n",
    "\"seven\",\n",
    "\"eight\",\n",
    "\"nine\",\n",
    "\"ten\",\n",
    "\"twenty\",\n",
    "\"thirty\",\n",
    "\"forty\",\n",
    "\"fifty\",\n",
    "\"sixty\",\n",
    "\"seventy\",\n",
    "\"eighty\",\n",
    "\"ninety\",\n",
    "\"hundred\",\n",
    "\"hundreds\",\n",
    "\"thousand\",\n",
    "\"thousands\",\n",
    "\"million\",\n",
    "\"millions\",\n",
    "\"first\",\n",
    "\"last\",\n",
    "\"second\",\n",
    "\"third\",\n",
    "\"fourth\",\n",
    "\"fifth\",\n",
    "\"sixth\",\n",
    "\"seventh\",\n",
    "\"eigth\",\n",
    "\"ninth\",\n",
    "\"tenth\",\n",
    "\"firstly\",\n",
    "\"secondly\",\n",
    "\"thirdly\",\n",
    "\"lastly\",\n",
    "# Greetings and slang\n",
    "\"hello\",\n",
    "\"hi\",\n",
    "\"hey\",\n",
    "\"sup\",\n",
    "\"yo\",\n",
    "\"greetings\",\n",
    "\"please\",\n",
    "\"okay\",\n",
    "\"ok\",\n",
    "\"y'all\",\n",
    "\"lol\",\n",
    "\"rofl\",\n",
    "\"thank\",\n",
    "\"thanks\",\n",
    "\"alright\",\n",
    "\"kinda\",\n",
    "\"dont\",\n",
    "\"sorry\",\n",
    "\"idk\",\n",
    "\"tldr\",\n",
    "\"tl\",\n",
    "\"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "\"tbh\",\n",
    "\"dude\",\n",
    "\"tho\",\n",
    "\"aka\",\n",
    "\"plz\",\n",
    "\"pls\",\n",
    "\"bit\",\n",
    "\"don\",\n",
    "# Miscellaneous\n",
    "\"www\",\n",
    "\"https\",\n",
    "\"http\",\n",
    "\"com\",\n",
    "\"etc\"\n",
    "\"html\",\n",
    "\"reddit\",\n",
    "\"subreddit\",\n",
    "\"subreddits\",\n",
    "\"comments\",\n",
    "\"reply\",\n",
    "\"replies\",\n",
    "\"thread\",\n",
    "\"threads\",\n",
    "\"post\",\n",
    "\"posts\",\n",
    "\"website\",\n",
    "\"websites\",\n",
    "\"web site\",\n",
    "\"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bqd8EiCD2pPd",
    "outputId": "6074aa75-6a65-40ab-92f9-0aea92c44249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KveocHc97R71"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLbBiNdIIcys",
    "outputId": "02d64cd9-87e1-4abf-fc47-6fdc86885d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best accuracy is 93.037.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__stop_words': ['most', 'through', 'everything', 'had', 'have', 'these', 'did', 'un', 'still', 'anyone', 'her', 'almost', 'mine', \"hadn't\", 'its', 'one', \"shouldn't\", 'thence', 'never', 'your', 'doing', 'out', 'three', 'some', 'due', 'below', 'although', 'wasn', 'made', 'very', 'other', 'what', 'bill', 'am', 'as', 'see', 'cant', 'whose', 'fifty', 'wherein', 'amount', 'twenty', 'nobody', 'somewhere', \"you're\", 'hereafter', 'along', 've', 'hence', 'against', 'hadn', 'often', 'noone', 'more', 'fifteen', 'becomes', 'seem', 'mustn', 'ltd', 'upon', 'two', \"haven't\", 'won', 'among', 'something', \"aren't\", 'them', 'do', 'then', 'yourselves', 'give', 'onto', \"needn't\", 'whither', 'under', 'last', \"mightn't\", 'seems', 'shan', \"won't\", 'becoming', 'therefore', 'after', 'done', 'i', 'couldnt', 'another', 'put', 'towards', 'myself', \"you'd\", 'yet', \"shan't\", 'all', 'be', 'back', 'hers', 'you', 'from', 'on', \"wouldn't\", 'wherever', 'not', 'y', 'if', 'because', 'become', 'such', 'so', 'an', 'co', 'once', 'move', 'several', 'ourselves', 'even', 'nowhere', 'ours', 'himself', 'toward', 're', 'hasn', 'whence', 'him', 'must', 'meanwhile', 'there', 'four', 'behind', \"doesn't\", 'ain', 'whereupon', 'needn', 'anything', 'where', 'together', 'well', 'everyone', 'else', 'none', 'don', 'couldn', 'take', 'should', 'than', 'anyhow', 'might', 'further', 'whatever', 'someone', 'mightn', 'who', 'thereupon', 'across', 'full', 'least', 'throughout', 'twelve', 'haven', 'being', 'namely', 'call', 'isn', 'ever', 'until', 'yours', 'will', 'inc', \"hasn't\", 'm', \"weren't\", 'whoever', 'my', 'down', 'at', 'sometime', 'she', 't', 'herein', 'itself', 'part', 'sixty', 'here', \"couldn't\", 'he', 'theirs', 'whereas', 'otherwise', 'yourself', 'that', 'again', 'forty', 's', 'always', 'which', 'bottom', 'how', 'can', 'go', 'hereupon', 'since', 'just', 'latterly', 'could', 'hereby', 'll', 'mostly', \"you've\", 'much', 'seemed', \"mustn't\", 'was', 'our', 'without', 'beforehand', 'serious', 'via', 'me', 'formerly', 'why', 'enough', \"should've\", 'whereafter', 'perhaps', 'sincere', 'five', 'many', 'now', 'thereafter', 'about', 'detail', 'and', 'wouldn', 'cannot', 'having', \"didn't\", 'it', 'eleven', 'nor', 'cry', 'either', 'thin', 'sometimes', 'seeming', 'we', 'd', 'con', 'same', 'to', 'per', 'his', 'the', 'fire', 'found', 'describe', 'already', 'within', 'whether', 'doesn', 'latter', 'has', 'therein', 'rather', 'of', 'anywhere', 'amongst', 'ten', 'o', 'would', 'front', 'de', 'alone', 'system', 'elsewhere', 'those', 'for', 'thick', 'etc', 'a', 'are', 'find', 'though', 'neither', 'whereby', 'own', 'over', 'only', 'thereby', \"don't\", 'no', 'whenever', 'themselves', 'also', 'beside', 'nothing', 'thus', 'ie', 'third', 'aren', 'too', 'during', 'off', 'became', 'didn', 'fill', 'indeed', 'please', 'in', 'hasnt', 'hundred', 'afterwards', 'mill', 'name', 'their', 'former', 'but', 'moreover', 'thru', 'however', 'whole', 'been', 'next', 'besides', 'eg', 'side', \"you'll\", 'first', 'keep', 'somehow', 'weren', 'each', 'nevertheless', 'up', 'is', 'they', 'amoungst', 'any', 'everywhere', 'around', 'empty', \"isn't\", 'anyway', 'shouldn', \"that'll\", 'nine', 'beyond', 'while', 'whom', 'were', 'top', \"she's\", 'interest', 'show', 'get', 'ma', 'less', 'between', 'by', 'does', 'herself', 'few', 'above', 'into', 'with', 'six', 'may', 'except', 'eight', \"wasn't\", 'others', \"it's\", 'us', 'both', 'every', 'this', 'when', 'or', 'before']}\n",
      "Run time: 15.71203327178955 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words. LemmaTokenizer_word\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    #'classify__class_weight': [None],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)],\n",
    "    \"selecter__k\":[5000],\n",
    "    #\"vect__tokenizer\": [LemmaTokenizer_word()]\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stop_words_nltk \n",
    "#stop_words_sklearn \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hI1fx0kwPWK2",
    "outputId": "6b4ae69e-1d0b-4192-9a7a-d361cbb89aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The best accuracy is 92.339.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['most', 'through', 'everything', 'had', 'have', 'these', 'did', 'un', 'still', 'anyone', 'her', 'almost', 'mine', \"hadn't\", 'its', 'one', \"shouldn't\", 'thence', 'never', 'your', 'doing', 'out', 'three', 'some', 'due', 'below', 'although', 'wasn', 'made', 'very', 'other', 'what', 'bill', 'am', 'as', 'see', 'cant', 'whose', 'fifty', 'wherein', 'amount', 'twenty', 'nobody', 'somewhere', \"you're\", 'hereafter', 'along', 've', 'hence', 'against', 'hadn', 'often', 'noone', 'more', 'fifteen', 'becomes', 'seem', 'mustn', 'ltd', 'upon', 'two', \"haven't\", 'won', 'among', 'something', \"aren't\", 'them', 'do', 'then', 'yourselves', 'give', 'onto', \"needn't\", 'whither', 'under', 'last', \"mightn't\", 'seems', 'shan', \"won't\", 'becoming', 'therefore', 'after', 'done', 'i', 'couldnt', 'another', 'put', 'towards', 'myself', \"you'd\", 'yet', \"shan't\", 'all', 'be', 'back', 'hers', 'you', 'from', 'on', \"wouldn't\", 'wherever', 'not', 'y', 'if', 'because', 'become', 'such', 'so', 'an', 'co', 'once', 'move', 'several', 'ourselves', 'even', 'nowhere', 'ours', 'himself', 'toward', 're', 'hasn', 'whence', 'him', 'must', 'meanwhile', 'there', 'four', 'behind', \"doesn't\", 'ain', 'whereupon', 'needn', 'anything', 'where', 'together', 'well', 'everyone', 'else', 'none', 'don', 'couldn', 'take', 'should', 'than', 'anyhow', 'might', 'further', 'whatever', 'someone', 'mightn', 'who', 'thereupon', 'across', 'full', 'least', 'throughout', 'twelve', 'haven', 'being', 'namely', 'call', 'isn', 'ever', 'until', 'yours', 'will', 'inc', \"hasn't\", 'm', \"weren't\", 'whoever', 'my', 'down', 'at', 'sometime', 'she', 't', 'herein', 'itself', 'part', 'sixty', 'here', \"couldn't\", 'he', 'theirs', 'whereas', 'otherwise', 'yourself', 'that', 'again', 'forty', 's', 'always', 'which', 'bottom', 'how', 'can', 'go', 'hereupon', 'since', 'just', 'latterly', 'could', 'hereby', 'll', 'mostly', \"you've\", 'much', 'seemed', \"mustn't\", 'was', 'our', 'without', 'beforehand', 'serious', 'via', 'me', 'formerly', 'why', 'enough', \"should've\", 'whereafter', 'perhaps', 'sincere', 'five', 'many', 'now', 'thereafter', 'about', 'detail', 'and', 'wouldn', 'cannot', 'having', \"didn't\", 'it', 'eleven', 'nor', 'cry', 'either', 'thin', 'sometimes', 'seeming', 'we', 'd', 'con', 'same', 'to', 'per', 'his', 'the', 'fire', 'found', 'describe', 'already', 'within', 'whether', 'doesn', 'latter', 'has', 'therein', 'rather', 'of', 'anywhere', 'amongst', 'ten', 'o', 'would', 'front', 'de', 'alone', 'system', 'elsewhere', 'those', 'for', 'thick', 'etc', 'a', 'are', 'find', 'though', 'neither', 'whereby', 'own', 'over', 'only', 'thereby', \"don't\", 'no', 'whenever', 'themselves', 'also', 'beside', 'nothing', 'thus', 'ie', 'third', 'aren', 'too', 'during', 'off', 'became', 'didn', 'fill', 'indeed', 'please', 'in', 'hasnt', 'hundred', 'afterwards', 'mill', 'name', 'their', 'former', 'but', 'moreover', 'thru', 'however', 'whole', 'been', 'next', 'besides', 'eg', 'side', \"you'll\", 'first', 'keep', 'somehow', 'weren', 'each', 'nevertheless', 'up', 'is', 'they', 'amoungst', 'any', 'everywhere', 'around', 'empty', \"isn't\", 'anyway', 'shouldn', \"that'll\", 'nine', 'beyond', 'while', 'whom', 'were', 'top', \"she's\", 'interest', 'show', 'get', 'ma', 'less', 'between', 'by', 'does', 'herself', 'few', 'above', 'into', 'with', 'six', 'may', 'except', 'eight', \"wasn't\", 'others', \"it's\", 'us', 'both', 'every', 'this', 'when', 'or', 'before']}\n",
      "Run time: 9.341354131698608 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    #'classify__class_weight': ['balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)],\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"vect__ngram_range\":[(1,1)]\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"normalizer\",normalizer),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDCs_3diXOKo",
    "outputId": "e6bff5ae-d2a6-4971-b959-27364ccc8da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "The best accuracy is 92.899.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__class_weight': None, 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['most', 'through', 'everything', 'had', 'have', 'these', 'did', 'un', 'still', 'anyone', 'her', 'almost', 'mine', \"hadn't\", 'its', 'one', \"shouldn't\", 'thence', 'never', 'your', 'doing', 'out', 'three', 'some', 'due', 'below', 'although', 'wasn', 'made', 'very', 'other', 'what', 'bill', 'am', 'as', 'see', 'cant', 'whose', 'fifty', 'wherein', 'amount', 'twenty', 'nobody', 'somewhere', \"you're\", 'hereafter', 'along', 've', 'hence', 'against', 'hadn', 'often', 'noone', 'more', 'fifteen', 'becomes', 'seem', 'mustn', 'ltd', 'upon', 'two', \"haven't\", 'won', 'among', 'something', \"aren't\", 'them', 'do', 'then', 'yourselves', 'give', 'onto', \"needn't\", 'whither', 'under', 'last', \"mightn't\", 'seems', 'shan', \"won't\", 'becoming', 'therefore', 'after', 'done', 'i', 'couldnt', 'another', 'put', 'towards', 'myself', \"you'd\", 'yet', \"shan't\", 'all', 'be', 'back', 'hers', 'you', 'from', 'on', \"wouldn't\", 'wherever', 'not', 'y', 'if', 'because', 'become', 'such', 'so', 'an', 'co', 'once', 'move', 'several', 'ourselves', 'even', 'nowhere', 'ours', 'himself', 'toward', 're', 'hasn', 'whence', 'him', 'must', 'meanwhile', 'there', 'four', 'behind', \"doesn't\", 'ain', 'whereupon', 'needn', 'anything', 'where', 'together', 'well', 'everyone', 'else', 'none', 'don', 'couldn', 'take', 'should', 'than', 'anyhow', 'might', 'further', 'whatever', 'someone', 'mightn', 'who', 'thereupon', 'across', 'full', 'least', 'throughout', 'twelve', 'haven', 'being', 'namely', 'call', 'isn', 'ever', 'until', 'yours', 'will', 'inc', \"hasn't\", 'm', \"weren't\", 'whoever', 'my', 'down', 'at', 'sometime', 'she', 't', 'herein', 'itself', 'part', 'sixty', 'here', \"couldn't\", 'he', 'theirs', 'whereas', 'otherwise', 'yourself', 'that', 'again', 'forty', 's', 'always', 'which', 'bottom', 'how', 'can', 'go', 'hereupon', 'since', 'just', 'latterly', 'could', 'hereby', 'll', 'mostly', \"you've\", 'much', 'seemed', \"mustn't\", 'was', 'our', 'without', 'beforehand', 'serious', 'via', 'me', 'formerly', 'why', 'enough', \"should've\", 'whereafter', 'perhaps', 'sincere', 'five', 'many', 'now', 'thereafter', 'about', 'detail', 'and', 'wouldn', 'cannot', 'having', \"didn't\", 'it', 'eleven', 'nor', 'cry', 'either', 'thin', 'sometimes', 'seeming', 'we', 'd', 'con', 'same', 'to', 'per', 'his', 'the', 'fire', 'found', 'describe', 'already', 'within', 'whether', 'doesn', 'latter', 'has', 'therein', 'rather', 'of', 'anywhere', 'amongst', 'ten', 'o', 'would', 'front', 'de', 'alone', 'system', 'elsewhere', 'those', 'for', 'thick', 'etc', 'a', 'are', 'find', 'though', 'neither', 'whereby', 'own', 'over', 'only', 'thereby', \"don't\", 'no', 'whenever', 'themselves', 'also', 'beside', 'nothing', 'thus', 'ie', 'third', 'aren', 'too', 'during', 'off', 'became', 'didn', 'fill', 'indeed', 'please', 'in', 'hasnt', 'hundred', 'afterwards', 'mill', 'name', 'their', 'former', 'but', 'moreover', 'thru', 'however', 'whole', 'been', 'next', 'besides', 'eg', 'side', \"you'll\", 'first', 'keep', 'somehow', 'weren', 'each', 'nevertheless', 'up', 'is', 'they', 'amoungst', 'any', 'everywhere', 'around', 'empty', \"isn't\", 'anyway', 'shouldn', \"that'll\", 'nine', 'beyond', 'while', 'whom', 'were', 'top', \"she's\", 'interest', 'show', 'get', 'ma', 'less', 'between', 'by', 'does', 'herself', 'few', 'above', 'into', 'with', 'six', 'may', 'except', 'eight', \"wasn't\", 'others', \"it's\", 'us', 'both', 'every', 'this', 'when', 'or', 'before']}\n",
      "Run time: 9.679741621017456 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words. 93.038\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    'classify__class_weight': [None, 'balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],  ##[list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)]\n",
    "    \"selecter__k\":[5000],\n",
    "    \"vect__ngram_range\":[(1,1)]\n",
    "\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stop_words_nltk \n",
    "#stop_words_sklearn \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Flv3bRzZ8DY",
    "outputId": "018699ce-7360-4436-a712-d330ba57f9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The best accuracy is 93.038.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__class_weight': None, 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['most', 'through', 'everything', 'had', 'have', 'these', 'did', 'un', 'still', 'anyone', 'her', 'almost', 'mine', \"hadn't\", 'its', 'one', \"shouldn't\", 'thence', 'never', 'your', 'doing', 'out', 'three', 'some', 'due', 'below', 'although', 'wasn', 'made', 'very', 'other', 'what', 'bill', 'am', 'as', 'see', 'cant', 'whose', 'fifty', 'wherein', 'amount', 'twenty', 'nobody', 'somewhere', \"you're\", 'hereafter', 'along', 've', 'hence', 'against', 'hadn', 'often', 'noone', 'more', 'fifteen', 'becomes', 'seem', 'mustn', 'ltd', 'upon', 'two', \"haven't\", 'won', 'among', 'something', \"aren't\", 'them', 'do', 'then', 'yourselves', 'give', 'onto', \"needn't\", 'whither', 'under', 'last', \"mightn't\", 'seems', 'shan', \"won't\", 'becoming', 'therefore', 'after', 'done', 'i', 'couldnt', 'another', 'put', 'towards', 'myself', \"you'd\", 'yet', \"shan't\", 'all', 'be', 'back', 'hers', 'you', 'from', 'on', \"wouldn't\", 'wherever', 'not', 'y', 'if', 'because', 'become', 'such', 'so', 'an', 'co', 'once', 'move', 'several', 'ourselves', 'even', 'nowhere', 'ours', 'himself', 'toward', 're', 'hasn', 'whence', 'him', 'must', 'meanwhile', 'there', 'four', 'behind', \"doesn't\", 'ain', 'whereupon', 'needn', 'anything', 'where', 'together', 'well', 'everyone', 'else', 'none', 'don', 'couldn', 'take', 'should', 'than', 'anyhow', 'might', 'further', 'whatever', 'someone', 'mightn', 'who', 'thereupon', 'across', 'full', 'least', 'throughout', 'twelve', 'haven', 'being', 'namely', 'call', 'isn', 'ever', 'until', 'yours', 'will', 'inc', \"hasn't\", 'm', \"weren't\", 'whoever', 'my', 'down', 'at', 'sometime', 'she', 't', 'herein', 'itself', 'part', 'sixty', 'here', \"couldn't\", 'he', 'theirs', 'whereas', 'otherwise', 'yourself', 'that', 'again', 'forty', 's', 'always', 'which', 'bottom', 'how', 'can', 'go', 'hereupon', 'since', 'just', 'latterly', 'could', 'hereby', 'll', 'mostly', \"you've\", 'much', 'seemed', \"mustn't\", 'was', 'our', 'without', 'beforehand', 'serious', 'via', 'me', 'formerly', 'why', 'enough', \"should've\", 'whereafter', 'perhaps', 'sincere', 'five', 'many', 'now', 'thereafter', 'about', 'detail', 'and', 'wouldn', 'cannot', 'having', \"didn't\", 'it', 'eleven', 'nor', 'cry', 'either', 'thin', 'sometimes', 'seeming', 'we', 'd', 'con', 'same', 'to', 'per', 'his', 'the', 'fire', 'found', 'describe', 'already', 'within', 'whether', 'doesn', 'latter', 'has', 'therein', 'rather', 'of', 'anywhere', 'amongst', 'ten', 'o', 'would', 'front', 'de', 'alone', 'system', 'elsewhere', 'those', 'for', 'thick', 'etc', 'a', 'are', 'find', 'though', 'neither', 'whereby', 'own', 'over', 'only', 'thereby', \"don't\", 'no', 'whenever', 'themselves', 'also', 'beside', 'nothing', 'thus', 'ie', 'third', 'aren', 'too', 'during', 'off', 'became', 'didn', 'fill', 'indeed', 'please', 'in', 'hasnt', 'hundred', 'afterwards', 'mill', 'name', 'their', 'former', 'but', 'moreover', 'thru', 'however', 'whole', 'been', 'next', 'besides', 'eg', 'side', \"you'll\", 'first', 'keep', 'somehow', 'weren', 'each', 'nevertheless', 'up', 'is', 'they', 'amoungst', 'any', 'everywhere', 'around', 'empty', \"isn't\", 'anyway', 'shouldn', \"that'll\", 'nine', 'beyond', 'while', 'whom', 'were', 'top', \"she's\", 'interest', 'show', 'get', 'ma', 'less', 'between', 'by', 'does', 'herself', 'few', 'above', 'into', 'with', 'six', 'may', 'except', 'eight', \"wasn't\", 'others', \"it's\", 'us', 'both', 'every', 'this', 'when', 'or', 'before']}\n",
      "Run time: 30.26263689994812 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words. \n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    'classify__class_weight': [None, 'balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)],  ##[list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)]\n",
    "    \"selecter__k\":[5000],\n",
    "    \"vect__ngram_range\":[(1,1)],\n",
    "    # \"vect__binary\": [False]\n",
    "    #\"vect__preprocessor\": [preprocess_text,remove_punctuation,None]\n",
    "    #\"vect__binary\": [False]\n",
    "\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qydKLvN8jB4s",
    "outputId": "97ff78ee-dac5-4b52-dba7-ee166bc975a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "The best accuracy is 93.038.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__class_weight': None, 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['most', 'through', 'everything', 'had', 'have', 'these', 'did', 'un', 'still', 'anyone', 'her', 'almost', 'mine', \"hadn't\", 'its', 'one', \"shouldn't\", 'thence', 'never', 'your', 'doing', 'out', 'three', 'some', 'due', 'below', 'although', 'wasn', 'made', 'very', 'other', 'what', 'bill', 'am', 'as', 'see', 'cant', 'whose', 'fifty', 'wherein', 'amount', 'twenty', 'nobody', 'somewhere', \"you're\", 'hereafter', 'along', 've', 'hence', 'against', 'hadn', 'often', 'noone', 'more', 'fifteen', 'becomes', 'seem', 'mustn', 'ltd', 'upon', 'two', \"haven't\", 'won', 'among', 'something', \"aren't\", 'them', 'do', 'then', 'yourselves', 'give', 'onto', \"needn't\", 'whither', 'under', 'last', \"mightn't\", 'seems', 'shan', \"won't\", 'becoming', 'therefore', 'after', 'done', 'i', 'couldnt', 'another', 'put', 'towards', 'myself', \"you'd\", 'yet', \"shan't\", 'all', 'be', 'back', 'hers', 'you', 'from', 'on', \"wouldn't\", 'wherever', 'not', 'y', 'if', 'because', 'become', 'such', 'so', 'an', 'co', 'once', 'move', 'several', 'ourselves', 'even', 'nowhere', 'ours', 'himself', 'toward', 're', 'hasn', 'whence', 'him', 'must', 'meanwhile', 'there', 'four', 'behind', \"doesn't\", 'ain', 'whereupon', 'needn', 'anything', 'where', 'together', 'well', 'everyone', 'else', 'none', 'don', 'couldn', 'take', 'should', 'than', 'anyhow', 'might', 'further', 'whatever', 'someone', 'mightn', 'who', 'thereupon', 'across', 'full', 'least', 'throughout', 'twelve', 'haven', 'being', 'namely', 'call', 'isn', 'ever', 'until', 'yours', 'will', 'inc', \"hasn't\", 'm', \"weren't\", 'whoever', 'my', 'down', 'at', 'sometime', 'she', 't', 'herein', 'itself', 'part', 'sixty', 'here', \"couldn't\", 'he', 'theirs', 'whereas', 'otherwise', 'yourself', 'that', 'again', 'forty', 's', 'always', 'which', 'bottom', 'how', 'can', 'go', 'hereupon', 'since', 'just', 'latterly', 'could', 'hereby', 'll', 'mostly', \"you've\", 'much', 'seemed', \"mustn't\", 'was', 'our', 'without', 'beforehand', 'serious', 'via', 'me', 'formerly', 'why', 'enough', \"should've\", 'whereafter', 'perhaps', 'sincere', 'five', 'many', 'now', 'thereafter', 'about', 'detail', 'and', 'wouldn', 'cannot', 'having', \"didn't\", 'it', 'eleven', 'nor', 'cry', 'either', 'thin', 'sometimes', 'seeming', 'we', 'd', 'con', 'same', 'to', 'per', 'his', 'the', 'fire', 'found', 'describe', 'already', 'within', 'whether', 'doesn', 'latter', 'has', 'therein', 'rather', 'of', 'anywhere', 'amongst', 'ten', 'o', 'would', 'front', 'de', 'alone', 'system', 'elsewhere', 'those', 'for', 'thick', 'etc', 'a', 'are', 'find', 'though', 'neither', 'whereby', 'own', 'over', 'only', 'thereby', \"don't\", 'no', 'whenever', 'themselves', 'also', 'beside', 'nothing', 'thus', 'ie', 'third', 'aren', 'too', 'during', 'off', 'became', 'didn', 'fill', 'indeed', 'please', 'in', 'hasnt', 'hundred', 'afterwards', 'mill', 'name', 'their', 'former', 'but', 'moreover', 'thru', 'however', 'whole', 'been', 'next', 'besides', 'eg', 'side', \"you'll\", 'first', 'keep', 'somehow', 'weren', 'each', 'nevertheless', 'up', 'is', 'they', 'amoungst', 'any', 'everywhere', 'around', 'empty', \"isn't\", 'anyway', 'shouldn', \"that'll\", 'nine', 'beyond', 'while', 'whom', 'were', 'top', \"she's\", 'interest', 'show', 'get', 'ma', 'less', 'between', 'by', 'does', 'herself', 'few', 'above', 'into', 'with', 'six', 'may', 'except', 'eight', \"wasn't\", 'others', \"it's\", 'us', 'both', 'every', 'this', 'when', 'or', 'before']}\n",
      "Run time: 114.15120077133179 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words. 93.038\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    'classify__class_weight': [None, 'balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library),list(stop_words_library)],  ##[list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)]\n",
    "    \"selecter__k\":[5000],\n",
    "    \"vect__ngram_range\":[(1,1)],\n",
    "    # \"vect__binary\": [False]\n",
    "    \"vect__preprocessor\": [preprocess_text,remove_punctuation,None]\n",
    "    #\"vect__binary\": [False]\n",
    "\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyxS67JkkfpL",
    "outputId": "56edfa41-2d4b-43a7-db6f-270284ee4a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "The best accuracy is 93.037.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__class_weight': 'balanced', 'classify__max_iter': 1000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['see', 'fifty', 'several', 'much', 'yet', 'often', \"isn't\", \"shan't\", 'further', 'of', 'together', 'and', 'd', \"needn't\", 'cannot', \"aren't\", 'eight', 'across', 'anything', \"hadn't\", 'con', 'theirs', 'once', 'anyhow', 'twelve', 'those', 'full', 'itself', 'only', 'o', 've', 'in', 'why', 'haven', 'same', 'them', 'give', 'sometime', 'behind', 'enough', 'couldnt', 'becoming', 'already', 'everywhere', 'third', 'hereupon', 'interest', 'just', 'through', 'without', 'except', 'un', 'another', 'but', 'least', 'somewhere', 're', 'perhaps', 'made', 'co', 'hasn', 'mightn', 'didn', 'onto', 'should', 'cant', 'into', 'whatever', 'from', 'since', 'six', 'wherever', 'having', 'everything', 'ltd', 'as', 'because', 'under', 'or', 'hence', 'meanwhile', 'yourself', 'bottom', 'can', 'nine', 'anyway', \"mightn't\", 'him', 'wasn', 'everyone', 'rather', \"it's\", 'becomes', 'cry', 'do', 'ever', 'hundred', 'become', 'on', 'anyone', 'then', 'most', \"you've\", 'will', 'keep', 'else', \"haven't\", 'whoever', 'being', 'during', \"that'll\", \"she's\", 'yours', 'they', 'five', 'whenever', 'seemed', 'did', 'therefore', 'get', 'call', 'up', 'ten', 'your', 'last', 'to', 'seeming', 'every', 'along', 'is', 'be', 'the', 'all', 'either', 'myself', 'never', \"you'd\", 'doesn', 'who', \"won't\", 'amoungst', 'thereby', \"don't\", 'whereafter', 'beyond', 'are', 'thence', 'show', 'although', 'latter', 'thereupon', 'twenty', 'something', 'his', 'side', 'had', 'somehow', 'their', 'nowhere', 'whereupon', 'ie', 'fifteen', 's', \"shouldn't\", 'over', 'after', 'out', 'sincere', 'someone', 'fire', 'each', 't', 'beside', 'etc', 'some', 'nobody', 'shan', \"should've\", 'other', 'about', 'two', 'have', 'done', 'we', 'put', 'one', 'move', 'nothing', 'more', 'yourselves', 'others', 'll', 'among', 'whereby', 'three', 'toward', 'whose', 'an', 'herself', 'towards', \"you'll\", 'might', 'whom', 'isn', 'these', 'though', 'whether', 'no', 'back', 'ain', 'even', 'herein', 'both', 'hereafter', 'am', 'whence', 'whereas', 'bill', 'name', 'part', 'such', 'it', 'wouldn', 'down', 'thereafter', 'if', 'she', 'don', \"didn't\", 'now', 'won', 'besides', 'me', 'own', 'her', 'a', \"wouldn't\", 'hasnt', 'nevertheless', 'nor', 'ours', 'fill', 'he', 'does', 'there', 'between', 'take', 'again', 'not', 'please', 'four', 'almost', 'thick', 'while', 'us', 'alone', 'serious', \"couldn't\", 'throughout', 'top', 'could', 'therein', 'noone', 'forty', 'than', 'first', 'de', 'mine', 'latterly', 'any', 'himself', 'also', 'go', 'amount', 'wherein', 'namely', 'were', 'neither', 'find', 'has', 'before', 'at', 'less', 'may', 'elsewhere', 'couldn', 'above', 'per', 'seems', 'many', 'whole', 'still', 'been', 'so', 'around', \"mustn't\", 'themselves', 'here', 'hereby', 'few', 'off', 'formerly', 'thru', 'sometimes', 'was', 'i', 'eg', 'via', 'well', 'ma', 'empty', 'describe', 'mostly', 'by', 'within', 'with', 'whither', \"wasn't\", 'my', 'doing', 'eleven', 'for', 'upon', 'became', 'moreover', 'thin', 'would', 'below', 'always', 'former', 'mill', 'afterwards', 'too', 'seem', 'amongst', 'anywhere', 'front', 'hadn', 'needn', 'due', 'detail', 'what', 'which', 'y', 'against', 'next', 'otherwise', \"you're\", 'hers', 'very', 'aren', \"hasn't\", 'that', 'm', 'however', 'weren', 'sixty', \"weren't\", 'when', 'beforehand', 'ourselves', 'where', 'you', 'indeed', 'system', \"doesn't\", 'inc', 'shouldn', 'thus', 'until', 'how', 'its', 'mustn', 'found', 'this', 'none', 'our', 'must']}\n",
      "Run time: 110.83977627754211 seconds\n",
      "File saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words. 93.038\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [1000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    'classify__class_weight': [None, 'balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library),list(stop_words_library)],  ##[list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)]\n",
    "    \"selecter__k\":[5000],\n",
    "    \"vect__ngram_range\":[(1,1)],\n",
    "    # \"vect__binary\": [False]\n",
    "    \"vect__preprocessor\": [preprocess_text,remove_punctuation,None]\n",
    "\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "y_pred = grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"LogisticReg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSmPuIMjnVk9",
    "outputId": "dacf5b56-ba60-487a-ffbe-6af26a2b9dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "The best accuracy is 92.481.\n",
      "The winning parameters are {'classify__C': 10.0, 'classify__class_weight': None, 'classify__max_iter': 10000, 'classify__penalty': 'l2', 'classify__solver': 'sag', 'selecter__k': 5000, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['see', 'fifty', 'several', 'much', 'yet', 'often', \"isn't\", \"shan't\", 'further', 'of', 'together', 'and', 'd', \"needn't\", 'cannot', \"aren't\", 'eight', 'across', 'anything', \"hadn't\", 'con', 'theirs', 'once', 'anyhow', 'twelve', 'those', 'full', 'itself', 'only', 'o', 've', 'in', 'why', 'haven', 'same', 'them', 'give', 'sometime', 'behind', 'enough', 'couldnt', 'becoming', 'already', 'everywhere', 'third', 'hereupon', 'interest', 'just', 'through', 'without', 'except', 'un', 'another', 'but', 'least', 'somewhere', 're', 'perhaps', 'made', 'co', 'hasn', 'mightn', 'didn', 'onto', 'should', 'cant', 'into', 'whatever', 'from', 'since', 'six', 'wherever', 'having', 'everything', 'ltd', 'as', 'because', 'under', 'or', 'hence', 'meanwhile', 'yourself', 'bottom', 'can', 'nine', 'anyway', \"mightn't\", 'him', 'wasn', 'everyone', 'rather', \"it's\", 'becomes', 'cry', 'do', 'ever', 'hundred', 'become', 'on', 'anyone', 'then', 'most', \"you've\", 'will', 'keep', 'else', \"haven't\", 'whoever', 'being', 'during', \"that'll\", \"she's\", 'yours', 'they', 'five', 'whenever', 'seemed', 'did', 'therefore', 'get', 'call', 'up', 'ten', 'your', 'last', 'to', 'seeming', 'every', 'along', 'is', 'be', 'the', 'all', 'either', 'myself', 'never', \"you'd\", 'doesn', 'who', \"won't\", 'amoungst', 'thereby', \"don't\", 'whereafter', 'beyond', 'are', 'thence', 'show', 'although', 'latter', 'thereupon', 'twenty', 'something', 'his', 'side', 'had', 'somehow', 'their', 'nowhere', 'whereupon', 'ie', 'fifteen', 's', \"shouldn't\", 'over', 'after', 'out', 'sincere', 'someone', 'fire', 'each', 't', 'beside', 'etc', 'some', 'nobody', 'shan', \"should've\", 'other', 'about', 'two', 'have', 'done', 'we', 'put', 'one', 'move', 'nothing', 'more', 'yourselves', 'others', 'll', 'among', 'whereby', 'three', 'toward', 'whose', 'an', 'herself', 'towards', \"you'll\", 'might', 'whom', 'isn', 'these', 'though', 'whether', 'no', 'back', 'ain', 'even', 'herein', 'both', 'hereafter', 'am', 'whence', 'whereas', 'bill', 'name', 'part', 'such', 'it', 'wouldn', 'down', 'thereafter', 'if', 'she', 'don', \"didn't\", 'now', 'won', 'besides', 'me', 'own', 'her', 'a', \"wouldn't\", 'hasnt', 'nevertheless', 'nor', 'ours', 'fill', 'he', 'does', 'there', 'between', 'take', 'again', 'not', 'please', 'four', 'almost', 'thick', 'while', 'us', 'alone', 'serious', \"couldn't\", 'throughout', 'top', 'could', 'therein', 'noone', 'forty', 'than', 'first', 'de', 'mine', 'latterly', 'any', 'himself', 'also', 'go', 'amount', 'wherein', 'namely', 'were', 'neither', 'find', 'has', 'before', 'at', 'less', 'may', 'elsewhere', 'couldn', 'above', 'per', 'seems', 'many', 'whole', 'still', 'been', 'so', 'around', \"mustn't\", 'themselves', 'here', 'hereby', 'few', 'off', 'formerly', 'thru', 'sometimes', 'was', 'i', 'eg', 'via', 'well', 'ma', 'empty', 'describe', 'mostly', 'by', 'within', 'with', 'whither', \"wasn't\", 'my', 'doing', 'eleven', 'for', 'upon', 'became', 'moreover', 'thin', 'would', 'below', 'always', 'former', 'mill', 'afterwards', 'too', 'seem', 'amongst', 'anywhere', 'front', 'hadn', 'needn', 'due', 'detail', 'what', 'which', 'y', 'against', 'next', 'otherwise', \"you're\", 'hers', 'very', 'aren', \"hasn't\", 'that', 'm', 'however', 'weren', 'sixty', \"weren't\", 'when', 'beforehand', 'ourselves', 'where', 'you', 'indeed', 'system', \"doesn't\", 'inc', 'shouldn', 'thus', 'until', 'how', 'its', 'mustn', 'found', 'this', 'none', 'our', 'must']}\n",
      "Run time: 67.39258456230164 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training with stop words. 93.038\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__penalty': ['l2'],   #'classify__penalty': ['l1', 'l2'],\n",
    "    'classify__C': [10.0],     #'classify__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classify__solver': ['sag'],   #'classify__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'classify__max_iter': [10000], # 'classify__max_iter': [100, 500, 1000],\n",
    "    'classify__class_weight': [None, 'balanced'],     #'classify__class_weight': [None, 'balanced'],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library),list(stop_words_library)],  ##[list(stop_words_nltk), list(stop_words_sklearn), list(stop_words_library)]\n",
    "    \"selecter__k\":[5000],\n",
    "    \"vect__ngram_range\":[(1,1)],\n",
    "    \"vect__binary\": [False],\n",
    "   \n",
    "    \n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = LogisticRegression()\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     [(\"vect\", vectorizer),(\"selecter\", selecter),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#y_pred = grid.predict(test_x)\n",
    "#create_test_csv(y_pred,\"LogisticReg.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
