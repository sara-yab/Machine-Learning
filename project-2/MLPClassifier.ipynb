{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXEl2ep9cMy6",
    "outputId": "b5cd3288-77a9-4cff-b63e-d95f3396acb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh4Rncx_cQho",
    "outputId": "fca1609a-184d-4f12-8a8f-9d9822e06f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "el1eg8tdcTVL"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JuT3IHQ_cpoG"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nZcGyPMzcU1c"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pah6DSKHe7xc"
   },
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "   translator = str.maketrans('', '', string.punctuation)\n",
    "   text = text.translate(translator)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2pKmqofAmZUw"
   },
   "outputs": [],
   "source": [
    "#remove numeric values,lowercase words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ADnjdUOU36o_"
   },
   "outputs": [],
   "source": [
    "def print_best_params(grid):\n",
    "  bestParameters = grid.best_estimator_.get_params()\n",
    "  for paramName in sorted(bestParameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (paramName, bestParameters[paramName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dK-qvvi_Oa26"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6MkCaFiKv0ys"
   },
   "outputs": [],
   "source": [
    "#stemmer lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCei6Xj02RaG",
    "outputId": "248483cf-b232-49f4-98d6-fb043fe8620d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length custom: 590\n"
     ]
    }
   ],
   "source": [
    "stop_words_custom = [\n",
    "    # All pronouns and associated words\n",
    "    \"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\n",
    "    \"you'll\",\n",
    "    \"you'd\",\n",
    "    \"you're\",\n",
    "    \"you've\",\n",
    "    \"yourself\",\n",
    "    \"he\",\n",
    "    \"he'll\",\n",
    "    \"he'd\",\n",
    "    \"he's\",\n",
    "    \"him\",\n",
    "    \"she\",\n",
    "    \"she'll\",\n",
    "    \"she'd\",\n",
    "    \"she's\",\n",
    "    \"her\",\n",
    "    \"it\",\n",
    "    \"it'll\",\n",
    "    \"it'd\",\n",
    "    \"it's\",\n",
    "    \"itself\",\n",
    "    \"oneself\",\n",
    "    \"we\",\n",
    "    \"we'll\",\n",
    "    \"we'd\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"us\",\n",
    "    \"ourselves\",\n",
    "    \"they\",\n",
    "    \"they'll\",\n",
    "    \"they'd\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"them\",\n",
    "    \"themselves\",            \n",
    "    \"everyone\",\n",
    "    \"everyone's\",\n",
    "    \"everybody\",\n",
    "    \"everybody's\",\n",
    "    \"someone\",\n",
    "    \"someone's\",\n",
    "    \"somebody\",\n",
    "    \"somebody's\",\n",
    "    \"nobody\",\n",
    "    \"nobody's\",\n",
    "    \"anyone\",\n",
    "    \"anyone's\",\n",
    "    \"everything\",\n",
    "    \"everything's\",\n",
    "    \"something\",\n",
    "    \"something's\",\n",
    "    \"nothing\",\n",
    "    \"nothing's\",\n",
    "    \"anything\",\n",
    "    \"anything's\",\n",
    "    # All determiners and associated words\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"that's\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"my\",\n",
    "    #\"mine\",   #Omitted since mine can refer to something else\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"his\",\n",
    "    \"hers\",\n",
    "    \"its\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"own\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"few\",\n",
    "    \"much\",\n",
    "    \"many\",\n",
    "    \"lot\",\n",
    "    \"lots\",\n",
    "    \"some\",\n",
    "    \"any\",\n",
    "    \"enough\",\n",
    "    \"all\",\n",
    "    \"both\",\n",
    "    \"half\",\n",
    "    \"either\",\n",
    "    \"neither\",\n",
    "    \"each\",\n",
    "    \"every\",\n",
    "    \"certain\",\n",
    "    \"other\",\n",
    "    \"another\",\n",
    "    \"such\",\n",
    "    \"several\",\n",
    "    \"multiple\",\n",
    "    # \"what\",    #Dealt with later on\n",
    "    \"rather\",\n",
    "    \"quite\",\n",
    "    # All prepositions\n",
    "    \"aboard\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"across\",\n",
    "    \"after\",\n",
    "    \"against\",\n",
    "    \"along\",\n",
    "    \"amid\",\n",
    "    \"amidst\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"anti\",\n",
    "    \"around\",\n",
    "    \"as\",\n",
    "    \"at\",\n",
    "    \"away\",\n",
    "    \"before\",\n",
    "    \"behind\",\n",
    "    \"below\",\n",
    "    \"beneath\",\n",
    "    \"beside\",\n",
    "    \"besides\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"but\",\n",
    "    \"by\",\n",
    "    \"concerning\",\n",
    "    \"considering\",\n",
    "    \"despite\",\n",
    "    \"down\",\n",
    "    \"during\",\n",
    "    \"except\",\n",
    "    \"excepting\",\n",
    "    \"excluding\",\n",
    "    \"far\",\n",
    "    \"following\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"here\",\n",
    "    \"here's\",\n",
    "    \"in\",\n",
    "    \"inside\",\n",
    "    \"into\",\n",
    "    \"left\",\n",
    "    \"like\",\n",
    "    \"minus\",\n",
    "    \"near\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"onto\",\n",
    "    \"opposite\",\n",
    "    \"out\",\n",
    "    \"outside\",\n",
    "    \"over\",\n",
    "    \"past\",\n",
    "    \"per\",\n",
    "    \"plus\",\n",
    "    \"regarding\",\n",
    "    \"right\",\n",
    "    #\"round\",   #Omitted\n",
    "    #\"save\",    #Omitted\n",
    "    \"since\",\n",
    "    \"than\",\n",
    "    \"there\",\n",
    "    \"there's\",\n",
    "    \"through\",\n",
    "    \"to\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"under\",\n",
    "    \"underneath\",\n",
    "    \"unlike\",\n",
    "    \"until\",\n",
    "    \"up\",\n",
    "    \"upon\",\n",
    "    \"versus\",\n",
    "    \"via\",\n",
    "    \"with\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    # Irrelevant verbs\n",
    "    \"may\",\n",
    "    \"might\",\n",
    "    \"will\",\n",
    "    \"won't\",\n",
    "    \"would\",\n",
    "    \"wouldn't\",\n",
    "    \"can\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"could\",\n",
    "    \"couldn't\",\n",
    "    \"should\",\n",
    "    \"shouldn't\",\n",
    "    \"must\",\n",
    "    \"must've\",\n",
    "    \"be\",\n",
    "    \"being\",\n",
    "    \"been\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"aren't\",\n",
    "    \"ain't\",\n",
    "    \"is\",\n",
    "    \"isn't\",\n",
    "    \"was\",\n",
    "    \"wasn't\",\n",
    "    \"were\",\n",
    "    \"weren't\",\n",
    "    \"do\",\n",
    "    \"doing\",\n",
    "    \"don't\",\n",
    "    \"does\",\n",
    "    \"doesn't\",\n",
    "    \"did\",\n",
    "    \"didn't\",\n",
    "    \"done\",\n",
    "    \"have\",\n",
    "    \"haven't\",\n",
    "    \"having\",\n",
    "    \"has\",\n",
    "    \"hasn't\",\n",
    "    \"had\",\n",
    "    \"hadn't\",\n",
    "    \"get\",\n",
    "    \"getting\",\n",
    "    \"gets\",\n",
    "    \"got\",\n",
    "    \"gotten\",\n",
    "    \"go\",\n",
    "    \"going\",\n",
    "    \"gonna\",\n",
    "    \"goes\",\n",
    "    \"went\",\n",
    "    \"gone\",\n",
    "    \"make\",\n",
    "    \"making\",\n",
    "    \"makes\",\n",
    "    \"made\",\n",
    "    \"take\",\n",
    "    \"taking\",\n",
    "    \"takes\",\n",
    "    \"took\",\n",
    "    \"taken\",\n",
    "    \"need\",\n",
    "    \"needing\",\n",
    "    \"needs\",\n",
    "    \"needed\",\n",
    "    \"use\",\n",
    "    \"using\",\n",
    "    \"uses\",\n",
    "    \"used\",\n",
    "    \"want\",\n",
    "    \"wanna\",\n",
    "    \"wanting\",\n",
    "    \"wants\",\n",
    "    \"let\",\n",
    "    \"lets\",\n",
    "    \"letting\",\n",
    "    \"let's\",\n",
    "    \"suppose\",\n",
    "    \"supposing\",\n",
    "    \"supposes\",\n",
    "    \"supposed\",\n",
    "    \"seem\",\n",
    "    \"seeming\",\n",
    "    \"seems\",\n",
    "    \"seemed\",\n",
    "    \"say\",\n",
    "    \"saying\",\n",
    "    \"says\",\n",
    "    \"said\",\n",
    "    \"know\",\n",
    "    \"knowing\",\n",
    "    \"knows\",\n",
    "    \"knew\",\n",
    "    \"known\",\n",
    "    \"look\",\n",
    "    \"looking\",\n",
    "    \"looked\",\n",
    "    \"think\",\n",
    "    \"thinking\",\n",
    "    \"thinks\",\n",
    "    \"thought\",\n",
    "    \"feel\",\n",
    "    \"feels\",\n",
    "    \"felt\",\n",
    "    \"based\",\n",
    "    \"put\",\n",
    "    \"puts\",\n",
    "    #\"wanted\"   #Omitted since the advective is relevant\n",
    "    # Question words and associated words\n",
    "    \"who\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"who'd\",\n",
    "    \"whoever\",\n",
    "    \"whoever's\",\n",
    "    \"whom\",\n",
    "    \"whomever\",\n",
    "    \"whomever's\",\n",
    "    \"whose\",\n",
    "    \"whosever\",\n",
    "    \"whosever's\",\n",
    "    \"when\",\n",
    "    \"whenever\",\n",
    "    \"which\",\n",
    "    \"whichever\",\n",
    "    \"where\",\n",
    "    \"where's\",\n",
    "    \"where'd\",\n",
    "    \"wherever\",\n",
    "    \"why\",\n",
    "    \"why's\",\n",
    "    \"why'd\",\n",
    "    \"whyever\",\n",
    "    \"what\",\n",
    "    \"what's\",\n",
    "    \"whatever\",\n",
    "    \"whence\",\n",
    "    \"how\",\n",
    "    \"how's\",\n",
    "    \"how'd\",\n",
    "    \"however\",\n",
    "    \"whether\",\n",
    "    \"whatsoever\",\n",
    "    # Connector words and irrelevant adverbs\n",
    "    \"and\",\n",
    "    \"or\",\n",
    "    \"not\",\n",
    "    \"because\",\n",
    "    \"also\",\n",
    "    \"always\",\n",
    "    \"never\",\n",
    "    \"only\",\n",
    "    \"really\",\n",
    "    \"very\",\n",
    "    \"greatly\",\n",
    "    \"extremely\",\n",
    "    \"somewhat\",\n",
    "    \"no\",\n",
    "    \"nope\",\n",
    "    \"nah\",\n",
    "    \"yes\",\n",
    "    \"yep\",\n",
    "    \"yeh\",\n",
    "    \"yeah\",\n",
    "    \"maybe\",\n",
    "    \"perhaps\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"less\",\n",
    "    \"least\",\n",
    "    \"good\",\n",
    "    \"great\",\n",
    "    \"well\",\n",
    "    \"better\",\n",
    "    \"best\",\n",
    "    \"bad\",\n",
    "    \"worse\",\n",
    "    \"worst\",\n",
    "    \"too\",\n",
    "    \"thru\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"yet\",\n",
    "    \"already\",\n",
    "    \"then\",\n",
    "    \"even\",\n",
    "    \"now\",\n",
    "    \"sometimes\",\n",
    "    \"still\",\n",
    "    \"together\",\n",
    "    \"altogether\",\n",
    "    \"entirely\",\n",
    "    \"fully\",\n",
    "    \"entire\",\n",
    "    \"whole\",\n",
    "    \"completely\",\n",
    "    \"utterly\",\n",
    "    \"seemingly\",\n",
    "    \"apparently\",\n",
    "    \"clearly\",\n",
    "    \"obviously\",\n",
    "    \"actually\",\n",
    "    \"actual\",\n",
    "    \"usually\",\n",
    "    \"usual\",\n",
    "    \"literally\",\n",
    "    \"honestly\",\n",
    "    \"absolutely\",\n",
    "    \"definitely\",\n",
    "    \"generally\",\n",
    "    \"totally\",\n",
    "    \"finally\",\n",
    "    \"basically\",\n",
    "    \"essentially\",\n",
    "    \"fundamentally\",\n",
    "    \"automatically\",\n",
    "    \"immediately\",\n",
    "    \"necessarily\",\n",
    "    \"primarily\",\n",
    "    \"normally\",\n",
    "    \"perfectly\",\n",
    "    \"constantly\",\n",
    "    \"particularly\",\n",
    "    \"eventually\",\n",
    "    \"hopefully\",\n",
    "    \"mainly\",\n",
    "    \"typically\",\n",
    "    \"specifically\",\n",
    "    \"differently\",\n",
    "    \"appropriately\",\n",
    "    \"plenty\",\n",
    "    \"certainly\",\n",
    "    \"unfortunately\",\n",
    "    \"ultimately\",\n",
    "    \"unlikely\",\n",
    "    \"likely\",\n",
    "    \"potentially\",\n",
    "    \"fortunately\",\n",
    "    \"personally\",\n",
    "    \"directly\",\n",
    "    \"indirectly\",\n",
    "    \"nearly\",\n",
    "    \"closely\",\n",
    "    \"slightly\",\n",
    "    \"probably\",\n",
    "    \"possibly\",\n",
    "    \"especially\",\n",
    "    \"frequently\",\n",
    "    \"often\",\n",
    "    \"oftentimes\",\n",
    "    \"seldom\",\n",
    "    \"rarely\",\n",
    "    \"sure\",\n",
    "    \"while\",\n",
    "    \"whilst\",\n",
    "    \"able\",\n",
    "    \"unable\",\n",
    "    \"else\",\n",
    "    \"ever\",\n",
    "    \"once\",\n",
    "    \"twice\",\n",
    "    \"thrice\",\n",
    "    \"almost\",\n",
    "    \"again\",\n",
    "    \"instead\",\n",
    "    \"next\",\n",
    "    \"previous\",\n",
    "    \"unless\",\n",
    "    \"somehow\",\n",
    "    \"anyhow\",\n",
    "    \"anywhere\",\n",
    "    \"somewhere\",\n",
    "    \"everywhere\",\n",
    "    \"nowhere\",\n",
    "    \"further\",\n",
    "    \"anymore\",\n",
    "    \"later\",\n",
    "    \"ago\",\n",
    "    \"ahead\",\n",
    "    \"just\",\n",
    "    \"same\",\n",
    "    \"different\",\n",
    "    \"big\",\n",
    "    \"small\",\n",
    "    \"little\",\n",
    "    \"tiny\",\n",
    "    \"large\",\n",
    "    \"huge\",\n",
    "    \"pretty\",\n",
    "    \"mostly\",\n",
    "    \"anyway\",\n",
    "    \"anyways\",\n",
    "    \"otherwise\",\n",
    "    \"regardless\",\n",
    "    \"throughout\",\n",
    "    \"additionally\",\n",
    "    \"moreover\",\n",
    "    \"furthermore\",\n",
    "    \"meanwhile\",\n",
    "    \"afterwards\",\n",
    "    # Irrelevant nouns\n",
    "    \"thing\",\n",
    "    \"thing's\",\n",
    "    \"things\",\n",
    "    \"stuff\",\n",
    "    \"other's\",\n",
    "    \"others\",\n",
    "    \"another's\",\n",
    "    \"total\",\n",
    "    \"\",\n",
    "    \"false\",\n",
    "    \"none\",\n",
    "    \"way\",\n",
    "    \"kind\",\n",
    "    # Lettered numbers and order\n",
    "    \"zero\",\n",
    "    \"zeros\",\n",
    "    \"zeroes\",\n",
    "    \"one\",\n",
    "    \"ones\",\n",
    "    \"two\",\n",
    "    \"three\",\n",
    "    \"four\",\n",
    "    \"five\",\n",
    "    \"six\", \n",
    "    \"seven\",\n",
    "    \"eight\",\n",
    "    \"nine\",\n",
    "    \"ten\",\n",
    "    \"twenty\",\n",
    "    \"thirty\",\n",
    "    \"forty\",\n",
    "    \"fifty\",\n",
    "    \"sixty\",\n",
    "    \"seventy\",\n",
    "    \"eighty\",\n",
    "    \"ninety\",\n",
    "    \"hundred\",\n",
    "    \"hundreds\",\n",
    "    \"thousand\",\n",
    "    \"thousands\",\n",
    "    \"million\",\n",
    "    \"millions\",\n",
    "    \"first\",\n",
    "    \"last\",\n",
    "    \"second\",\n",
    "    \"third\",\n",
    "    \"fourth\",\n",
    "    \"fifth\",\n",
    "    \"sixth\",\n",
    "    \"seventh\",\n",
    "    \"eigth\",\n",
    "    \"ninth\",\n",
    "    \"tenth\",\n",
    "    \"firstly\",\n",
    "    \"secondly\",\n",
    "    \"thirdly\",\n",
    "    \"lastly\",\n",
    "    # Greetings and slang\n",
    "    \"hello\",\n",
    "    \"hi\",\n",
    "    \"hey\",\n",
    "    \"sup\",\n",
    "    \"yo\",\n",
    "    \"greetings\",\n",
    "    \"please\",\n",
    "    \"okay\",\n",
    "    \"ok\",\n",
    "    \"y'all\",\n",
    "    \"lol\",\n",
    "    \"rofl\",\n",
    "    \"thank\",\n",
    "    \"thanks\",\n",
    "    \"alright\",\n",
    "    \"kinda\",\n",
    "    \"dont\",\n",
    "    \"sorry\",\n",
    "    \"idk\",\n",
    "    \"tldr\",\n",
    "    \"tl\",\n",
    "    \"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "    \"tbh\",\n",
    "    \"dude\",\n",
    "    \"tho\",\n",
    "    \"aka\",\n",
    "    \"plz\",\n",
    "    \"pls\",\n",
    "    \"bit\",\n",
    "    \"don\",\n",
    "    # Miscellaneous\n",
    "    \"www\",\n",
    "    \"https\",\n",
    "    \"http\",\n",
    "    \"com\",\n",
    "    \"etc\",\n",
    "    \"html\",\n",
    "    \"reddit\",\n",
    "    \"subreddit\",\n",
    "    \"subreddits\",\n",
    "    \"comments\",\n",
    "    \"reply\",\n",
    "    \"replies\",\n",
    "    \"thread\",\n",
    "    \"threads\",\n",
    "    \"post\",\n",
    "    \"posts\",\n",
    "    \"website\",\n",
    "    \"websites\",\n",
    "    \"web site\",\n",
    "    \"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUIZVtDXOHoj"
   },
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cY1siK8fFbt0",
    "outputId": "9da47804-3b91-4115-9632-78166b1cda8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 94.843.\n",
      "The winning parameters are {'clf__activation': 'relu', 'clf__alpha': 0.1, 'clf__hidden_layer_sizes': (100,), 'clf__max_iter': 2000, 'clf__solver': 'adam', 'selecter__k': 2500, 'tfidf__max_df': 0.5, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etc', 'html', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n",
      "Run time: 78.14167332649231 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "parameters = {\n",
    "   'tfidf__max_df': (0.25, 0.5),\n",
    "    'clf__hidden_layer_sizes': [(100,)],\n",
    "    'selecter__k':[2500],\n",
    "    'tfidf__ngram_range':[(1,1)],\n",
    "    #'clf__learning_rate':['adaptive'],\n",
    "    'clf__activation':[\"relu\"],\n",
    "        'clf__solver':[\"adam\"],\n",
    "    'clf__max_iter':[2000],\n",
    "     \"tfidf__stop_words\": [list(stop_words_custom)],\n",
    "    'clf__alpha': [0.1]\n",
    "}\n",
    "\n",
    "# Define the MLP architecture\n",
    "mlp = MLPClassifier()\n",
    "normalizer = Normalizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    (\"selecter\", selecter),\n",
    "    (\"normalizer\", normalizer),\n",
    "    ('clf', mlp)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHXz-8ktb4S-",
    "outputId": "0333bc11-d139-4e9a-e69d-e5df208f4cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.982\n",
      "Run time: 36.68663692474365 seconds\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "print(round(grid_search.best_score_ * 100,3))\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "y_pred = grid_search.predict(test_x)\n",
    "create_test_csv(y_pred,\"CNN_07032023_01.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p987x4a8OrUZ"
   },
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
