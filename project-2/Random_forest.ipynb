{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eE2zTbfsPnL",
    "outputId": "943b0d64-deda-4cf2-a5e9-42a9b3261614"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvFS2oIVsjy0",
    "outputId": "7c0efce1-a9e0-4cae-9f66-8919bea2d535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf0HoqmQsrwf"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1hJvgBCsuBs"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6HzYUjTsxHd"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "#train_data = train_data.sample(500).reset_index(drop=True)\n",
    "#train_data = train_data.head(200)\n",
    "\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXX2L9SPs3TR",
    "outputId": "bfcf7845-9273-4fc9-e015-0ad1fb119271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there /u/LakotaPride! Welcome to /r/Trump.  [](/sp)    \n",
      " \n",
      "Thank you for posting on r/Trump Please follow all rules and guidelines. Inform the mods if you have any concerns. [](/sp) Join our live [discord](https://discord.gg/kh4Wv9DavE) chat to talk to your fellow patriots! If you have any issues please reach out.\n",
      "\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/trump) if you have any questions or concerns.*\n"
     ]
    }
   ],
   "source": [
    "print(train_x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpT6sjHAtYNZ",
    "outputId": "f42b24ed-94a1-49f6-fd71-00d7ae72ba75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length custom: 590\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n",
    "\n",
    "stop_words_custom = [\n",
    "    # All pronouns and associated words\n",
    "    \"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\n",
    "    \"you'll\",\n",
    "    \"you'd\",\n",
    "    \"you're\",\n",
    "    \"you've\",\n",
    "    \"yourself\",\n",
    "    \"he\",\n",
    "    \"he'll\",\n",
    "    \"he'd\",\n",
    "    \"he's\",\n",
    "    \"him\",\n",
    "    \"she\",\n",
    "    \"she'll\",\n",
    "    \"she'd\",\n",
    "    \"she's\",\n",
    "    \"her\",\n",
    "    \"it\",\n",
    "    \"it'll\",\n",
    "    \"it'd\",\n",
    "    \"it's\",\n",
    "    \"itself\",\n",
    "    \"oneself\",\n",
    "    \"we\",\n",
    "    \"we'll\",\n",
    "    \"we'd\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"us\",\n",
    "    \"ourselves\",\n",
    "    \"they\",\n",
    "    \"they'll\",\n",
    "    \"they'd\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"them\",\n",
    "    \"themselves\",            \n",
    "    \"everyone\",\n",
    "    \"everyone's\",\n",
    "    \"everybody\",\n",
    "    \"everybody's\",\n",
    "    \"someone\",\n",
    "    \"someone's\",\n",
    "    \"somebody\",\n",
    "    \"somebody's\",\n",
    "    \"nobody\",\n",
    "    \"nobody's\",\n",
    "    \"anyone\",\n",
    "    \"anyone's\",\n",
    "    \"everything\",\n",
    "    \"everything's\",\n",
    "    \"something\",\n",
    "    \"something's\",\n",
    "    \"nothing\",\n",
    "    \"nothing's\",\n",
    "    \"anything\",\n",
    "    \"anything's\",\n",
    "    # All determiners and associated words\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"that's\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"my\",\n",
    "    #\"mine\",   #Omitted since mine can refer to something else\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"his\",\n",
    "    \"hers\",\n",
    "    \"its\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"own\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"few\",\n",
    "    \"much\",\n",
    "    \"many\",\n",
    "    \"lot\",\n",
    "    \"lots\",\n",
    "    \"some\",\n",
    "    \"any\",\n",
    "    \"enough\",\n",
    "    \"all\",\n",
    "    \"both\",\n",
    "    \"half\",\n",
    "    \"either\",\n",
    "    \"neither\",\n",
    "    \"each\",\n",
    "    \"every\",\n",
    "    \"certain\",\n",
    "    \"other\",\n",
    "    \"another\",\n",
    "    \"such\",\n",
    "    \"several\",\n",
    "    \"multiple\",\n",
    "    # \"what\",    #Dealt with later on\n",
    "    \"rather\",\n",
    "    \"quite\",\n",
    "    # All prepositions\n",
    "    \"aboard\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"across\",\n",
    "    \"after\",\n",
    "    \"against\",\n",
    "    \"along\",\n",
    "    \"amid\",\n",
    "    \"amidst\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"anti\",\n",
    "    \"around\",\n",
    "    \"as\",\n",
    "    \"at\",\n",
    "    \"away\",\n",
    "    \"before\",\n",
    "    \"behind\",\n",
    "    \"below\",\n",
    "    \"beneath\",\n",
    "    \"beside\",\n",
    "    \"besides\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"but\",\n",
    "    \"by\",\n",
    "    \"concerning\",\n",
    "    \"considering\",\n",
    "    \"despite\",\n",
    "    \"down\",\n",
    "    \"during\",\n",
    "    \"except\",\n",
    "    \"excepting\",\n",
    "    \"excluding\",\n",
    "    \"far\",\n",
    "    \"following\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"here\",\n",
    "    \"here's\",\n",
    "    \"in\",\n",
    "    \"inside\",\n",
    "    \"into\",\n",
    "    \"left\",\n",
    "    \"like\",\n",
    "    \"minus\",\n",
    "    \"near\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"onto\",\n",
    "    \"opposite\",\n",
    "    \"out\",\n",
    "    \"outside\",\n",
    "    \"over\",\n",
    "    \"past\",\n",
    "    \"per\",\n",
    "    \"plus\",\n",
    "    \"regarding\",\n",
    "    \"right\",\n",
    "    #\"round\",   #Omitted\n",
    "    #\"save\",    #Omitted\n",
    "    \"since\",\n",
    "    \"than\",\n",
    "    \"there\",\n",
    "    \"there's\",\n",
    "    \"through\",\n",
    "    \"to\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"under\",\n",
    "    \"underneath\",\n",
    "    \"unlike\",\n",
    "    \"until\",\n",
    "    \"up\",\n",
    "    \"upon\",\n",
    "    \"versus\",\n",
    "    \"via\",\n",
    "    \"with\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    # Irrelevant verbs\n",
    "    \"may\",\n",
    "    \"might\",\n",
    "    \"will\",\n",
    "    \"won't\",\n",
    "    \"would\",\n",
    "    \"wouldn't\",\n",
    "    \"can\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"could\",\n",
    "    \"couldn't\",\n",
    "    \"should\",\n",
    "    \"shouldn't\",\n",
    "    \"must\",\n",
    "    \"must've\",\n",
    "    \"be\",\n",
    "    \"being\",\n",
    "    \"been\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"aren't\",\n",
    "    \"ain't\",\n",
    "    \"is\",\n",
    "    \"isn't\",\n",
    "    \"was\",\n",
    "    \"wasn't\",\n",
    "    \"were\",\n",
    "    \"weren't\",\n",
    "    \"do\",\n",
    "    \"doing\",\n",
    "    \"don't\",\n",
    "    \"does\",\n",
    "    \"doesn't\",\n",
    "    \"did\",\n",
    "    \"didn't\",\n",
    "    \"done\",\n",
    "    \"have\",\n",
    "    \"haven't\",\n",
    "    \"having\",\n",
    "    \"has\",\n",
    "    \"hasn't\",\n",
    "    \"had\",\n",
    "    \"hadn't\",\n",
    "    \"get\",\n",
    "    \"getting\",\n",
    "    \"gets\",\n",
    "    \"got\",\n",
    "    \"gotten\",\n",
    "    \"go\",\n",
    "    \"going\",\n",
    "    \"gonna\",\n",
    "    \"goes\",\n",
    "    \"went\",\n",
    "    \"gone\",\n",
    "    \"make\",\n",
    "    \"making\",\n",
    "    \"makes\",\n",
    "    \"made\",\n",
    "    \"take\",\n",
    "    \"taking\",\n",
    "    \"takes\",\n",
    "    \"took\",\n",
    "    \"taken\",\n",
    "    \"need\",\n",
    "    \"needing\",\n",
    "    \"needs\",\n",
    "    \"needed\",\n",
    "    \"use\",\n",
    "    \"using\",\n",
    "    \"uses\",\n",
    "    \"used\",\n",
    "    \"want\",\n",
    "    \"wanna\",\n",
    "    \"wanting\",\n",
    "    \"wants\",\n",
    "    \"let\",\n",
    "    \"lets\",\n",
    "    \"letting\",\n",
    "    \"let's\",\n",
    "    \"suppose\",\n",
    "    \"supposing\",\n",
    "    \"supposes\",\n",
    "    \"supposed\",\n",
    "    \"seem\",\n",
    "    \"seeming\",\n",
    "    \"seems\",\n",
    "    \"seemed\",\n",
    "    \"say\",\n",
    "    \"saying\",\n",
    "    \"says\",\n",
    "    \"said\",\n",
    "    \"know\",\n",
    "    \"knowing\",\n",
    "    \"knows\",\n",
    "    \"knew\",\n",
    "    \"known\",\n",
    "    \"look\",\n",
    "    \"looking\",\n",
    "    \"looked\",\n",
    "    \"think\",\n",
    "    \"thinking\",\n",
    "    \"thinks\",\n",
    "    \"thought\",\n",
    "    \"feel\",\n",
    "    \"feels\",\n",
    "    \"felt\",\n",
    "    \"based\",\n",
    "    \"put\",\n",
    "    \"puts\",\n",
    "    #\"wanted\"   #Omitted since the advective is relevant\n",
    "    # Question words and associated words\n",
    "    \"who\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"who'd\",\n",
    "    \"whoever\",\n",
    "    \"whoever's\",\n",
    "    \"whom\",\n",
    "    \"whomever\",\n",
    "    \"whomever's\",\n",
    "    \"whose\",\n",
    "    \"whosever\",\n",
    "    \"whosever's\",\n",
    "    \"when\",\n",
    "    \"whenever\",\n",
    "    \"which\",\n",
    "    \"whichever\",\n",
    "    \"where\",\n",
    "    \"where's\",\n",
    "    \"where'd\",\n",
    "    \"wherever\",\n",
    "    \"why\",\n",
    "    \"why's\",\n",
    "    \"why'd\",\n",
    "    \"whyever\",\n",
    "    \"what\",\n",
    "    \"what's\",\n",
    "    \"whatever\",\n",
    "    \"whence\",\n",
    "    \"how\",\n",
    "    \"how's\",\n",
    "    \"how'd\",\n",
    "    \"however\",\n",
    "    \"whether\",\n",
    "    \"whatsoever\",\n",
    "    # Connector words and irrelevant adverbs\n",
    "    \"and\",\n",
    "    \"or\",\n",
    "    \"not\",\n",
    "    \"because\",\n",
    "    \"also\",\n",
    "    \"always\",\n",
    "    \"never\",\n",
    "    \"only\",\n",
    "    \"really\",\n",
    "    \"very\",\n",
    "    \"greatly\",\n",
    "    \"extremely\",\n",
    "    \"somewhat\",\n",
    "    \"no\",\n",
    "    \"nope\",\n",
    "    \"nah\",\n",
    "    \"yes\",\n",
    "    \"yep\",\n",
    "    \"yeh\",\n",
    "    \"yeah\",\n",
    "    \"maybe\",\n",
    "    \"perhaps\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"less\",\n",
    "    \"least\",\n",
    "    \"good\",\n",
    "    \"great\",\n",
    "    \"well\",\n",
    "    \"better\",\n",
    "    \"best\",\n",
    "    \"bad\",\n",
    "    \"worse\",\n",
    "    \"worst\",\n",
    "    \"too\",\n",
    "    \"thru\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"yet\",\n",
    "    \"already\",\n",
    "    \"then\",\n",
    "    \"even\",\n",
    "    \"now\",\n",
    "    \"sometimes\",\n",
    "    \"still\",\n",
    "    \"together\",\n",
    "    \"altogether\",\n",
    "    \"entirely\",\n",
    "    \"fully\",\n",
    "    \"entire\",\n",
    "    \"whole\",\n",
    "    \"completely\",\n",
    "    \"utterly\",\n",
    "    \"seemingly\",\n",
    "    \"apparently\",\n",
    "    \"clearly\",\n",
    "    \"obviously\",\n",
    "    \"actually\",\n",
    "    \"actual\",\n",
    "    \"usually\",\n",
    "    \"usual\",\n",
    "    \"literally\",\n",
    "    \"honestly\",\n",
    "    \"absolutely\",\n",
    "    \"definitely\",\n",
    "    \"generally\",\n",
    "    \"totally\",\n",
    "    \"finally\",\n",
    "    \"basically\",\n",
    "    \"essentially\",\n",
    "    \"fundamentally\",\n",
    "    \"automatically\",\n",
    "    \"immediately\",\n",
    "    \"necessarily\",\n",
    "    \"primarily\",\n",
    "    \"normally\",\n",
    "    \"perfectly\",\n",
    "    \"constantly\",\n",
    "    \"particularly\",\n",
    "    \"eventually\",\n",
    "    \"hopefully\",\n",
    "    \"mainly\",\n",
    "    \"typically\",\n",
    "    \"specifically\",\n",
    "    \"differently\",\n",
    "    \"appropriately\",\n",
    "    \"plenty\",\n",
    "    \"certainly\",\n",
    "    \"unfortunately\",\n",
    "    \"ultimately\",\n",
    "    \"unlikely\",\n",
    "    \"likely\",\n",
    "    \"potentially\",\n",
    "    \"fortunately\",\n",
    "    \"personally\",\n",
    "    \"directly\",\n",
    "    \"indirectly\",\n",
    "    \"nearly\",\n",
    "    \"closely\",\n",
    "    \"slightly\",\n",
    "    \"probably\",\n",
    "    \"possibly\",\n",
    "    \"especially\",\n",
    "    \"frequently\",\n",
    "    \"often\",\n",
    "    \"oftentimes\",\n",
    "    \"seldom\",\n",
    "    \"rarely\",\n",
    "    \"sure\",\n",
    "    \"while\",\n",
    "    \"whilst\",\n",
    "    \"able\",\n",
    "    \"unable\",\n",
    "    \"else\",\n",
    "    \"ever\",\n",
    "    \"once\",\n",
    "    \"twice\",\n",
    "    \"thrice\",\n",
    "    \"almost\",\n",
    "    \"again\",\n",
    "    \"instead\",\n",
    "    \"next\",\n",
    "    \"previous\",\n",
    "    \"unless\",\n",
    "    \"somehow\",\n",
    "    \"anyhow\",\n",
    "    \"anywhere\",\n",
    "    \"somewhere\",\n",
    "    \"everywhere\",\n",
    "    \"nowhere\",\n",
    "    \"further\",\n",
    "    \"anymore\",\n",
    "    \"later\",\n",
    "    \"ago\",\n",
    "    \"ahead\",\n",
    "    \"just\",\n",
    "    \"same\",\n",
    "    \"different\",\n",
    "    \"big\",\n",
    "    \"small\",\n",
    "    \"little\",\n",
    "    \"tiny\",\n",
    "    \"large\",\n",
    "    \"huge\",\n",
    "    \"pretty\",\n",
    "    \"mostly\",\n",
    "    \"anyway\",\n",
    "    \"anyways\",\n",
    "    \"otherwise\",\n",
    "    \"regardless\",\n",
    "    \"throughout\",\n",
    "    \"additionally\",\n",
    "    \"moreover\",\n",
    "    \"furthermore\",\n",
    "    \"meanwhile\",\n",
    "    \"afterwards\",\n",
    "    # Irrelevant nouns\n",
    "    \"thing\",\n",
    "    \"thing's\",\n",
    "    \"things\",\n",
    "    \"stuff\",\n",
    "    \"other's\",\n",
    "    \"others\",\n",
    "    \"another's\",\n",
    "    \"total\",\n",
    "    \"\",\n",
    "    \"false\",\n",
    "    \"none\",\n",
    "    \"way\",\n",
    "    \"kind\",\n",
    "    # Lettered numbers and order\n",
    "    \"zero\",\n",
    "    \"zeros\",\n",
    "    \"zeroes\",\n",
    "    \"one\",\n",
    "    \"ones\",\n",
    "    \"two\",\n",
    "    \"three\",\n",
    "    \"four\",\n",
    "    \"five\",\n",
    "    \"six\", \n",
    "    \"seven\",\n",
    "    \"eight\",\n",
    "    \"nine\",\n",
    "    \"ten\",\n",
    "    \"twenty\",\n",
    "    \"thirty\",\n",
    "    \"forty\",\n",
    "    \"fifty\",\n",
    "    \"sixty\",\n",
    "    \"seventy\",\n",
    "    \"eighty\",\n",
    "    \"ninety\",\n",
    "    \"hundred\",\n",
    "    \"hundreds\",\n",
    "    \"thousand\",\n",
    "    \"thousands\",\n",
    "    \"million\",\n",
    "    \"millions\",\n",
    "    \"first\",\n",
    "    \"last\",\n",
    "    \"second\",\n",
    "    \"third\",\n",
    "    \"fourth\",\n",
    "    \"fifth\",\n",
    "    \"sixth\",\n",
    "    \"seventh\",\n",
    "    \"eigth\",\n",
    "    \"ninth\",\n",
    "    \"tenth\",\n",
    "    \"firstly\",\n",
    "    \"secondly\",\n",
    "    \"thirdly\",\n",
    "    \"lastly\",\n",
    "    # Greetings and slang\n",
    "    \"hello\",\n",
    "    \"hi\",\n",
    "    \"hey\",\n",
    "    \"sup\",\n",
    "    \"yo\",\n",
    "    \"greetings\",\n",
    "    \"please\",\n",
    "    \"okay\",\n",
    "    \"ok\",\n",
    "    \"y'all\",\n",
    "    \"lol\",\n",
    "    \"rofl\",\n",
    "    \"thank\",\n",
    "    \"thanks\",\n",
    "    \"alright\",\n",
    "    \"kinda\",\n",
    "    \"dont\",\n",
    "    \"sorry\",\n",
    "    \"idk\",\n",
    "    \"tldr\",\n",
    "    \"tl\",\n",
    "    \"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "    \"tbh\",\n",
    "    \"dude\",\n",
    "    \"tho\",\n",
    "    \"aka\",\n",
    "    \"plz\",\n",
    "    \"pls\",\n",
    "    \"bit\",\n",
    "    \"don\",\n",
    "    # Miscellaneous\n",
    "    \"www\",\n",
    "    \"https\",\n",
    "    \"http\",\n",
    "    \"com\",\n",
    "    \"etc\",\n",
    "    \"html\",\n",
    "    \"reddit\",\n",
    "    \"subreddit\",\n",
    "    \"subreddits\",\n",
    "    \"comments\",\n",
    "    \"reply\",\n",
    "    \"replies\",\n",
    "    \"thread\",\n",
    "    \"threads\",\n",
    "    \"post\",\n",
    "    \"posts\",\n",
    "    \"website\",\n",
    "    \"websites\",\n",
    "    \"web site\",\n",
    "    \"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlwwyGtmu-aT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIBYk0WYtFK6"
   },
   "outputs": [],
   "source": [
    "#stem lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZbbYJxy0R-C",
    "outputId": "8694117b-d0d5-485d-8908-e4344840dd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "The best accuracy is 89.274.\n",
      "The winning parameters are {'classify__n_estimators': 100, 'normalizer__norm': 'max', 'selecter__k': 3000, 'vect__binary': True, 'vect__stop_words': ['top', 'them', 'however', 'together', 'sixty', 'such', 'elsewhere', 'done', 'for', 'please', \"needn't\", 'are', 'and', 'the', 'did', 'during', 'as', 'alone', 'though', 'yourselves', 'same', 'bottom', 'should', 'upon', 'themselves', 'at', 'himself', 'll', \"haven't\", 'ma', 'hasnt', 've', 'always', 'off', 'beforehand', 'i', 'any', 'whose', 'around', 'your', 'along', 'yours', \"you're\", 'hereby', 'don', 'he', 'mightn', 'yourself', 'ain', 'often', 'some', 'we', 'mustn', 'thereupon', 'per', 'even', 'since', 'will', 'couldnt', 'that', 'someone', \"you'll\", 'anything', 'then', 'etc', 'few', 'other', 'those', 'never', 'once', 'ourselves', 'hadn', 'system', 'take', 'now', 'under', 'who', 'seemed', 'had', 'well', 'whatever', 'weren', 'hers', 'nothing', 'next', \"don't\", 'aren', 'needn', 'its', 'somewhere', 'up', \"wasn't\", 'side', 'above', 'third', 'full', 'until', 'not', 'fifteen', 'fill', 'hence', 'or', 'rather', 'five', 'put', \"wouldn't\", 'seeming', 'only', 'after', 'theirs', 'one', \"shan't\", 'o', 'just', 'thru', 's', 'being', 'latter', 'amount', 'whereafter', 'front', 'itself', 'fifty', 'within', \"she's\", 'these', 'me', 'were', \"doesn't\", 'against', 'mill', 'whoever', 'thereby', 'wherein', \"isn't\", 'con', 'twenty', 'anyone', 'least', 'via', 'an', 'nine', \"it's\", 'seem', 'nevertheless', 'she', 'our', 'am', 'ever', 'thin', 'whence', 'how', 'hasn', 'whereupon', 'amoungst', \"hadn't\", 'was', 'm', 'him', 'while', 'many', 'too', 'into', 'herself', 'three', 'neither', 'can', 'hereupon', 'but', 'get', \"should've\", 'inc', 'nobody', 'two', 'on', 'further', 'whom', 'whither', 'their', 'afterwards', 'toward', 'ltd', 'everywhere', 'out', 'due', 'whenever', 'might', 'less', 'mine', 'several', 'of', 'four', 'name', \"weren't\", 'they', 'with', 'having', 'hereafter', 'forty', 'eleven', 'shan', 'over', 'have', 'herein', 'go', 'formerly', 'my', 'also', 'another', 'where', 'else', \"you've\", 'anywhere', 'describe', 'yet', 't', 'none', 'there', 'thence', 'without', 'ie', 'whether', 'besides', 'except', 'y', 'every', 'shouldn', 'why', 'indeed', 'otherwise', \"mustn't\", 'meanwhile', 'before', 'somehow', 'which', 'so', 'more', 'when', 'cant', 'twelve', 'didn', 'if', 'very', 'see', 'beside', 'mostly', \"won't\", 'doing', 'from', 'again', 'first', 'nor', 'nowhere', \"aren't\", 'seems', 'much', \"hasn't\", 'becoming', 'find', 'ours', 'a', 'eight', 'thus', 'everything', 'this', 'un', 'isn', 'either', 'give', 'may', 'below', 'move', 'about', 'both', \"didn't\", 'anyway', 'own', 'cry', 'couldn', 'no', 'eg', 'empty', 'must', 'haven', 'be', 'sometimes', 'ten', 'show', 'here', 'interest', 'what', 'co', 'doesn', 'd', 'between', 'de', 'her', 'made', 'namely', 'won', 'almost', 'hundred', 'across', 'fire', 'latterly', 'cannot', 'whole', 'do', 'among', 'his', 're', 'all', 'became', 'been', 'onto', 'than', 'would', 'moreover', 'becomes', 'although', 'still', \"mightn't\", 'most', 'to', 'beyond', 'former', 'thick', 'each', 'does', \"couldn't\", 'sometime', 'found', 'could', 'it', 'us', 'in', 'sincere', 'behind', 'everyone', 'last', \"you'd\", 'bill', 'whereas', 'detail', 'through', 'thereafter', 'noone', 'wasn', 'therein', 'six', 'towards', 'anyhow', 'has', 'call', 'myself', 'become', 'therefore', 'whereby', 'keep', 'down', 'by', 'something', 'wouldn', 'already', 'amongst', 'is', 'back', 'you', \"shouldn't\", 'part', 'enough', 'wherever', 'perhaps', \"that'll\", 'because', 'others', 'serious', 'throughout']}\n",
      "Run time: 28.590136528015137 seconds\n"
     ]
    }
   ],
   "source": [
    "# best model\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False,True],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"normalizer__norm\": ['l2','l1','max'],\n",
    "    'classify__n_estimators':[100]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter),(\"normalizer\",normalizer),(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Y-vgdnU0dFM",
    "outputId": "1ccef3d5-b941-4182-d2e8-6b8e1af9385c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"random_forest_06032023_01.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
