{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXEl2ep9cMy6",
    "outputId": "8524b92f-2f15-4146-8a54-9fc06186c634"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh4Rncx_cQho",
    "outputId": "9313014c-a5e6-45b7-ece0-3b26bd39bf83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el1eg8tdcTVL"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuT3IHQ_cpoG"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZcGyPMzcU1c"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pKmqofAmZUw"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK-qvvi_Oa26"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = list(stop_words_sklearn.union(stop_words_nltk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUIZVtDXOHoj"
   },
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec3yRFyGOpZo",
    "outputId": "79ce24bb-3aac-47a5-c210-5473e06073f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "The best accuracy is 86.072.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 5}\n",
      "Run time: 45.189074993133545 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training of DecisionTree\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__max_depth': [10, 50, 100, None],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SphmHjjY2Wcs",
    "outputId": "e135acf2-8502-4e8e-ee41-20cb5330260a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length custom: 589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words_custom = [\n",
    "# All pronouns and associated words\n",
    "\"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\"you'll\",\"you'd\",\"you're\",\"you've\",\"yourself\",\"he\",\"he'll\",\n",
    "\"he'd\",\n",
    "\"he's\",\n",
    "\"him\",\n",
    "\"she\",\n",
    "\"she'll\",\n",
    "\"she'd\",\n",
    "\"she's\",\n",
    "\"her\",\n",
    "\"it\",\n",
    "\"it'll\",\n",
    "\"it'd\",\n",
    "\"it's\",\n",
    "\"itself\",\n",
    "\"oneself\",\n",
    "\"we\",\n",
    "\"we'll\",\n",
    "\"we'd\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"us\",\n",
    "\"ourselves\",\n",
    "\"they\",\n",
    "\"they'll\",\n",
    "\"they'd\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"them\",\n",
    "\"themselves\",\n",
    "\"everyone\",\n",
    "\"everyone's\",\n",
    "\"everybody\",\n",
    "\"everybody's\",\n",
    "\"someone\",\n",
    "\"someone's\",\n",
    "\"somebody\",\n",
    "\"somebody's\",\n",
    "\"nobody\",\n",
    "\"nobody's\",\n",
    "\"anyone\",\n",
    "\"anyone's\",\n",
    "\"everything\",\n",
    "\"everything's\",\n",
    "\"something\",\n",
    "\"something's\",\n",
    "\"nothing\",\n",
    "\"nothing's\",\n",
    "\"anything\",\n",
    "\"anything's\",\n",
    "# All determiners and associated words\n",
    "\"a\",\n",
    "\"an\",\n",
    "\"the\",\n",
    "\"this\",\n",
    "\"that\",\n",
    "\"that's\",\n",
    "\"these\",\n",
    "\"those\",\n",
    "\"my\",\n",
    "#\"mine\",   #Omitted since mine can refer to something else\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"his\",\n",
    "\"hers\",\n",
    "\"its\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"own\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"few\",\n",
    "\"much\",\n",
    "\"many\",\n",
    "\"lot\",\n",
    "\"lots\",\n",
    "\"some\",\n",
    "\"any\",\n",
    "\"enough\",\n",
    "\"all\",\n",
    "\"both\",\n",
    "\"half\",\n",
    "\"either\",\n",
    "\"neither\",\n",
    "\"each\",\n",
    "\"every\",\n",
    "\"certain\",\n",
    "\"other\",\n",
    "\"another\",\n",
    "\"such\",\n",
    "\"several\",\n",
    "\"multiple\",\n",
    "# \"what\",#Dealt with later on\n",
    "\"rather\",\n",
    "\"quite\",\n",
    "# All prepositions\n",
    "\"aboard\",\n",
    "\"about\",\n",
    "\"above\",\n",
    "\"across\",\n",
    "\"after\",\n",
    "\"against\",\n",
    "\"along\",\n",
    "\"amid\",\n",
    "\"amidst\",\n",
    "\"among\",\n",
    "\"amongst\",\n",
    "\"anti\",\n",
    "\"around\",\n",
    "\"as\",\n",
    "\"at\",\n",
    "\"away\",\n",
    "\"before\",\n",
    "\"behind\",\n",
    "\"below\",\n",
    "\"beneath\",\n",
    "\"beside\",\n",
    "\"besides\",\n",
    "\"between\",\n",
    "\"beyond\",\n",
    "\"but\",\n",
    "\"by\",\n",
    "\"concerning\",\n",
    "\"considering\",\n",
    "\"despite\",\n",
    "\"down\",\n",
    "\"during\",\n",
    "\"except\",\n",
    "\"excepting\",\n",
    "\"excluding\",\n",
    "\"far\",\n",
    "\"following\",\n",
    "\"for\",\n",
    "\"from\",\n",
    "\"here\",\n",
    "\"here's\",\n",
    "\"in\",\n",
    "\"inside\",\n",
    "\"into\",\n",
    "\"left\",\n",
    "\"like\",\n",
    "\"minus\",\n",
    "\"near\",\n",
    "\"of\",\n",
    "\"off\",\n",
    "\"on\",\n",
    "\"onto\",\n",
    "\"opposite\",\n",
    "\"out\",\n",
    "\"outside\",\n",
    "\"over\",\n",
    "\"past\",\n",
    "\"per\",\n",
    "\"plus\",\n",
    "\"regarding\",\n",
    "\"right\",\n",
    "#\"round\",   #Omitted\n",
    "#\"save\",#Omitted\n",
    "\"since\",\n",
    "\"than\",\n",
    "\"there\",\n",
    "\"there's\",\n",
    "\"through\",\n",
    "\"to\",\n",
    "\"toward\",\n",
    "\"towards\",\n",
    "\"under\",\n",
    "\"underneath\",\n",
    "\"unlike\",\n",
    "\"until\",\n",
    "\"up\",\n",
    "\"upon\",\n",
    "\"versus\",\n",
    "\"via\",\n",
    "\"with\",\n",
    "\"within\",\n",
    "\"without\",\n",
    "# Irrelevant verbs\n",
    "\"may\",\n",
    "\"might\",\n",
    "\"will\",\n",
    "\"won't\",\n",
    "\"would\",\n",
    "\"wouldn't\",\n",
    "\"can\",\n",
    "\"can't\",\n",
    "\"cannot\",\n",
    "\"could\",\n",
    "\"couldn't\",\n",
    "\"should\",\n",
    "\"shouldn't\",\n",
    "\"must\",\n",
    "\"must've\",\n",
    "\"be\",\n",
    "\"being\",\n",
    "\"been\",\n",
    "\"am\",\n",
    "\"are\",\n",
    "\"aren't\",\n",
    "\"ain't\",\n",
    "\"is\",\n",
    "\"isn't\",\n",
    "\"was\",\n",
    "\"wasn't\",\n",
    "\"were\",\n",
    "\"weren't\",\n",
    "\"do\",\n",
    "\"doing\",\n",
    "\"don't\",\n",
    "\"does\",\n",
    "\"doesn't\",\n",
    "\"did\",\n",
    "\"didn't\",\n",
    "\"done\",\n",
    "\"have\",\n",
    "\"haven't\",\n",
    "\"having\",\n",
    "\"has\",\n",
    "\"hasn't\",\n",
    "\"had\",\n",
    "\"hadn't\",\n",
    "\"get\",\n",
    "\"getting\",\n",
    "\"gets\",\n",
    "\"got\",\n",
    "\"gotten\",\n",
    "\"go\",\n",
    "\"going\",\n",
    "\"gonna\",\n",
    "\"goes\",\n",
    "\"went\",\n",
    "\"gone\",\n",
    "\"make\",\n",
    "\"making\",\n",
    "\"makes\",\n",
    "\"made\",\n",
    "\"take\",\n",
    "\"taking\",\n",
    "\"takes\",\n",
    "\"took\",\n",
    "\"taken\",\n",
    "\"need\",\n",
    "\"needing\",\n",
    "\"needs\",\n",
    "\"needed\",\n",
    "\"use\",\n",
    "\"using\",\n",
    "\"uses\",\n",
    "\"used\",\n",
    "\"want\",\n",
    "\"wanna\",\n",
    "\"wanting\",\n",
    "\"wants\",\n",
    "\"let\",\n",
    "\"lets\",\n",
    "\"letting\",\n",
    "\"let's\",\n",
    "\"suppose\",\n",
    "\"supposing\",\n",
    "\"supposes\",\n",
    "\"supposed\",\n",
    "\"seem\",\n",
    "\"seeming\",\n",
    "\"seems\",\n",
    "\"seemed\",\n",
    "\"say\",\n",
    "\"saying\",\n",
    "\"says\",\n",
    "\"said\",\n",
    "\"know\",\n",
    "\"knowing\",\n",
    "\"knows\",\n",
    "\"knew\",\n",
    "\"known\",\n",
    "\"look\",\n",
    "\"looking\",\n",
    "\"looked\",\n",
    "\"think\",\n",
    "\"thinking\",\n",
    "\"thinks\",\n",
    "\"thought\",\n",
    "\"feel\",\n",
    "\"feels\",\n",
    "\"felt\",\n",
    "\"based\",\n",
    "\"put\",\n",
    "\"puts\",\n",
    "#\"wanted\"   #Omitted since the advective is relevant\n",
    "# Question words and associated words\n",
    "\"who\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"who'd\",\n",
    "\"whoever\",\n",
    "\"whoever's\",\n",
    "\"whom\",\n",
    "\"whomever\",\n",
    "\"whomever's\",\n",
    "\"whose\",\n",
    "\"whosever\",\n",
    "\"whosever's\",\n",
    "\"when\",\n",
    "\"whenever\",\n",
    "\"which\",\n",
    "\"whichever\",\n",
    "\"where\",\n",
    "\"where's\",\n",
    "\"where'd\",\n",
    "\"wherever\",\n",
    "\"why\",\n",
    "\"why's\",\n",
    "\"why'd\",\n",
    "\"whyever\",\n",
    "\"what\",\n",
    "\"what's\",\n",
    "\"whatever\",\n",
    "\"whence\",\n",
    "\"how\",\n",
    "\"how's\",\n",
    "\"how'd\",\n",
    "\"however\",\n",
    "\"whether\",\n",
    "\"whatsoever\",\n",
    "# Connector words and irrelevant adverbs\n",
    "\"and\",\n",
    "\"or\",\n",
    "\"not\",\n",
    "\"because\",\n",
    "\"also\",\n",
    "\"always\",\n",
    "\"never\",\n",
    "\"only\",\n",
    "\"really\",\n",
    "\"very\",\n",
    "\"greatly\",\n",
    "\"extremely\",\n",
    "\"somewhat\",\n",
    "\"no\",\n",
    "\"nope\",\n",
    "\"nah\",\n",
    "\"yes\",\n",
    "\"yep\",\n",
    "\"yeh\",\n",
    "\"yeah\",\n",
    "\"maybe\",\n",
    "\"perhaps\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"less\",\n",
    "\"least\",\n",
    "\"good\",\n",
    "\"great\",\n",
    "\"well\",\n",
    "\"better\",\n",
    "\"best\",\n",
    "\"bad\",\n",
    "\"worse\",\n",
    "\"worst\",\n",
    "\"too\",\n",
    "\"thru\",\n",
    "\"though\",\n",
    "\"although\",\n",
    "\"yet\",\n",
    "\"already\",\n",
    "\"then\",\n",
    "\"even\",\n",
    "\"now\",\n",
    "\"sometimes\",\n",
    "\"still\",\n",
    "\"together\",\n",
    "\"altogether\",\n",
    "\"entirely\",\n",
    "\"fully\",\n",
    "\"entire\",\n",
    "\"whole\",\n",
    "\"completely\",\n",
    "\"utterly\",\n",
    "\"seemingly\",\n",
    "\"apparently\",\n",
    "\"clearly\",\n",
    "\"obviously\",\n",
    "\"actually\",\n",
    "\"actual\",\n",
    "\"usually\",\n",
    "\"usual\",\n",
    "\"literally\",\n",
    "\"honestly\",\n",
    "\"absolutely\",\n",
    "\"definitely\",\n",
    "\"generally\",\n",
    "\"totally\",\n",
    "\"finally\",\n",
    "\"basically\",\n",
    "\"essentially\",\n",
    "\"fundamentally\",\n",
    "\"automatically\",\n",
    "\"immediately\",\n",
    "\"necessarily\",\n",
    "\"primarily\",\n",
    "\"normally\",\n",
    "\"perfectly\",\n",
    "\"constantly\",\n",
    "\"particularly\",\n",
    "\"eventually\",\n",
    "\"hopefully\",\n",
    "\"mainly\",\n",
    "\"typically\",\n",
    "\"specifically\",\n",
    "\"differently\",\n",
    "\"appropriately\",\n",
    "\"plenty\",\n",
    "\"certainly\",\n",
    "\"unfortunately\",\n",
    "\"ultimately\",\n",
    "\"unlikely\",\n",
    "\"likely\",\n",
    "\"potentially\",\n",
    "\"fortunately\",\n",
    "\"personally\",\n",
    "\"directly\",\n",
    "\"indirectly\",\n",
    "\"nearly\",\n",
    "\"closely\",\n",
    "\"slightly\",\n",
    "\"probably\",\n",
    "\"possibly\",\n",
    "\"especially\",\n",
    "\"frequently\",\n",
    "\"often\",\n",
    "\"oftentimes\",\n",
    "\"seldom\",\n",
    "\"rarely\",\n",
    "\"sure\",\n",
    "\"while\",\n",
    "\"whilst\",\n",
    "\"able\",\n",
    "\"unable\",\n",
    "\"else\",\n",
    "\"ever\",\n",
    "\"once\",\n",
    "\"twice\",\n",
    "\"thrice\",\n",
    "\"almost\",\n",
    "\"again\",\n",
    "\"instead\",\n",
    "\"next\",\n",
    "\"previous\",\n",
    "\"unless\",\n",
    "\"somehow\",\n",
    "\"anyhow\",\n",
    "\"anywhere\",\n",
    "\"somewhere\",\n",
    "\"everywhere\",\n",
    "\"nowhere\",\n",
    "\"further\",\n",
    "\"anymore\",\n",
    "\"later\",\n",
    "\"ago\",\n",
    "\"ahead\",\n",
    "\"just\",\n",
    "\"same\",\n",
    "\"different\",\n",
    "\"big\",\n",
    "\"small\",\n",
    "\"little\",\n",
    "\"tiny\",\n",
    "\"large\",\n",
    "\"huge\",\n",
    "\"pretty\",\n",
    "\"mostly\",\n",
    "\"anyway\",\n",
    "\"anyways\",\n",
    "\"otherwise\",\n",
    "\"regardless\",\n",
    "\"throughout\",\n",
    "\"additionally\",\n",
    "\"moreover\",\n",
    "\"furthermore\",\n",
    "\"meanwhile\",\n",
    "\"afterwards\",\n",
    "# Irrelevant nouns\n",
    "\"thing\",\n",
    "\"thing's\",\n",
    "\"things\",\n",
    "\"stuff\",\n",
    "\"other's\",\n",
    "\"others\",\n",
    "\"another's\",\n",
    "\"total\",\n",
    "\"\",\n",
    "\"false\",\n",
    "\"none\",\n",
    "\"way\",\n",
    "\"kind\",\n",
    "# Lettered numbers and order\n",
    "\"zero\",\n",
    "\"zeros\",\n",
    "\"zeroes\",\n",
    "\"one\",\n",
    "\"ones\",\n",
    "\"two\",\n",
    "\"three\",\n",
    "\"four\",\n",
    "\"five\",\n",
    "\"six\", \n",
    "\"seven\",\n",
    "\"eight\",\n",
    "\"nine\",\n",
    "\"ten\",\n",
    "\"twenty\",\n",
    "\"thirty\",\n",
    "\"forty\",\n",
    "\"fifty\",\n",
    "\"sixty\",\n",
    "\"seventy\",\n",
    "\"eighty\",\n",
    "\"ninety\",\n",
    "\"hundred\",\n",
    "\"hundreds\",\n",
    "\"thousand\",\n",
    "\"thousands\",\n",
    "\"million\",\n",
    "\"millions\",\n",
    "\"first\",\n",
    "\"last\",\n",
    "\"second\",\n",
    "\"third\",\n",
    "\"fourth\",\n",
    "\"fifth\",\n",
    "\"sixth\",\n",
    "\"seventh\",\n",
    "\"eigth\",\n",
    "\"ninth\",\n",
    "\"tenth\",\n",
    "\"firstly\",\n",
    "\"secondly\",\n",
    "\"thirdly\",\n",
    "\"lastly\",\n",
    "# Greetings and slang\n",
    "\"hello\",\n",
    "\"hi\",\n",
    "\"hey\",\n",
    "\"sup\",\n",
    "\"yo\",\n",
    "\"greetings\",\n",
    "\"please\",\n",
    "\"okay\",\n",
    "\"ok\",\n",
    "\"y'all\",\n",
    "\"lol\",\n",
    "\"rofl\",\n",
    "\"thank\",\n",
    "\"thanks\",\n",
    "\"alright\",\n",
    "\"kinda\",\n",
    "\"dont\",\n",
    "\"sorry\",\n",
    "\"idk\",\n",
    "\"tldr\",\n",
    "\"tl\",\n",
    "\"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "\"tbh\",\n",
    "\"dude\",\n",
    "\"tho\",\n",
    "\"aka\",\n",
    "\"plz\",\n",
    "\"pls\",\n",
    "\"bit\",\n",
    "\"don\",\n",
    "# Miscellaneous\n",
    "\"www\",\n",
    "\"https\",\n",
    "\"http\",\n",
    "\"com\",\n",
    "\"etc\"\n",
    "\"html\",\n",
    "\"reddit\",\n",
    "\"subreddit\",\n",
    "\"subreddits\",\n",
    "\"comments\",\n",
    "\"reply\",\n",
    "\"replies\",\n",
    "\"thread\",\n",
    "\"threads\",\n",
    "\"post\",\n",
    "\"posts\",\n",
    "\"website\",\n",
    "\"websites\",\n",
    "\"web site\",\n",
    "\"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4w1BXsuwOJ7X",
    "outputId": "97f3831d-cb91-4348-9591-bf460a555b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "The best accuracy is 88.305.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'vect__stop_words': ['to', 'made', 'his', 'sometime', 'those', 'except', 'no', 'which', 'shan', 'detail', 'when', 'con', 'haven', \"needn't\", 'take', 'ever', 'would', \"shan't\", 'side', 'thick', 'thereby', 'hence', 'third', 'ours', 'six', 'she', 'then', 'seeming', 'therein', 'front', 'show', 'below', 'amoungst', 'none', \"couldn't\", 'keep', 'bottom', 'hereby', 'wherein', 'latterly', 'we', 'upon', 'must', 'once', 'more', 'therefore', 'doesn', 'same', 'seems', 'sincere', 'last', 'whence', 'inc', 'about', \"she's\", 'needn', 'just', 'somewhere', \"don't\", 'anything', 'empty', 'via', 'often', 'me', 'whole', 'together', 'call', 'whoever', 'everything', 'former', 'am', 'others', 'elsewhere', 'may', 'thereafter', \"you've\", 'seem', \"wouldn't\", 'himself', 'both', 'ltd', 'because', 'hasnt', 'hereupon', 'even', 'herein', 'could', 'among', 'part', 'should', 'system', 'hadn', 'under', 'yourselves', \"wasn't\", 'fill', 'who', 'a', 'mill', 'whose', 'whether', 'nothing', 'hereafter', 'any', 'are', 'here', 'nor', 'anyhow', \"didn't\", 'ma', 'as', 'before', 'd', 'many', 'cant', 'own', 'whereafter', 'otherwise', 'not', 'my', 'hundred', 'against', 'what', 'two', 'twelve', 'onto', 'eg', 'serious', \"you're\", 'of', 'well', 'the', 'us', 'your', 'he', 'thereupon', 'along', \"hadn't\", 'etc', 'whereby', 'been', 'from', 'theirs', 've', 'mustn', 'though', 'ten', 'in', 'never', \"you'd\", 'least', 'having', 'every', 'with', 'without', 'beforehand', 'i', 'across', 'full', 'couldnt', 'somehow', 'again', 'how', 'becomes', 'you', 'four', 'five', 'until', 'y', 'might', 'll', 'mostly', 'co', 'beside', 'mightn', 'wasn', \"mightn't\", 'other', 'already', 'her', \"it's\", \"isn't\", 'always', 'by', 'indeed', 'around', 'either', 'noone', 'although', 'beyond', 'or', 'yours', 'is', 'cannot', 'shouldn', 'cry', 'o', 'for', 'only', 'mine', 'out', 'isn', 'less', \"mustn't\", 'fire', 'fifteen', 'nowhere', 'into', 'nevertheless', 'seemed', 'moreover', 'now', \"won't\", 'each', 'why', 'eleven', 'at', 'very', 'besides', 'amongst', 'didn', 'don', 'couldn', 'describe', 'ourselves', 'be', 'thin', 'such', 'during', 'someone', \"you'll\", 'on', 'thence', 'herself', 'rather', 'twenty', 'get', 'toward', 'everyone', 'won', 'too', 'else', 'wouldn', 'if', 'find', 'also', 'eight', 'whereas', \"that'll\", 'bill', 'itself', 'since', 'sometimes', 'these', 'this', 'off', 'interest', 'where', 'above', 'alone', 'up', \"doesn't\", 'hers', 'some', 'it', \"shouldn't\", 'un', 'have', 'anyone', 'please', 'fifty', \"weren't\", 'all', 'neither', 'they', 'afterwards', \"aren't\", 'wherever', 'becoming', 'and', 'whereupon', 's', 'name', 'that', 'something', 'has', 'enough', 'further', 'an', 'meanwhile', 'will', 'next', 'thru', 'another', 'perhaps', 'still', 'forty', 'one', 'whatever', 'doing', 'being', 'several', 'sixty', 'everywhere', 'move', 'yourself', 'per', 'whither', 'put', 'give', 'nobody', 'than', 'however', 'aren', 'through', 'namely', 'can', 'almost', 'latter', 'themselves', 'was', 'anywhere', 'there', 'behind', 'formerly', 'its', 'three', 'much', 'nine', 're', 'does', 'back', 'most', 'between', 'within', 'down', 'de', 'over', 'anyway', 'their', 'were', 'amount', 'weren', \"should've\", 'while', 'towards', 'ain', 'few', 'hasn', 'but', 'become', 'throughout', 't', 'whenever', 'had', 'top', 'ie', 'myself', 'so', 'see', \"haven't\", 'yet', 'done', 'after', 'whom', 'first', 'them', \"hasn't\", 'go', 'found', 'due', 'became', 'm', 'did', 'him', 'do', 'thus', 'our']}\n",
      "Run time: 30.90384078025818 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing stop words\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'clf__max_depth': [10, 50, 100, None],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    \n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gzydUK4afym",
    "outputId": "04f3a1e4-be94-46fc-a10a-a6c16aed5b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "The best accuracy is 88.719.\n",
      "The winning parameters are {'clf__criterion': 'gini', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'selecter__k': 3000, 'vect__stop_words': ['to', 'made', 'his', 'sometime', 'those', 'except', 'no', 'which', 'shan', 'detail', 'when', 'con', 'haven', \"needn't\", 'take', 'ever', 'would', \"shan't\", 'side', 'thick', 'thereby', 'hence', 'third', 'ours', 'six', 'she', 'then', 'seeming', 'therein', 'front', 'show', 'below', 'amoungst', 'none', \"couldn't\", 'keep', 'bottom', 'hereby', 'wherein', 'latterly', 'we', 'upon', 'must', 'once', 'more', 'therefore', 'doesn', 'same', 'seems', 'sincere', 'last', 'whence', 'inc', 'about', \"she's\", 'needn', 'just', 'somewhere', \"don't\", 'anything', 'empty', 'via', 'often', 'me', 'whole', 'together', 'call', 'whoever', 'everything', 'former', 'am', 'others', 'elsewhere', 'may', 'thereafter', \"you've\", 'seem', \"wouldn't\", 'himself', 'both', 'ltd', 'because', 'hasnt', 'hereupon', 'even', 'herein', 'could', 'among', 'part', 'should', 'system', 'hadn', 'under', 'yourselves', \"wasn't\", 'fill', 'who', 'a', 'mill', 'whose', 'whether', 'nothing', 'hereafter', 'any', 'are', 'here', 'nor', 'anyhow', \"didn't\", 'ma', 'as', 'before', 'd', 'many', 'cant', 'own', 'whereafter', 'otherwise', 'not', 'my', 'hundred', 'against', 'what', 'two', 'twelve', 'onto', 'eg', 'serious', \"you're\", 'of', 'well', 'the', 'us', 'your', 'he', 'thereupon', 'along', \"hadn't\", 'etc', 'whereby', 'been', 'from', 'theirs', 've', 'mustn', 'though', 'ten', 'in', 'never', \"you'd\", 'least', 'having', 'every', 'with', 'without', 'beforehand', 'i', 'across', 'full', 'couldnt', 'somehow', 'again', 'how', 'becomes', 'you', 'four', 'five', 'until', 'y', 'might', 'll', 'mostly', 'co', 'beside', 'mightn', 'wasn', \"mightn't\", 'other', 'already', 'her', \"it's\", \"isn't\", 'always', 'by', 'indeed', 'around', 'either', 'noone', 'although', 'beyond', 'or', 'yours', 'is', 'cannot', 'shouldn', 'cry', 'o', 'for', 'only', 'mine', 'out', 'isn', 'less', \"mustn't\", 'fire', 'fifteen', 'nowhere', 'into', 'nevertheless', 'seemed', 'moreover', 'now', \"won't\", 'each', 'why', 'eleven', 'at', 'very', 'besides', 'amongst', 'didn', 'don', 'couldn', 'describe', 'ourselves', 'be', 'thin', 'such', 'during', 'someone', \"you'll\", 'on', 'thence', 'herself', 'rather', 'twenty', 'get', 'toward', 'everyone', 'won', 'too', 'else', 'wouldn', 'if', 'find', 'also', 'eight', 'whereas', \"that'll\", 'bill', 'itself', 'since', 'sometimes', 'these', 'this', 'off', 'interest', 'where', 'above', 'alone', 'up', \"doesn't\", 'hers', 'some', 'it', \"shouldn't\", 'un', 'have', 'anyone', 'please', 'fifty', \"weren't\", 'all', 'neither', 'they', 'afterwards', \"aren't\", 'wherever', 'becoming', 'and', 'whereupon', 's', 'name', 'that', 'something', 'has', 'enough', 'further', 'an', 'meanwhile', 'will', 'next', 'thru', 'another', 'perhaps', 'still', 'forty', 'one', 'whatever', 'doing', 'being', 'several', 'sixty', 'everywhere', 'move', 'yourself', 'per', 'whither', 'put', 'give', 'nobody', 'than', 'however', 'aren', 'through', 'namely', 'can', 'almost', 'latter', 'themselves', 'was', 'anywhere', 'there', 'behind', 'formerly', 'its', 'three', 'much', 'nine', 're', 'does', 'back', 'most', 'between', 'within', 'down', 'de', 'over', 'anyway', 'their', 'were', 'amount', 'weren', \"should've\", 'while', 'towards', 'ain', 'few', 'hasn', 'but', 'become', 'throughout', 't', 'whenever', 'had', 'top', 'ie', 'myself', 'so', 'see', \"haven't\", 'yet', 'done', 'after', 'whom', 'first', 'them', \"hasn't\", 'go', 'found', 'due', 'became', 'm', 'did', 'him', 'do', 'thus', 'our']}\n",
      "Run time: 15.338690280914307 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing features\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOZ40rvh54xY"
   },
   "outputs": [],
   "source": [
    "#stem lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxjUGRS658II",
    "outputId": "53c92d68-f5b8-4a48-cee4-6a4b5af2e58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 86.073.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5, 'selecter__k': 5000, 'vect__stop_words': ['to', 'made', 'his', 'sometime', 'those', 'except', 'no', 'which', 'shan', 'detail', 'when', 'con', 'haven', \"needn't\", 'take', 'ever', 'would', \"shan't\", 'side', 'thick', 'thereby', 'hence', 'third', 'ours', 'six', 'she', 'then', 'seeming', 'therein', 'front', 'show', 'below', 'amoungst', 'none', \"couldn't\", 'keep', 'bottom', 'hereby', 'wherein', 'latterly', 'we', 'upon', 'must', 'once', 'more', 'therefore', 'doesn', 'same', 'seems', 'sincere', 'last', 'whence', 'inc', 'about', \"she's\", 'needn', 'just', 'somewhere', \"don't\", 'anything', 'empty', 'via', 'often', 'me', 'whole', 'together', 'call', 'whoever', 'everything', 'former', 'am', 'others', 'elsewhere', 'may', 'thereafter', \"you've\", 'seem', \"wouldn't\", 'himself', 'both', 'ltd', 'because', 'hasnt', 'hereupon', 'even', 'herein', 'could', 'among', 'part', 'should', 'system', 'hadn', 'under', 'yourselves', \"wasn't\", 'fill', 'who', 'a', 'mill', 'whose', 'whether', 'nothing', 'hereafter', 'any', 'are', 'here', 'nor', 'anyhow', \"didn't\", 'ma', 'as', 'before', 'd', 'many', 'cant', 'own', 'whereafter', 'otherwise', 'not', 'my', 'hundred', 'against', 'what', 'two', 'twelve', 'onto', 'eg', 'serious', \"you're\", 'of', 'well', 'the', 'us', 'your', 'he', 'thereupon', 'along', \"hadn't\", 'etc', 'whereby', 'been', 'from', 'theirs', 've', 'mustn', 'though', 'ten', 'in', 'never', \"you'd\", 'least', 'having', 'every', 'with', 'without', 'beforehand', 'i', 'across', 'full', 'couldnt', 'somehow', 'again', 'how', 'becomes', 'you', 'four', 'five', 'until', 'y', 'might', 'll', 'mostly', 'co', 'beside', 'mightn', 'wasn', \"mightn't\", 'other', 'already', 'her', \"it's\", \"isn't\", 'always', 'by', 'indeed', 'around', 'either', 'noone', 'although', 'beyond', 'or', 'yours', 'is', 'cannot', 'shouldn', 'cry', 'o', 'for', 'only', 'mine', 'out', 'isn', 'less', \"mustn't\", 'fire', 'fifteen', 'nowhere', 'into', 'nevertheless', 'seemed', 'moreover', 'now', \"won't\", 'each', 'why', 'eleven', 'at', 'very', 'besides', 'amongst', 'didn', 'don', 'couldn', 'describe', 'ourselves', 'be', 'thin', 'such', 'during', 'someone', \"you'll\", 'on', 'thence', 'herself', 'rather', 'twenty', 'get', 'toward', 'everyone', 'won', 'too', 'else', 'wouldn', 'if', 'find', 'also', 'eight', 'whereas', \"that'll\", 'bill', 'itself', 'since', 'sometimes', 'these', 'this', 'off', 'interest', 'where', 'above', 'alone', 'up', \"doesn't\", 'hers', 'some', 'it', \"shouldn't\", 'un', 'have', 'anyone', 'please', 'fifty', \"weren't\", 'all', 'neither', 'they', 'afterwards', \"aren't\", 'wherever', 'becoming', 'and', 'whereupon', 's', 'name', 'that', 'something', 'has', 'enough', 'further', 'an', 'meanwhile', 'will', 'next', 'thru', 'another', 'perhaps', 'still', 'forty', 'one', 'whatever', 'doing', 'being', 'several', 'sixty', 'everywhere', 'move', 'yourself', 'per', 'whither', 'put', 'give', 'nobody', 'than', 'however', 'aren', 'through', 'namely', 'can', 'almost', 'latter', 'themselves', 'was', 'anywhere', 'there', 'behind', 'formerly', 'its', 'three', 'much', 'nine', 're', 'does', 'back', 'most', 'between', 'within', 'down', 'de', 'over', 'anyway', 'their', 'were', 'amount', 'weren', \"should've\", 'while', 'towards', 'ain', 'few', 'hasn', 'but', 'become', 'throughout', 't', 'whenever', 'had', 'top', 'ie', 'myself', 'so', 'see', \"haven't\", 'yet', 'done', 'after', 'whom', 'first', 'them', \"hasn't\", 'go', 'found', 'due', 'became', 'm', 'did', 'him', 'do', 'thus', 'our'], 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f6d73ed0b80>}\n",
      "Run time: 81.05935955047607 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing lemma => slight improvement\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3H1LBUUW3Jwm"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5MxYahl6inv",
    "outputId": "c2f95525-8a0f-4ef9-a195-d759d0ec598c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 86.632.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 5, 'selecter__k': 3000, 'vect__preprocessor': <function preprocess_text at 0x7f6d6a362670>, 'vect__stop_words': ['to', 'made', 'his', 'sometime', 'those', 'except', 'no', 'which', 'shan', 'detail', 'when', 'con', 'haven', \"needn't\", 'take', 'ever', 'would', \"shan't\", 'side', 'thick', 'thereby', 'hence', 'third', 'ours', 'six', 'she', 'then', 'seeming', 'therein', 'front', 'show', 'below', 'amoungst', 'none', \"couldn't\", 'keep', 'bottom', 'hereby', 'wherein', 'latterly', 'we', 'upon', 'must', 'once', 'more', 'therefore', 'doesn', 'same', 'seems', 'sincere', 'last', 'whence', 'inc', 'about', \"she's\", 'needn', 'just', 'somewhere', \"don't\", 'anything', 'empty', 'via', 'often', 'me', 'whole', 'together', 'call', 'whoever', 'everything', 'former', 'am', 'others', 'elsewhere', 'may', 'thereafter', \"you've\", 'seem', \"wouldn't\", 'himself', 'both', 'ltd', 'because', 'hasnt', 'hereupon', 'even', 'herein', 'could', 'among', 'part', 'should', 'system', 'hadn', 'under', 'yourselves', \"wasn't\", 'fill', 'who', 'a', 'mill', 'whose', 'whether', 'nothing', 'hereafter', 'any', 'are', 'here', 'nor', 'anyhow', \"didn't\", 'ma', 'as', 'before', 'd', 'many', 'cant', 'own', 'whereafter', 'otherwise', 'not', 'my', 'hundred', 'against', 'what', 'two', 'twelve', 'onto', 'eg', 'serious', \"you're\", 'of', 'well', 'the', 'us', 'your', 'he', 'thereupon', 'along', \"hadn't\", 'etc', 'whereby', 'been', 'from', 'theirs', 've', 'mustn', 'though', 'ten', 'in', 'never', \"you'd\", 'least', 'having', 'every', 'with', 'without', 'beforehand', 'i', 'across', 'full', 'couldnt', 'somehow', 'again', 'how', 'becomes', 'you', 'four', 'five', 'until', 'y', 'might', 'll', 'mostly', 'co', 'beside', 'mightn', 'wasn', \"mightn't\", 'other', 'already', 'her', \"it's\", \"isn't\", 'always', 'by', 'indeed', 'around', 'either', 'noone', 'although', 'beyond', 'or', 'yours', 'is', 'cannot', 'shouldn', 'cry', 'o', 'for', 'only', 'mine', 'out', 'isn', 'less', \"mustn't\", 'fire', 'fifteen', 'nowhere', 'into', 'nevertheless', 'seemed', 'moreover', 'now', \"won't\", 'each', 'why', 'eleven', 'at', 'very', 'besides', 'amongst', 'didn', 'don', 'couldn', 'describe', 'ourselves', 'be', 'thin', 'such', 'during', 'someone', \"you'll\", 'on', 'thence', 'herself', 'rather', 'twenty', 'get', 'toward', 'everyone', 'won', 'too', 'else', 'wouldn', 'if', 'find', 'also', 'eight', 'whereas', \"that'll\", 'bill', 'itself', 'since', 'sometimes', 'these', 'this', 'off', 'interest', 'where', 'above', 'alone', 'up', \"doesn't\", 'hers', 'some', 'it', \"shouldn't\", 'un', 'have', 'anyone', 'please', 'fifty', \"weren't\", 'all', 'neither', 'they', 'afterwards', \"aren't\", 'wherever', 'becoming', 'and', 'whereupon', 's', 'name', 'that', 'something', 'has', 'enough', 'further', 'an', 'meanwhile', 'will', 'next', 'thru', 'another', 'perhaps', 'still', 'forty', 'one', 'whatever', 'doing', 'being', 'several', 'sixty', 'everywhere', 'move', 'yourself', 'per', 'whither', 'put', 'give', 'nobody', 'than', 'however', 'aren', 'through', 'namely', 'can', 'almost', 'latter', 'themselves', 'was', 'anywhere', 'there', 'behind', 'formerly', 'its', 'three', 'much', 'nine', 're', 'does', 'back', 'most', 'between', 'within', 'down', 'de', 'over', 'anyway', 'their', 'were', 'amount', 'weren', \"should've\", 'while', 'towards', 'ain', 'few', 'hasn', 'but', 'become', 'throughout', 't', 'whenever', 'had', 'top', 'ie', 'myself', 'so', 'see', \"haven't\", 'yet', 'done', 'after', 'whom', 'first', 'them', \"hasn't\", 'go', 'found', 'due', 'became', 'm', 'did', 'him', 'do', 'thus', 'our'], 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f6d73fcd1c0>}\n",
      "Run time: 81.01828145980835 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing preprocessor for lowering words and removing numeric values => slight improvement\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__preprocessor': [preprocess_text],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "L_ujkG5O7Xe8",
    "outputId": "10057ba7-1b15-4df6-e901-f3ccbbc98348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a3178dcbc5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mt_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#testing binary in vectorize\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__binary': [True,False],\n",
    "    'vect__preprocessor': [preprocess_text],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-Yrtrcu72pi",
    "outputId": "b7342130-105e-4ed1-c3a5-c05d735afbe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "60 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 855, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py\", line 1955, in transform\n",
      "    return normalize(X, norm=self.norm, axis=1, copy=copy)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py\", line 1783, in normalize\n",
      "    raise ValueError(\"'%s' is not a supported norm\" % norm)\n",
      "ValueError: 'None' is not a supported norm\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.83984071 0.84119075 0.8356352  0.8412296         nan        nan\n",
      " 0.83706294 0.83565462 0.83424631 0.83981158        nan        nan\n",
      " 0.84123932 0.83981158 0.84120047 0.82591298        nan        nan\n",
      " 0.85374903 0.83844211 0.83008936 0.8342366         nan        nan\n",
      " 0.85654623 0.84958236 0.85516706 0.84260878        nan        nan\n",
      " 0.86213092 0.85933372 0.85652681 0.84677545        nan        nan]\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 86.213.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 5, 'normalizer__norm': 'l2', 'selecter__k': 5000, 'vect__binary': False, 'vect__preprocessor': <function preprocess_text at 0x7f407144cee0>, 'vect__stop_words': frozenset({'now', 'along', 'empty', 'don', 'yours', 'well', 'll', 'about', 'four', 'top', 'serious', 'yourselves', 'than', 'both', 'due', 'and', 'into', 'her', 'thereby', 've', 'except', 'see', 'i', 'down', 'ourselves', 'as', 'thick', 'must', 'do', 'she', 'my', 'own', 'us', 'thus', 'very', 'of', 'wasn', 'your', 'its', 'he', 'former', 'yet', 'almost', 'wherever', 'any', 'had', 'that', 'an', 'itself', \"shan't\", 'nine', 'besides', 'some', 'whereafter', 'who', 'haven', 'thence', 'namely', 'would', 'everything', 'others', 'seems', 'ain', 'ma', 'rather', \"aren't\", 'while', \"mightn't\", 'needn', \"you'll\", 'beyond', \"wouldn't\", 'five', 'them', 'thru', 'several', 'two', 'name', 'bottom', \"couldn't\", 'per', 'most', \"doesn't\", 'ltd', 'give', \"wasn't\", 'without', 'get', 'ten', \"don't\", 'couldn', 'hasn', 'made', 'or', \"weren't\", \"hadn't\", 'how', 'found', 'anyhow', 'against', 'myself', 'to', 'always', \"won't\", 'here', 'has', 'co', 'around', 'does', \"you're\", 'before', 'sincere', 'anything', \"you'd\", 'becomes', 'their', 'hereupon', 'hadn', 'inc', 'having', 'whoever', 'until', 'within', 'd', 'because', 'above', 'part', 'we', 'm', 'afterwards', \"mustn't\", 'hundred', 'perhaps', 'via', 'three', 'mine', 'where', 'nowhere', 'few', 'thereupon', 'upon', 'whole', 'then', 'somewhere', 'less', 'for', 'often', 'ever', 'amount', 'neither', 'front', \"it's\", 'these', 'onto', 'they', 'meanwhile', 'twelve', 'all', 'being', 'last', 'towards', 'below', 'many', 'six', 'o', 'seeming', 'throughout', 'together', 'again', \"that'll\", 'may', 'un', 'seemed', 'doesn', 'amoungst', 'con', 'anyone', 'each', 'shan', 'forty', 'am', 'across', 'over', 'everyone', 'this', 'hence', 'herein', 'full', 'fifteen', 'so', 'least', 'only', 'another', 'third', 'please', 'thereafter', 'sometimes', 'there', 'never', 'can', 'nevertheless', 'when', 'whereupon', 'him', 'not', 'such', 'next', 'those', 'why', 'himself', 'could', 'same', 'should', 'shouldn', 'our', 're', \"didn't\", 'just', 'back', 'first', 'alone', 'since', 'hers', 'still', 'whenever', 'won', 'anywhere', 'further', 'seem', 'during', 'thin', 'might', \"should've\", 'was', 'even', 'move', 'fire', 'bill', 'been', 's', 'up', 'at', 'whereas', 'will', 'too', 'eleven', 'mill', 'system', 'whom', 'noone', 'out', 'which', 'but', 'hereafter', 'among', 'cant', 'either', 'nobody', \"she's\", 'eight', 'indeed', \"needn't\", 'cry', 'a', 'nothing', 'on', 'also', 'ie', 'find', 'keep', 'themselves', \"haven't\", 'formerly', 'though', 'someone', 'behind', 'twenty', 'everywhere', 'whose', 'wouldn', \"you've\", 'therefore', 'be', 'cannot', 'were', 'none', 'one', 'aren', 'mustn', 'whereby', 'through', \"hasn't\", 'enough', 'once', 'mostly', 'much', 'although', 'his', 'me', 'become', 'amongst', 'the', \"isn't\", 'done', 'latter', 'you', 'nor', 'whence', 'isn', 'if', 'between', 'every', 'couldnt', 'yourself', 'what', 'weren', 'therein', 'de', 'mightn', 'more', 'ours', 'became', 'eg', 'take', 'have', 'latterly', 'go', 'etc', 'already', 'with', 'wherein', 'from', 'other', 'herself', \"shouldn't\", 'beforehand', 'call', 'off', 'beside', 'whether', 'sixty', 'somehow', 'in', 'fifty', 'otherwise', 'whatever', 'toward', 'did', 'elsewhere', 'didn', 't', 'sometime', 'hereby', 'moreover', 'show', 'detail', 'no', 'hasnt', 'however', 'side', 'anyway', 'theirs', 'is', 'put', 'interest', 'it', 'by', 'else', 'y', 'whither', 'after', 'fill', 'becoming', 'describe', 'are', 'doing', 'something', 'under'}), 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f40709b1430>}\n",
      "Run time: 137.79279041290283 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing normalize => not good\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__binary': [False],\n",
    "    'vect__preprocessor': [preprocess_text],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000],\n",
    "    'normalizer__norm': ['l2','l1',None]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"normalizer\", normalizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5supRSrA9emy",
    "outputId": "665f6d04-c394-4ecd-c942-cbaed4b6c4fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "The best accuracy is 87.888.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'selecter__k': 3000, 'vect__binary': False, 'vect__preprocessor': <function preprocess_text at 0x7f6d6a362670>, 'vect__stop_words': ['to', 'made', 'his', 'sometime', 'those', 'except', 'no', 'which', 'shan', 'detail', 'when', 'con', 'haven', \"needn't\", 'take', 'ever', 'would', \"shan't\", 'side', 'thick', 'thereby', 'hence', 'third', 'ours', 'six', 'she', 'then', 'seeming', 'therein', 'front', 'show', 'below', 'amoungst', 'none', \"couldn't\", 'keep', 'bottom', 'hereby', 'wherein', 'latterly', 'we', 'upon', 'must', 'once', 'more', 'therefore', 'doesn', 'same', 'seems', 'sincere', 'last', 'whence', 'inc', 'about', \"she's\", 'needn', 'just', 'somewhere', \"don't\", 'anything', 'empty', 'via', 'often', 'me', 'whole', 'together', 'call', 'whoever', 'everything', 'former', 'am', 'others', 'elsewhere', 'may', 'thereafter', \"you've\", 'seem', \"wouldn't\", 'himself', 'both', 'ltd', 'because', 'hasnt', 'hereupon', 'even', 'herein', 'could', 'among', 'part', 'should', 'system', 'hadn', 'under', 'yourselves', \"wasn't\", 'fill', 'who', 'a', 'mill', 'whose', 'whether', 'nothing', 'hereafter', 'any', 'are', 'here', 'nor', 'anyhow', \"didn't\", 'ma', 'as', 'before', 'd', 'many', 'cant', 'own', 'whereafter', 'otherwise', 'not', 'my', 'hundred', 'against', 'what', 'two', 'twelve', 'onto', 'eg', 'serious', \"you're\", 'of', 'well', 'the', 'us', 'your', 'he', 'thereupon', 'along', \"hadn't\", 'etc', 'whereby', 'been', 'from', 'theirs', 've', 'mustn', 'though', 'ten', 'in', 'never', \"you'd\", 'least', 'having', 'every', 'with', 'without', 'beforehand', 'i', 'across', 'full', 'couldnt', 'somehow', 'again', 'how', 'becomes', 'you', 'four', 'five', 'until', 'y', 'might', 'll', 'mostly', 'co', 'beside', 'mightn', 'wasn', \"mightn't\", 'other', 'already', 'her', \"it's\", \"isn't\", 'always', 'by', 'indeed', 'around', 'either', 'noone', 'although', 'beyond', 'or', 'yours', 'is', 'cannot', 'shouldn', 'cry', 'o', 'for', 'only', 'mine', 'out', 'isn', 'less', \"mustn't\", 'fire', 'fifteen', 'nowhere', 'into', 'nevertheless', 'seemed', 'moreover', 'now', \"won't\", 'each', 'why', 'eleven', 'at', 'very', 'besides', 'amongst', 'didn', 'don', 'couldn', 'describe', 'ourselves', 'be', 'thin', 'such', 'during', 'someone', \"you'll\", 'on', 'thence', 'herself', 'rather', 'twenty', 'get', 'toward', 'everyone', 'won', 'too', 'else', 'wouldn', 'if', 'find', 'also', 'eight', 'whereas', \"that'll\", 'bill', 'itself', 'since', 'sometimes', 'these', 'this', 'off', 'interest', 'where', 'above', 'alone', 'up', \"doesn't\", 'hers', 'some', 'it', \"shouldn't\", 'un', 'have', 'anyone', 'please', 'fifty', \"weren't\", 'all', 'neither', 'they', 'afterwards', \"aren't\", 'wherever', 'becoming', 'and', 'whereupon', 's', 'name', 'that', 'something', 'has', 'enough', 'further', 'an', 'meanwhile', 'will', 'next', 'thru', 'another', 'perhaps', 'still', 'forty', 'one', 'whatever', 'doing', 'being', 'several', 'sixty', 'everywhere', 'move', 'yourself', 'per', 'whither', 'put', 'give', 'nobody', 'than', 'however', 'aren', 'through', 'namely', 'can', 'almost', 'latter', 'themselves', 'was', 'anywhere', 'there', 'behind', 'formerly', 'its', 'three', 'much', 'nine', 're', 'does', 'back', 'most', 'between', 'within', 'down', 'de', 'over', 'anyway', 'their', 'were', 'amount', 'weren', \"should've\", 'while', 'towards', 'ain', 'few', 'hasn', 'but', 'become', 'throughout', 't', 'whenever', 'had', 'top', 'ie', 'myself', 'so', 'see', \"haven't\", 'yet', 'done', 'after', 'whom', 'first', 'them', \"hasn't\", 'go', 'found', 'due', 'became', 'm', 'did', 'him', 'do', 'thus', 'our']}\n",
      "Run time: 16.844464778900146 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing tfidf  => not good\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    #'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__binary': [False],\n",
    "    'vect__preprocessor': [preprocess_text],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "normalizer = Normalizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"normalizer\", normalizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4mF2ueu-eyz",
    "outputId": "d27669bd-a1b2-4cd5-c4ac-d30847e184e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 86.074.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 2, 'selecter__k': 3000, 'vect__binary': False, 'vect__preprocessor': <function preprocess_text at 0x7f407144cee0>, 'vect__stop_words': frozenset({'now', 'along', 'empty', 'don', 'yours', 'well', 'll', 'about', 'four', 'top', 'serious', 'yourselves', 'than', 'both', 'due', 'and', 'into', 'her', 'thereby', 've', 'except', 'see', 'i', 'down', 'ourselves', 'as', 'thick', 'must', 'do', 'she', 'my', 'own', 'us', 'thus', 'very', 'of', 'wasn', 'your', 'its', 'he', 'former', 'yet', 'almost', 'wherever', 'any', 'had', 'that', 'an', 'itself', \"shan't\", 'nine', 'besides', 'some', 'whereafter', 'who', 'haven', 'thence', 'namely', 'would', 'everything', 'others', 'seems', 'ain', 'ma', 'rather', \"aren't\", 'while', \"mightn't\", 'needn', \"you'll\", 'beyond', \"wouldn't\", 'five', 'them', 'thru', 'several', 'two', 'name', 'bottom', \"couldn't\", 'per', 'most', \"doesn't\", 'ltd', 'give', \"wasn't\", 'without', 'get', 'ten', \"don't\", 'couldn', 'hasn', 'made', 'or', \"weren't\", \"hadn't\", 'how', 'found', 'anyhow', 'against', 'myself', 'to', 'always', \"won't\", 'here', 'has', 'co', 'around', 'does', \"you're\", 'before', 'sincere', 'anything', \"you'd\", 'becomes', 'their', 'hereupon', 'hadn', 'inc', 'having', 'whoever', 'until', 'within', 'd', 'because', 'above', 'part', 'we', 'm', 'afterwards', \"mustn't\", 'hundred', 'perhaps', 'via', 'three', 'mine', 'where', 'nowhere', 'few', 'thereupon', 'upon', 'whole', 'then', 'somewhere', 'less', 'for', 'often', 'ever', 'amount', 'neither', 'front', \"it's\", 'these', 'onto', 'they', 'meanwhile', 'twelve', 'all', 'being', 'last', 'towards', 'below', 'many', 'six', 'o', 'seeming', 'throughout', 'together', 'again', \"that'll\", 'may', 'un', 'seemed', 'doesn', 'amoungst', 'con', 'anyone', 'each', 'shan', 'forty', 'am', 'across', 'over', 'everyone', 'this', 'hence', 'herein', 'full', 'fifteen', 'so', 'least', 'only', 'another', 'third', 'please', 'thereafter', 'sometimes', 'there', 'never', 'can', 'nevertheless', 'when', 'whereupon', 'him', 'not', 'such', 'next', 'those', 'why', 'himself', 'could', 'same', 'should', 'shouldn', 'our', 're', \"didn't\", 'just', 'back', 'first', 'alone', 'since', 'hers', 'still', 'whenever', 'won', 'anywhere', 'further', 'seem', 'during', 'thin', 'might', \"should've\", 'was', 'even', 'move', 'fire', 'bill', 'been', 's', 'up', 'at', 'whereas', 'will', 'too', 'eleven', 'mill', 'system', 'whom', 'noone', 'out', 'which', 'but', 'hereafter', 'among', 'cant', 'either', 'nobody', \"she's\", 'eight', 'indeed', \"needn't\", 'cry', 'a', 'nothing', 'on', 'also', 'ie', 'find', 'keep', 'themselves', \"haven't\", 'formerly', 'though', 'someone', 'behind', 'twenty', 'everywhere', 'whose', 'wouldn', \"you've\", 'therefore', 'be', 'cannot', 'were', 'none', 'one', 'aren', 'mustn', 'whereby', 'through', \"hasn't\", 'enough', 'once', 'mostly', 'much', 'although', 'his', 'me', 'become', 'amongst', 'the', \"isn't\", 'done', 'latter', 'you', 'nor', 'whence', 'isn', 'if', 'between', 'every', 'couldnt', 'yourself', 'what', 'weren', 'therein', 'de', 'mightn', 'more', 'ours', 'became', 'eg', 'take', 'have', 'latterly', 'go', 'etc', 'already', 'with', 'wherein', 'from', 'other', 'herself', \"shouldn't\", 'beforehand', 'call', 'off', 'beside', 'whether', 'sixty', 'somehow', 'in', 'fifty', 'otherwise', 'whatever', 'toward', 'did', 'elsewhere', 'didn', 't', 'sometime', 'hereby', 'moreover', 'show', 'detail', 'no', 'hasnt', 'however', 'side', 'anyway', 'theirs', 'is', 'put', 'interest', 'it', 'by', 'else', 'y', 'whither', 'after', 'fill', 'becoming', 'describe', 'are', 'doing', 'something', 'under'}), 'vect__tokenizer': <__main__.StemTokenizer object at 0x7f407120e880>}\n",
      "Run time: 208.76408529281616 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing stemmization => does not improve\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library,None],\n",
    "    'vect__tokenizer': [StemTokenizer()],\n",
    "    'vect__binary': [False],\n",
    "    'vect__preprocessor': [preprocess_text],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000],}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"normalizer\", normalizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqGIA4SUAEOp",
    "outputId": "b464ccbd-d729-4a3b-c853-679e975e2752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 86.351.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 2, 'selecter__k': 3000, 'vect__binary': False, 'vect__preprocessor': <function preprocess_text at 0x7f407144cee0>, 'vect__stop_words': frozenset({'now', 'along', 'empty', 'don', 'yours', 'well', 'll', 'about', 'four', 'top', 'serious', 'yourselves', 'than', 'both', 'due', 'and', 'into', 'her', 'thereby', 've', 'except', 'see', 'i', 'down', 'ourselves', 'as', 'thick', 'must', 'do', 'she', 'my', 'own', 'us', 'thus', 'very', 'of', 'wasn', 'your', 'its', 'he', 'former', 'yet', 'almost', 'wherever', 'any', 'had', 'that', 'an', 'itself', \"shan't\", 'nine', 'besides', 'some', 'whereafter', 'who', 'haven', 'thence', 'namely', 'would', 'everything', 'others', 'seems', 'ain', 'ma', 'rather', \"aren't\", 'while', \"mightn't\", 'needn', \"you'll\", 'beyond', \"wouldn't\", 'five', 'them', 'thru', 'several', 'two', 'name', 'bottom', \"couldn't\", 'per', 'most', \"doesn't\", 'ltd', 'give', \"wasn't\", 'without', 'get', 'ten', \"don't\", 'couldn', 'hasn', 'made', 'or', \"weren't\", \"hadn't\", 'how', 'found', 'anyhow', 'against', 'myself', 'to', 'always', \"won't\", 'here', 'has', 'co', 'around', 'does', \"you're\", 'before', 'sincere', 'anything', \"you'd\", 'becomes', 'their', 'hereupon', 'hadn', 'inc', 'having', 'whoever', 'until', 'within', 'd', 'because', 'above', 'part', 'we', 'm', 'afterwards', \"mustn't\", 'hundred', 'perhaps', 'via', 'three', 'mine', 'where', 'nowhere', 'few', 'thereupon', 'upon', 'whole', 'then', 'somewhere', 'less', 'for', 'often', 'ever', 'amount', 'neither', 'front', \"it's\", 'these', 'onto', 'they', 'meanwhile', 'twelve', 'all', 'being', 'last', 'towards', 'below', 'many', 'six', 'o', 'seeming', 'throughout', 'together', 'again', \"that'll\", 'may', 'un', 'seemed', 'doesn', 'amoungst', 'con', 'anyone', 'each', 'shan', 'forty', 'am', 'across', 'over', 'everyone', 'this', 'hence', 'herein', 'full', 'fifteen', 'so', 'least', 'only', 'another', 'third', 'please', 'thereafter', 'sometimes', 'there', 'never', 'can', 'nevertheless', 'when', 'whereupon', 'him', 'not', 'such', 'next', 'those', 'why', 'himself', 'could', 'same', 'should', 'shouldn', 'our', 're', \"didn't\", 'just', 'back', 'first', 'alone', 'since', 'hers', 'still', 'whenever', 'won', 'anywhere', 'further', 'seem', 'during', 'thin', 'might', \"should've\", 'was', 'even', 'move', 'fire', 'bill', 'been', 's', 'up', 'at', 'whereas', 'will', 'too', 'eleven', 'mill', 'system', 'whom', 'noone', 'out', 'which', 'but', 'hereafter', 'among', 'cant', 'either', 'nobody', \"she's\", 'eight', 'indeed', \"needn't\", 'cry', 'a', 'nothing', 'on', 'also', 'ie', 'find', 'keep', 'themselves', \"haven't\", 'formerly', 'though', 'someone', 'behind', 'twenty', 'everywhere', 'whose', 'wouldn', \"you've\", 'therefore', 'be', 'cannot', 'were', 'none', 'one', 'aren', 'mustn', 'whereby', 'through', \"hasn't\", 'enough', 'once', 'mostly', 'much', 'although', 'his', 'me', 'become', 'amongst', 'the', \"isn't\", 'done', 'latter', 'you', 'nor', 'whence', 'isn', 'if', 'between', 'every', 'couldnt', 'yourself', 'what', 'weren', 'therein', 'de', 'mightn', 'more', 'ours', 'became', 'eg', 'take', 'have', 'latterly', 'go', 'etc', 'already', 'with', 'wherein', 'from', 'other', 'herself', \"shouldn't\", 'beforehand', 'call', 'off', 'beside', 'whether', 'sixty', 'somehow', 'in', 'fifty', 'otherwise', 'whatever', 'toward', 'did', 'elsewhere', 'didn', 't', 'sometime', 'hereby', 'moreover', 'show', 'detail', 'no', 'hasnt', 'however', 'side', 'anyway', 'theirs', 'is', 'put', 'interest', 'it', 'by', 'else', 'y', 'whither', 'after', 'fill', 'becoming', 'describe', 'are', 'doing', 'something', 'under'}), 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f4071487370>}\n",
      "Run time: 47.835500717163086 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing custom => 86.351.\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__binary': [False],\n",
    "    'vect__preprocessor': [preprocess_text],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urkl13XaA5kg",
    "outputId": "3f0bb30a-3b6b-42e8-e189-a942b908130c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 86.21.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'selecter__k': 5000, 'vect__binary': False, 'vect__stop_words': frozenset({'now', 'along', 'empty', 'don', 'yours', 'well', 'll', 'about', 'four', 'top', 'serious', 'yourselves', 'than', 'both', 'due', 'and', 'into', 'her', 'thereby', 've', 'except', 'see', 'i', 'down', 'ourselves', 'as', 'thick', 'must', 'do', 'she', 'my', 'own', 'us', 'thus', 'very', 'of', 'wasn', 'your', 'its', 'he', 'former', 'yet', 'almost', 'wherever', 'any', 'had', 'that', 'an', 'itself', \"shan't\", 'nine', 'besides', 'some', 'whereafter', 'who', 'haven', 'thence', 'namely', 'would', 'everything', 'others', 'seems', 'ain', 'ma', 'rather', \"aren't\", 'while', \"mightn't\", 'needn', \"you'll\", 'beyond', \"wouldn't\", 'five', 'them', 'thru', 'several', 'two', 'name', 'bottom', \"couldn't\", 'per', 'most', \"doesn't\", 'ltd', 'give', \"wasn't\", 'without', 'get', 'ten', \"don't\", 'couldn', 'hasn', 'made', 'or', \"weren't\", \"hadn't\", 'how', 'found', 'anyhow', 'against', 'myself', 'to', 'always', \"won't\", 'here', 'has', 'co', 'around', 'does', \"you're\", 'before', 'sincere', 'anything', \"you'd\", 'becomes', 'their', 'hereupon', 'hadn', 'inc', 'having', 'whoever', 'until', 'within', 'd', 'because', 'above', 'part', 'we', 'm', 'afterwards', \"mustn't\", 'hundred', 'perhaps', 'via', 'three', 'mine', 'where', 'nowhere', 'few', 'thereupon', 'upon', 'whole', 'then', 'somewhere', 'less', 'for', 'often', 'ever', 'amount', 'neither', 'front', \"it's\", 'these', 'onto', 'they', 'meanwhile', 'twelve', 'all', 'being', 'last', 'towards', 'below', 'many', 'six', 'o', 'seeming', 'throughout', 'together', 'again', \"that'll\", 'may', 'un', 'seemed', 'doesn', 'amoungst', 'con', 'anyone', 'each', 'shan', 'forty', 'am', 'across', 'over', 'everyone', 'this', 'hence', 'herein', 'full', 'fifteen', 'so', 'least', 'only', 'another', 'third', 'please', 'thereafter', 'sometimes', 'there', 'never', 'can', 'nevertheless', 'when', 'whereupon', 'him', 'not', 'such', 'next', 'those', 'why', 'himself', 'could', 'same', 'should', 'shouldn', 'our', 're', \"didn't\", 'just', 'back', 'first', 'alone', 'since', 'hers', 'still', 'whenever', 'won', 'anywhere', 'further', 'seem', 'during', 'thin', 'might', \"should've\", 'was', 'even', 'move', 'fire', 'bill', 'been', 's', 'up', 'at', 'whereas', 'will', 'too', 'eleven', 'mill', 'system', 'whom', 'noone', 'out', 'which', 'but', 'hereafter', 'among', 'cant', 'either', 'nobody', \"she's\", 'eight', 'indeed', \"needn't\", 'cry', 'a', 'nothing', 'on', 'also', 'ie', 'find', 'keep', 'themselves', \"haven't\", 'formerly', 'though', 'someone', 'behind', 'twenty', 'everywhere', 'whose', 'wouldn', \"you've\", 'therefore', 'be', 'cannot', 'were', 'none', 'one', 'aren', 'mustn', 'whereby', 'through', \"hasn't\", 'enough', 'once', 'mostly', 'much', 'although', 'his', 'me', 'become', 'amongst', 'the', \"isn't\", 'done', 'latter', 'you', 'nor', 'whence', 'isn', 'if', 'between', 'every', 'couldnt', 'yourself', 'what', 'weren', 'therein', 'de', 'mightn', 'more', 'ours', 'became', 'eg', 'take', 'have', 'latterly', 'go', 'etc', 'already', 'with', 'wherein', 'from', 'other', 'herself', \"shouldn't\", 'beforehand', 'call', 'off', 'beside', 'whether', 'sixty', 'somehow', 'in', 'fifty', 'otherwise', 'whatever', 'toward', 'did', 'elsewhere', 'didn', 't', 'sometime', 'hereby', 'moreover', 'show', 'detail', 'no', 'hasnt', 'however', 'side', 'anyway', 'theirs', 'is', 'put', 'interest', 'it', 'by', 'else', 'y', 'whither', 'after', 'fill', 'becoming', 'describe', 'are', 'doing', 'something', 'under'}), 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f406360e400>}\n",
      "Run time: 46.78005290031433 seconds\n"
     ]
    }
   ],
   "source": [
    "#removing custom preprocessor => 86.21\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [stop_words_library],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__binary': [False],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jd92xn6HBRME",
    "outputId": "3495aa05-e881-43cd-e5d8-eb168eb1fbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'ai', 'base', 'bite', 'ca', 'comment', 'concern', 'consider', 'exclude', 'follow', 'gon', 'greet', 'leave', \"n't\", 'na', 'regard', 'sit', 'site', 'wan', 'web', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 84.408.\n",
      "The winning parameters are {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 2, 'normalizer__norm': 'l1', 'selecter__k': 3000, 'vect__binary': False, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f6d73fe0dc0>}\n",
      "Run time: 170.62921714782715 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing Ngram\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'clf__criterion': ['entropy'],\n",
    "    'vect__stop_words': [list(stop_words_custom)],\n",
    "    'vect__tokenizer': [LemmaTokenizer_word()],\n",
    "    'vect__binary': [False],\n",
    "    'vect__ngram_range':[(1,1)],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000],\n",
    "    \"normalizer__norm\": ['l2','l1']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"normalizer\",normalizer),(\"selecter\", selecter),(\"clf\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQeBdw7h5p4U",
    "outputId": "08bf0e13-8327-40bc-b355-25a5eec72a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "The best accuracy is 88.444.\n",
      "The winning parameters are {'clf__criterion': 'gini', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5, 'selecter__k': 5000, 'vect__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n",
      "Run time: 14.492570400238037 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#testing features\n",
    "t_start = time.time()\n",
    "\n",
    "final_pipe_params = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'vect__stop_words': [list(stop_words_custom)],\n",
    "    'clf__max_depth': [100],\n",
    "    'clf__min_samples_split': [2, 5],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'selecter__k':[5000,3000]\n",
    "}\n",
    "\n",
    "final_vectorizer = CountVectorizer()\n",
    "final_selecter = SelectKBest(chi2)\n",
    "final_model = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", final_vectorizer),(\"selecter\", final_selecter),(\"clf\",final_model)]\n",
    ")\n",
    "\n",
    "final_grid = model_selection.GridSearchCV(pipe, final_pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "final_grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(final_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {final_grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkTDl8nB6HXE",
    "outputId": "c17731d6-15a5-40d1-f2fe-a90b6a59a1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.444\n",
      "Run time: 14.492570400238037 seconds\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "print(round(final_grid.best_score_ * 100,3))\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "y_pred = final_grid.predict(test_x)\n",
    "create_test_csv(y_pred,\"DesicionTree_04032023_02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgKsph5q73Z_"
   },
   "outputs": [],
   "source": [
    "def print_best_params(grid):\n",
    "  bestParameters = grid.best_estimator_.get_params()\n",
    "  # print(bestParameters)\n",
    "  for paramName in sorted(bestParameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (paramName, bestParameters[paramName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzClkWGh74Uy",
    "outputId": "64ac0fb7-38b3-4d3d-d04b-8ef46bc98aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf: DecisionTreeClassifier(max_depth=100, min_samples_split=5)\n",
      "\tclf__ccp_alpha: 0.0\n",
      "\tclf__class_weight: None\n",
      "\tclf__criterion: 'gini'\n",
      "\tclf__max_depth: 100\n",
      "\tclf__max_features: None\n",
      "\tclf__max_leaf_nodes: None\n",
      "\tclf__min_impurity_decrease: 0.0\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 5\n",
      "\tclf__min_weight_fraction_leaf: 0.0\n",
      "\tclf__random_state: None\n",
      "\tclf__splitter: 'best'\n",
      "\tmemory: None\n",
      "\tselecter: SelectKBest(k=5000, score_func=<function chi2 at 0x7f6d76ec2b80>)\n",
      "\tselecter__k: 5000\n",
      "\tselecter__score_func: <function chi2 at 0x7f6d76ec2b80>\n",
      "\tsteps: [('vect', CountVectorizer(stop_words=['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me',\n",
      "                            'myself', 'you', \"you'll\", \"you'd\", \"you're\",\n",
      "                            \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\",\n",
      "                            'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her',\n",
      "                            'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', ...])), ('selecter', SelectKBest(k=5000, score_func=<function chi2 at 0x7f6d76ec2b80>)), ('clf', DecisionTreeClassifier(max_depth=100, min_samples_split=5))]\n",
      "\tvect: CountVectorizer(stop_words=['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me',\n",
      "                            'myself', 'you', \"you'll\", \"you'd\", \"you're\",\n",
      "                            \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\",\n",
      "                            'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her',\n",
      "                            'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', ...])\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__binary: False\n",
      "\tvect__decode_error: 'strict'\n",
      "\tvect__dtype: <class 'numpy.int64'>\n",
      "\tvect__encoding: 'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__preprocessor: None\n",
      "\tvect__stop_words: ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: '(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\tverbose: False\n"
     ]
    }
   ],
   "source": [
    "print_best_params(final_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3TjAAZcppJT"
   },
   "outputs": [],
   "source": [
    "# Step 5: Make predictions on test data using the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvY1Dm14OKwi"
   },
   "outputs": [],
   "source": [
    "######################################################### final"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
