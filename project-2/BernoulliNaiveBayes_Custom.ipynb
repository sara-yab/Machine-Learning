{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wX8F4jLMW0Ht",
    "outputId": "3d205810-8635-45e9-c0ac-94bde64e227f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from enum import Enum\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGN4cdGRjOvO",
    "outputId": "d112d30d-cc7e-4795-bfa8-1be7e62b573f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1olisKyl8R5",
    "outputId": "267b0188-d90a-4f22-d14a-39576f210b26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body', 'subreddit']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_initial.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "iI1xzH2lmgOG",
    "outputId": "f0967602-032a-45e4-f56f-4e9aae92204f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4d15aca2-a800-49cb-954c-a0cf76baa721\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>636</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hi there /u/LakotaPride! Welcome to /r/Trump. ...</td>\n",
       "      <td>Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>30</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d15aca2-a800-49cb-954c-a0cf76baa721')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4d15aca2-a800-49cb-954c-a0cf76baa721 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4d15aca2-a800-49cb-954c-a0cf76baa721');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                     body subreddit\n",
       "count                                                 718       718\n",
       "unique                                                636         4\n",
       "top     Hi there /u/LakotaPride! Welcome to /r/Trump. ...     Obama\n",
       "freq                                                   30       180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_initial.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "DuIRc5L_nKGJ",
    "outputId": "b08e91b8-173b-44d0-8246-a26b82283837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama    180\n",
      "Trump    180\n",
      "Ford     179\n",
      "Musk     179\n",
      "Name: subreddit, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'subreddits')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZVklEQVR4nO3deZQlZZnn8e9PFkWkQQXzoKCFDupxRU0Z90kXulFnQGwXGMUF29IeUdrljGir0KLTKqKj41q0iLYOaIssLTSKtgm2I8oiAiLIItoggiIKDYICz/wRkcG1zKyKyps3b1bm93POPRnxxvbkU1n5ZMQb8UaqCkmSAO407gAkSUuHRUGS1LEoSJI6FgVJUseiIEnqbDruAIax7bbb1qpVq8YdhiRtVM4666xfVdV2sy3bqIvCqlWrOPPMM8cdhiRtVJL8dK5lXj6SJHUsCpKkjkVBktSxKEiSOhYFSVLHoiBJ6lgUJEkdi4IkqWNRkCR1Nuonmoe16sATxx3CWF3+nmcPtb35Gy5/0lK0oouCNE4WVf8oGcao/igZ2eWjJEckuSbJ+QNtX0hyTvu5PMk5bfuqJL8bWPaJUcUlSZrbKM8UjgQ+Anx2pqGqXjgzneQw4LcD619aVbuMMB5J0nqMrChU1WlJVs22LEmAFwBPG9XxJUkbblx9Ck8Grq6qiwfadkryfeB64G1V9a3ZNkyyGlgNMDExwfT09KhjXbbM3XDM33DM33BGlb9xFYV9gKMG5q8C7ltV1yZ5DHBckodW1fVrb1hVa4A1AJOTkzU1NTX/KE5e2R1VQ+UOzJ/5G4r5G87Q+ZvDoj+nkGRT4LnAF2baquqWqrq2nT4LuBR44GLHJkkr3TgeXnsGcGFVXTHTkGS7JJu00/cHdgYuG0NskrSijfKW1KOA7wAPSnJFkle0i/bmjy8dATwFOLe9RfVLwKur6tejik2SNLtR3n20zxztL5ul7RjgmFHFIknqx7GPJEkdi4IkqWNRkCR1LAqSpI5FQZLUsShIkjoWBUlSx6IgSepYFCRJHYuCJKljUZAkdSwKkqSORUGS1LEoSJI6FgVJUseiIEnqWBQkSR2LgiSpY1GQJHUsCpKkzsiKQpIjklyT5PyBtoOTXJnknPbzrIFlb0lySZKLkvzFqOKSJM1tlGcKRwK7z9L+warapf2cBJDkIcDewEPbbT6WZJMRxiZJmsXIikJVnQb8uufqewJHV9UtVfUT4BJg11HFJkma3aZjOOb+SV4CnAm8saquA+4DnD6wzhVt259IshpYDTAxMcH09PRoo13GzN1wzN9wzN9wRpW/xS4KHwcOAar9ehiw34bsoKrWAGsAJicna2pqav7RnHzi/LddBobKHZg/8zcU8zecofM3h0W9+6iqrq6q26rqduBw7rhEdCWw48CqO7RtkqRFtKhFIcn2A7N7ATN3Jp0A7J3kzkl2AnYGvreYsUmSRnj5KMlRwBSwbZIrgIOAqSS70Fw+uhx4FUBV/TDJF4ELgFuB11TVbaOKTZI0u5EVharaZ5bmT61j/XcD7x5VPJKk9fOJZklSx6IgSepYFCRJHYuCJKljUZAkdSwKkqTOeotCkicm2bKdfnGSDyS53+hDkyQttj5nCh8HbkrySOCNwKXAZ0calSRpLPoUhVurqmiGt/5IVX0U2Gq0YUmSxqHPE803JHkL8GLgKUnuBGw22rAkSePQ50zhhcAtwCuq6hc0I5geOtKoJElj0etMAfhQVd2W5IHAg4GjRhuWJGkc+pwpnAbcOcl9gK8B+9K8f1mStMz0KQqpqpuA5wIfq6rnAw8bbViSpHHoVRSSPB54ETDz/jsfepOkZajPL/cDgLcAx7Yvw7k/8M3RhiVJGof1djRX1Wk0/Qoz85cBrxtlUJKk8VhvUUiyHfA/gYcCd5lpr6qnjTAuSdIY9Ll89HngQmAn4O9o3q18xghjkiSNSZ+icM+q+hTwh6o6tar2AzxLkKRlqE9R+EP79aokz07yKOAe69soyRFJrkly/kDboUkuTHJukmOTbNO2r0ryuyTntJ9PzOebkSQNp09ReFeSrWlGSH0T8A/A63tsdySw+1ptpwAPq6pHAD+muatpxqVVtUv7eXWP/UuSFlifu4++0k7+Fnhq3x1X1WlJVq3V9rWB2dOB5/XdnyRp9OYsCkn+D1BzLa+qYW9L3Q/4wsD8Tkm+D1wPvK2qvjVHXKuB1QATExNMT08PGcbKZe6GY/6GY/6GM6r8retM4cyRHBFI8rfArTR3NgFcBdy3qq5N8hjguCQPrarr1962qtYAawAmJydrampq/oGcfOL611nGhsodmD/zNxTzN5yh8zeHOYtCVX1mFAdM8jLgvwJPb1/eQ1XdQjM8N1V1VpJLgQcywsIkSfpTfd7RfMrMXULt/N2TfHU+B0uyO82DcHu0g+zNtG+XZJN2+v7AzsBl8zmGJGn++rxPYbuq+s3MTFVdl+Re69soyVHAFLBtkiuAg2juNrozcEoSgNPbO42eArwzyR+A24FXV9WvN/B7kSQNqU9RuC3JfavqZwBJ7sc6OqBnVNU+szR/ao51jwGO6RGLJGmE+hSFvwX+LcmpQIAn0979I0laXvo8p3BykkcDj2ub/qaqfjXasCRJ49DnTIG2CHxlvStKkjZqvkFNktSxKEiSOr0uH7XPEEwMrj9zN5Ikafno8+a119I8Y3A1zTME0NyS+ogRxiVJGoM+ZwoHAA+qqmtHHYwkabz69Cn8O82w2ZKkZW5dQ2e/oZ28DJhOciLtoHUAVfWBEccmSVpk67p8tFX79WftZ/P2I0laptY1dPbfLWYgkqTxW9ShsyVJS1ufjuY/GTobWO/Q2ZKkjU+fonBbkvvOzPQdOluStPFx6GxJUsehsyVJnV5jHwG3AdcAdwEekoSqOm10YUmSxqHP2Ed/RTPUxQ7AOTRnDN8BnjbSyCRJi65PR/MBwGOBn1bVU4FHAb8ZZVCSpPHoUxRurqqbAZLcuaouBB7UZ+dJjkhyTZLzB9ru0T77cHH79e5te5J8OMklSc5t+zEkSYuoT1G4on147TjglCTHAz/tuf8jgd3XajsQ+EZV7Qx8o50HeCawc/tZDXy85zEkSQukz91He7WTByf5JrA1cHKfnVfVaUlWrdW8JzDVTn8GmAbe3LZ/tqoKOD3JNkm2r6qr+hxLkjS8vm9eexKwc1V9Osl2wH2An8zzmBMDv+h/QfNGN9p9/vvAele0bX9UFJKspn1OYmJigunp6XmGIXM3HPM3HPM3nFHlr8/dRwcBkzT9CJ8GNgM+Bzxx2INXVSXZoKejq2oNsAZgcnKypqam5h/AySfOf9tlYKjcgfkzf0Mxf8MZOn9z6NOnsBewB3AjQFX9nDuG1Z6Pq5NsD9B+vaZtvxLYcWC9Hdo2SdIi6VMUft9e5y+AJFsOecwTgJe20y8Fjh9of0l7F9LjgN/anyBJi6tPn8IXk3wS2CbJK4H9gMP77DzJUTSdytsmuQI4CHhPu89X0NzF9IJ29ZOAZwGXADcBL9+A70OStAD63H30/iS7AdfT9Cu8o6pO6bPzqtpnjkVPn2XdAl7TZ7+SpNHodfdRWwR6FQJJ0sarT5+CJGmFsChIkjpzFoUk32i/vnfxwpEkjdO6+hS2T/IEYI8kR9O8da1TVWePNDJJ0qJbV1F4B/B2mofIPrDWssL3KUjSsjNnUaiqLwFfSvL2qjpkEWOSJI1Jn+cUDkmyB/CUtmm6qr4y2rAkSeOw3ruPkvw9zdvXLmg/ByT5X6MOTJK0+Po8vPZsYJequh0gyWeA7wNvHWVgkqTF1/c5hW0GprceQRySpCWgz5nC3wPfb9+6Fpq+hQPXvYkkaWPUp6P5qCTTwGPbpjdX1S9GGpUkaSz6Doh3Fc37DiRJy5hjH0mSOhYFSVJnnUUhySZJLlysYCRJ47XOolBVtwEXJbnvIsUjSRqjPh3Ndwd+mOR7wI0zjVW1x8iikiSNRZ+i8PaFPGCSBwFfGGi6P82IrNsArwR+2ba/tapOWshjS5LWrc9zCqcmuR+wc1V9PcldgU3me8CqugjYBZo+C+BK4Fjg5cAHq+r98923JGk4fQbEeyXwJeCTbdN9gOMW6PhPBy6tqp8u0P4kSUPoc0vqa4AnAtcDVNXFwL0W6Ph7A0cNzO+f5NwkRyS5+wIdQ5LUU58+hVuq6vdJ8zbOJJvSvHltKEk2B/YA3tI2fRw4pN33IcBhwH6zbLcaWA0wMTHB9PT0sKGsWOZuOOZvOOZvOKPKX5+icGqStwJbJNkN+B/APy/AsZ8JnF1VVwPMfAVIcjgw64t8qmoNsAZgcnKypqam5h/BySfOf9tlYKjcgfkzf0Mxf8MZOn9z6HP56ECaO4LOA14FnAS8bQGOvQ8Dl46SbD+wbC/g/AU4hiRpA/S5++j29sU636W5tHNRVQ11+SjJlsBuNEVmxvuS7NIe4/K1lkmSFsF6i0KSZwOfAC6leZ/CTkleVVX/Mt+DVtWNwD3Xatt3vvuTJC2MPn0KhwFPrapLAJI8ADgRmHdRkCQtTX36FG6YKQity4AbRhSPJGmM5jxTSPLcdvLMJCcBX6S53v984IxFiE2StMjWdfnovw1MXw38l3b6l8AWI4tIkjQ2cxaFqnr5YgYiSRq/Pncf7QS8Flg1uL5DZ0vS8tPn7qPjgE/RPMV8+0ijkSSNVZ+icHNVfXjkkUiSxq5PUfhQkoOArwG3zDRW1dkji0qSNBZ9isLDgX2Bp3HH5aNq5yVJy0ifovB84P5V9ftRByNJGq8+TzSfT/P+ZEnSMtfnTGEb4MIkZ/DHfQrekipJy0yfonDQyKOQJC0Jfd6ncOpiBCJJGr8+TzTfwB3vZN4c2Ay4sar+bJSBSZIWX58zha1mppME2BN43CiDkiSNR5+7jzrVOA74i9GEI0kapz6Xj547MHsnYBK4eWQRSZLGps/dR4PvVbgVuJzmEpIkaZnp06cwkvcqJLmc5rWetwG3VtVkknsAX6AZpvty4AVVdd0oji9J+lPreh3nO9axXVXVIQtw/KdW1a8G5g8EvlFV70lyYDv/5gU4jiSph3V1NN84ywfgFYzuF/WewGfa6c8AzxnRcSRJs1jX6zgPm5lOshVwAPBy4GjgsLm22wAFfC1JAZ+sqjXARFVd1S7/BTCx9kZJVgOrASYmJpienl6AUFYmczcc8zcc8zecUeVvnX0K7TX+NwAvovnL/dELeI3/SVV1ZZJ7AackuXBwYVVVWzBYq30NsAZgcnKypqam5h/BySfOf9tlYKjcgfkzf0Mxf8MZOn9zmPPyUZJDgTNoOoMfXlUHL2Snb1Vd2X69BjgW2BW4Osn27fG3B65ZqONJktZvXX0KbwTuDbwN+HmS69vPDUmuH+agSbZsL0mRZEvgz2mG6D4BeGm72kuB44c5jiRpw6yrT2GDnnbeQBPAsc2oGWwK/N+qOrkdnvuLSV4B/BR4wQhjkCStpc/Dawuuqi4DHjlL+7XA0xc/IkkSbODYR5Kk5c2iIEnqWBQkSR2LgiSpY1GQJHUsCpKkjkVBktSxKEiSOhYFSVLHoiBJ6lgUJEkdi4IkqWNRkCR1LAqSpI5FQZLUsShIkjoWBUlSx6IgSepYFCRJHYuCJKmz6EUhyY5JvpnkgiQ/THJA235wkiuTnNN+nrXYsUnSSrfpGI55K/DGqjo7yVbAWUlOaZd9sKreP4aYJEmMoShU1VXAVe30DUl+BNxnseOQJP2pcZwpdJKsAh4FfBd4IrB/kpcAZ9KcTVw3yzargdUAExMTTE9PL1q8y425G475G475G86o8je2opDkbsAxwN9U1fVJPg4cAlT79TBgv7W3q6o1wBqAycnJmpqamn8QJ584/22XgaFyB+bP/A3F/A1n6PzNYSx3HyXZjKYgfL6qvgxQVVdX1W1VdTtwOLDrOGKTpJVsHHcfBfgU8KOq+sBA+/YDq+0FnL/YsUnSSjeOy0dPBPYFzktyTtv2VmCfJLvQXD66HHjVGGKTpBVtHHcf/RuQWRadtNixSJL+mE80S5I6FgVJUseiIEnqWBQkSR2LgiSpY1GQJHUsCpKkjkVBktSxKEiSOhYFSVLHoiBJ6lgUJEkdi4IkqWNRkCR1LAqSpI5FQZLUsShIkjoWBUlSx6IgSepYFCRJnSVXFJLsnuSiJJckOXDc8UjSSrKkikKSTYCPAs8EHgLsk+Qh441KklaOJVUUgF2BS6rqsqr6PXA0sOeYY5KkFSNVNe4YOkmeB+xeVX/Vzu8L/Oeq2n9gndXA6nb2QcBFix7owtkW+NW4g9iImb/hmL/hbMz5u19VbTfbgk0XO5JhVdUaYM2441gISc6sqslxx7GxMn/DMX/DWa75W2qXj64EdhyY36FtkyQtgqVWFM4Adk6yU5LNgb2BE8YckyStGEvq8lFV3Zpkf+CrwCbAEVX1wzGHNUrL4jLYGJm/4Zi/4SzL/C2pjmZJ0ngttctHkqQxsihIkjoWhRFIcluScwY+q+a5n1VJzl/g8JasJJXkcwPzmyb5ZZKvzHN/lyfZduEiXDqS7JDk+CQXJ7k0yYeSbJ7kZUk+Mu74NgZJ7jnwf/QXSa4cmN983PGNy5LqaF5GfldVu2zoRkk2rapbRxDPxuJG4GFJtqiq3wG74S3JfyJJgC8DH6+qPdvhYdYA7waW840ZC6qqrgV2AUhyMPAfVfX+meUr9f+jZwqLJMkuSU5Pcm6SY5PcvW2fTvK/k5wJHJDkMUl+kOQHwGvGG/VYnAQ8u53eBzhqZkGSg5O8aWD+/PZsasskJ7Z5Oz/JCwd3mGSLJP+S5JWL8h2M3tOAm6vq0wBVdRvwemA/4K7Aju3P1cVJDprZKMlxSc5K8sN2ZICZ9v9Icmjb/vUku7bbX5Zkj3adVUm+leTs9vOERf2OF0mSI5N8Isl3gfet42duVZIL2/V/nOTzSZ6R5Ntt3ndt1z84yT8m+U7bvuR/Bi0Ko7HFwGnosW3bZ4E3V9UjgPOAgwbW37yqJqvqMODTwGur6pGLHPNScTSwd5K7AI8Avttjm92Bn1fVI6vqYcDJA8vuBvwzcFRVHb7g0Y7HQ4GzBhuq6nrgZzRn/7sCf0mTv+cnmXnqdr+qegwwCbwuyT3b9i2Bf62qhwI3AO+iOUvbC3hnu841wG5V9WjghcCHR/S9LQU7AE+oqjesZ73/BBwGPLj9/HfgScCbgLcOrPcImkL+eOAdSe694BEvIIvCaPyuqnZpP3sl2RrYpqpObZd/BnjKwPpfAEiyTbveaW37Py5axEtEVZ0LrKI5Szip52bnAbsleW+SJ1fVbweWHQ98uqo+u7CRLmmnVNW17SW4L9P8ooKmEPwAOJ1m5ICd2/bfc0chPQ84tar+0E6vats3Aw5Pch7wTzSjGC9X/9Sefa3PT6rqvKq6neay3Tequcd/MG8Ax1fV76rqV8A3aYr2kmVRWBpuHHcAS8wJwPsZuHTUupU//pm9C0BV/Rh4NM1/xnclecfAOt8Gdm+vwy8XFwCPGWxI8mfAfWlytPbDR5VkCngG8Pj2LPT7tPkD/lB3PLB0O3ALQPvLbqbf8fXA1cAjac40lnNH7OD/x1l/5lq3DEzfPjA/mDeY5d9j2ABHyaKwCNq/XK9L8uS2aV/g1FnW+w3wmyQzf9m9aHEiXHKOAP6uqs5bq/1yml/+JHk0sFM7fW/gpqr6HHDozDqtdwDX0bynY7n4BnDXJC+B7j0khwFHAjfRnDXdI8kWwHNoCuPWwHVVdVOSBwOP28Bjbg1c1RaKfWlGHFgJLmeWn7kNtGeSu7SX66ZohvNZsiwKi+elwKFJzqW54+Gdc6z3cuCjSc4BltNft71V1RVVNds162OAeyT5IbA/8OO2/eHA99qcHURzTXzQATT9PO8bUciLqv2rfi+a/oKLafJwM3dcx/4eTa7OBY6pqjNpLg9tmuRHwHtoLiFtiI8BL20vPz2YlXN2O9fP3IY4l+ay0enAIVX18wWMb8E5zIUkjchst7oudZ4pSJI6nilIkjqeKUiSOhYFSVLHoiBJ6lgUpHVYe+ybBdjfkUmeN0v7VNrRYJPskeTAdvo5SZbz08NaYiwK0pCSLOhow1V1QlW9p519Dst7SAktMRYFrTizjaqagXcvJJlMMj2wySPXHuWy/cv+W0lOAC5Iskk70ugZaUbCfVW7XpJ8JMlFSb4O3Gsgjt3bkTbPBp470P6ydpsnAHvQPPR4TpIHJHldkgvaYxw98mRpxfF9ClqJZkZVfTZAO2Dhe9ex/iNohoXYEvh+khPb9kcDD6uqn6QZivq3VfXYJHcGvp3ka8CjgAfR/LU/QTNu0RHtKLCH04yeeQntoIiDqur/tUXnK1X1pTbWA4GdquqWdgBFaUF5pqCVaF2jqs5mrlEuv1dVP2mn/xx4STvUxneBe9KMQvoUmmG7b2uHN/jXdv0H04yyeXE7bEX3xrn1OBf4fJIX0wzWJi0oi4JWnDlGVR0cDfMua28yx/zg+D+heQ/GzJDpO1XV1xY4dGheQPRRmvjPWOj+DMmioBVnjlFVL+eO4aj/cq1N+oxy+VXgr5Ns1h7jgUm2BE4DXtj2OWwPPLVd/0JgVZIHtPP7zBHuDcBW7T7vBOxYVd8E3kwzcunden/jUg/+laGV6OE0nbe3A38A/hrYAvhUkkOA6bXWnxnlclvaUS6TPHCtdf6B5sUqZ7fvbvglzZ1Dx9L0G1xA82a07wBU1c1tP8SJSW4CvkX7y38tR9O83OZ1wN5tjFvTnJl8uB1uXVowjn0kSep4+UiS1LEoSJI6FgVJUseiIEnqWBQkSR2LgiSpY1GQJHX+P/4yh1CARbOmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " #distribution of each subreddit\n",
    "train_label_counts = train_data_initial[\"subreddit\"].value_counts()\n",
    "print(train_label_counts)\n",
    "unique_labels = train_data_initial[\"subreddit\"].unique()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(zorder=1, axis=\"y\")\n",
    "ax.bar(unique_labels, train_label_counts, zorder=2)\n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_xticklabels(unique_labels)\n",
    "ax.set_ylabel(\"Number of each class\")\n",
    "ax.set_xlabel(\"subreddits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tL3FvF9UoFm0",
    "outputId": "89f7c188-ae65-43dd-e17f-c15b4621046b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is duplicate: 82\n",
      "is null: False\n",
      "maxmium values body         you can collect free energy right now...with a...\n",
      "subreddit                                                Trump\n",
      "dtype: object\n",
      "minimum values body         \\nIf the link is behind a paywall, or for an a...\n",
      "subreddit                                                 Ford\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check if tehre are any duplicates or isnull values,min and max\n",
    "train_data_initial.duplicated().sum()\n",
    "print(\"is duplicate:\",train_data_initial.duplicated().sum())\n",
    "\n",
    "print(\"is null:\",train_data_initial.isnull().values.any())\n",
    "\n",
    "print(\"maxmium values\",train_data_initial.max(axis=0))\n",
    "print(\"minimum values\",train_data_initial.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKLS-MKNqVPX",
    "outputId": "1f9dd686-c021-41b8-f9ce-14a4133c3a1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    718.000000\n",
       "mean     102.512535\n",
       "std      104.806705\n",
       "min        1.000000\n",
       "25%       56.250000\n",
       "50%       60.000000\n",
       "75%       90.500000\n",
       "max      557.000000\n",
       "Name: body, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describe the train data\n",
    "train_data_initial['body'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNmxYWIirZqo",
    "outputId": "8969215c-9e48-4aaf-915a-3136969bb292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    279.000000\n",
       "mean     103.426523\n",
       "std      108.965248\n",
       "min        1.000000\n",
       "25%       59.000000\n",
       "50%       60.000000\n",
       "75%       87.000000\n",
       "max      496.000000\n",
       "Name: body, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describe the test data\n",
    "test_data['body'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kAT5Fp-WtRme"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iXA-jb-Di5pL"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3M52no5Hjii5",
    "outputId": "7767c380-57a3-4d61-a7eb-9137ce43e153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape=> (718, 2)\n",
      "train_data x=> (718,)\n",
      "train_data y=> (718,)\n"
     ]
    }
   ],
   "source": [
    "print('train_data shape=>',train_data.shape)\n",
    "print('train_data x=>',train_x.shape)\n",
    "print('train_data y=>',train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0R5Y-vQKk4dL",
    "outputId": "22a95c8a-eb4e-41a8-9722-3298838f3145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Obama\n",
      "1    Trump\n",
      "2     Musk\n",
      "3     Ford\n",
      "4    Obama\n",
      "Name: subreddit, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pfFMJ3_vDZF",
    "outputId": "3454493c-2138-4057-d4af-c620812d6377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "#stop words\n",
    "sklearn_stop_words = text.ENGLISH_STOP_WORDS\n",
    "print(len(sklearn_stop_words))\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(len(stop_words_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J341-Ry7vkfb"
   },
   "outputs": [],
   "source": [
    "#prior class probability can either be learned or a uniform priority be used\n",
    "prior_probabilities = Enum('prior_probabilities', ['learn', 'uniform'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MueiPcWz2332"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qTd7foSlSszy"
   },
   "outputs": [],
   "source": [
    "class CustomNaiveBayes:\n",
    "    def __init__(self, alpha=0.01, prior=prior_probabilities.learn):\n",
    "        self.alpha = alpha\n",
    "        self.prior = prior\n",
    "\n",
    "    #fit function\n",
    "    def fit(self, X, y):\n",
    "        X = X.toarray()\n",
    "        class_counts = y.value_counts()\n",
    "\n",
    "        num_labels = len(class_counts)\n",
    "\n",
    "        #calculate prior probability\n",
    "        if self.prior == prior_probabilities.learn:\n",
    "          self.class_probabilities = class_counts / len(y)\n",
    "        elif self.prior == prior_probabilities.uniform:\n",
    "           self.class_probabilities = pd.Series(np.repeat(1/num_labels, num_labels), \n",
    "                                               index = class_counts.index)\n",
    "\n",
    "        self.class_probabilities.sort_index(inplace=True)\n",
    "        class_counts.sort_index(inplace=True)\n",
    "        features_count = np.empty((num_labels, X.shape[1]))\n",
    "        y_numpy = y.to_numpy()\n",
    "\n",
    "        for i in range(num_labels):\n",
    "\n",
    "            label = self.class_probabilities.index[i]\n",
    "            X_this_label = X[np.nonzero(y_numpy == label), :]\n",
    "\n",
    "            features_count[i,:] = np.sum(X_this_label, axis=1)\n",
    "\n",
    "        # add Laplace smoothing\n",
    "        smoothed_numerator = features_count + self.alpha\n",
    "        smoothed_denominator = np.sum(smoothed_numerator,axis=1).reshape(-1,1)\n",
    "        self.parameters = pd.DataFrame(smoothed_numerator / smoothed_denominator, index=self.class_probabilities.index)\n",
    "\n",
    "    #predict function    \n",
    "    def predict(self, X):\n",
    "\n",
    "        X = X.toarray()\n",
    "        delta = pd.DataFrame(columns=self.class_probabilities.index)\n",
    "        for label in self.class_probabilities.index:\n",
    "\n",
    "            class_probability = self.class_probabilities[label]\n",
    "            theta_j_class = self.parameters.loc[label, :].to_numpy()\n",
    "            prob_features_given_y = (theta_j_class ** X) * (1 - theta_j_class) ** (\n",
    "                1 - X\n",
    "            )\n",
    "            prob_sample_given_y = np.prod(prob_features_given_y, axis=1)\n",
    "            # Compute final probability\n",
    "            term1 = np.log(class_probability)\n",
    "            term2 = np.sum(X * np.log(theta_j_class), axis=1)\n",
    "            term3 = np.sum((1 - X) * np.log(1 - theta_j_class), axis=1)\n",
    "            delta_k = term1 + term2 + term3\n",
    "            delta[label] = delta_k\n",
    "\n",
    "        predicted_class = delta.idxmax(axis=1)\n",
    "        return predicted_class.to_list()\n",
    "\n",
    "    #score function\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.count_nonzero(y == y_pred) / len(y_pred)\n",
    "        return accuracy\n",
    "       \n",
    "\n",
    "    #get parameters function\n",
    "    def get_params(self, deep=True):\n",
    "        params = {\"alpha\": self.alpha,\n",
    "                  \"prior\": self.prior}\n",
    "        return params\n",
    "\n",
    "    #set parameters function\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iENrtYFoYeAL"
   },
   "outputs": [],
   "source": [
    "#start testing different modes\n",
    "#remove punctuation\n",
    "import re\n",
    "import string\n",
    "\n",
    "data_x_punc = train_x.copy()\n",
    "\n",
    "for i in range(data_x_punc.shape[0]):\n",
    "  data_x_punc[i]= data_x_punc[i].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "\n",
    "test_x_punc = test_x.copy()\n",
    "\n",
    "for i in range(test_x_punc.shape[0]):\n",
    "  test_x_punc[i]= test_x_punc[i].translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyN4Sd0ecMRZ",
    "outputId": "e286499d-8080-4cdf-d884-255536f15314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(718,)\n",
      "(718,)\n",
      "(279,)\n"
     ]
    }
   ],
   "source": [
    "print(data_x_punc.shape)\n",
    "print(train_x.shape)\n",
    "print(test_x_punc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TmehKW7cSDC",
    "outputId": "7fbb5c97-4696-4a62-9800-ba8c16a3e084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "318\n",
      "length custom: 590\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(len(stop_words_nltk))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "print(len(stop_words_sklearn))\n",
    "\n",
    "stop_words_custom = [\n",
    "    # All pronouns and associated words\n",
    "    \"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\n",
    "    \"you'll\",\n",
    "    \"you'd\",\n",
    "    \"you're\",\n",
    "    \"you've\",\n",
    "    \"yourself\",\n",
    "    \"he\",\n",
    "    \"he'll\",\n",
    "    \"he'd\",\n",
    "    \"he's\",\n",
    "    \"him\",\n",
    "    \"she\",\n",
    "    \"she'll\",\n",
    "    \"she'd\",\n",
    "    \"she's\",\n",
    "    \"her\",\n",
    "    \"it\",\n",
    "    \"it'll\",\n",
    "    \"it'd\",\n",
    "    \"it's\",\n",
    "    \"itself\",\n",
    "    \"oneself\",\n",
    "    \"we\",\n",
    "    \"we'll\",\n",
    "    \"we'd\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"us\",\n",
    "    \"ourselves\",\n",
    "    \"they\",\n",
    "    \"they'll\",\n",
    "    \"they'd\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"them\",\n",
    "    \"themselves\",            \n",
    "    \"everyone\",\n",
    "    \"everyone's\",\n",
    "    \"everybody\",\n",
    "    \"everybody's\",\n",
    "    \"someone\",\n",
    "    \"someone's\",\n",
    "    \"somebody\",\n",
    "    \"somebody's\",\n",
    "    \"nobody\",\n",
    "    \"nobody's\",\n",
    "    \"anyone\",\n",
    "    \"anyone's\",\n",
    "    \"everything\",\n",
    "    \"everything's\",\n",
    "    \"something\",\n",
    "    \"something's\",\n",
    "    \"nothing\",\n",
    "    \"nothing's\",\n",
    "    \"anything\",\n",
    "    \"anything's\",\n",
    "    # All determiners and associated words\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"that's\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"my\",\n",
    "    #\"mine\",   #Omitted since mine can refer to something else\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"his\",\n",
    "    \"hers\",\n",
    "    \"its\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"own\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"few\",\n",
    "    \"much\",\n",
    "    \"many\",\n",
    "    \"lot\",\n",
    "    \"lots\",\n",
    "    \"some\",\n",
    "    \"any\",\n",
    "    \"enough\",\n",
    "    \"all\",\n",
    "    \"both\",\n",
    "    \"half\",\n",
    "    \"either\",\n",
    "    \"neither\",\n",
    "    \"each\",\n",
    "    \"every\",\n",
    "    \"certain\",\n",
    "    \"other\",\n",
    "    \"another\",\n",
    "    \"such\",\n",
    "    \"several\",\n",
    "    \"multiple\",\n",
    "    # \"what\",    #Dealt with later on\n",
    "    \"rather\",\n",
    "    \"quite\",\n",
    "    # All prepositions\n",
    "    \"aboard\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"across\",\n",
    "    \"after\",\n",
    "    \"against\",\n",
    "    \"along\",\n",
    "    \"amid\",\n",
    "    \"amidst\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"anti\",\n",
    "    \"around\",\n",
    "    \"as\",\n",
    "    \"at\",\n",
    "    \"away\",\n",
    "    \"before\",\n",
    "    \"behind\",\n",
    "    \"below\",\n",
    "    \"beneath\",\n",
    "    \"beside\",\n",
    "    \"besides\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"but\",\n",
    "    \"by\",\n",
    "    \"concerning\",\n",
    "    \"considering\",\n",
    "    \"despite\",\n",
    "    \"down\",\n",
    "    \"during\",\n",
    "    \"except\",\n",
    "    \"excepting\",\n",
    "    \"excluding\",\n",
    "    \"far\",\n",
    "    \"following\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"here\",\n",
    "    \"here's\",\n",
    "    \"in\",\n",
    "    \"inside\",\n",
    "    \"into\",\n",
    "    \"left\",\n",
    "    \"like\",\n",
    "    \"minus\",\n",
    "    \"near\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"onto\",\n",
    "    \"opposite\",\n",
    "    \"out\",\n",
    "    \"outside\",\n",
    "    \"over\",\n",
    "    \"past\",\n",
    "    \"per\",\n",
    "    \"plus\",\n",
    "    \"regarding\",\n",
    "    \"right\",\n",
    "    #\"round\",   #Omitted\n",
    "    #\"save\",    #Omitted\n",
    "    \"since\",\n",
    "    \"than\",\n",
    "    \"there\",\n",
    "    \"there's\",\n",
    "    \"through\",\n",
    "    \"to\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"under\",\n",
    "    \"underneath\",\n",
    "    \"unlike\",\n",
    "    \"until\",\n",
    "    \"up\",\n",
    "    \"upon\",\n",
    "    \"versus\",\n",
    "    \"via\",\n",
    "    \"with\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    # Irrelevant verbs\n",
    "    \"may\",\n",
    "    \"might\",\n",
    "    \"will\",\n",
    "    \"won't\",\n",
    "    \"would\",\n",
    "    \"wouldn't\",\n",
    "    \"can\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"could\",\n",
    "    \"couldn't\",\n",
    "    \"should\",\n",
    "    \"shouldn't\",\n",
    "    \"must\",\n",
    "    \"must've\",\n",
    "    \"be\",\n",
    "    \"being\",\n",
    "    \"been\",\n",
    "    \"am\",\n",
    "    \"are\",\n",
    "    \"aren't\",\n",
    "    \"ain't\",\n",
    "    \"is\",\n",
    "    \"isn't\",\n",
    "    \"was\",\n",
    "    \"wasn't\",\n",
    "    \"were\",\n",
    "    \"weren't\",\n",
    "    \"do\",\n",
    "    \"doing\",\n",
    "    \"don't\",\n",
    "    \"does\",\n",
    "    \"doesn't\",\n",
    "    \"did\",\n",
    "    \"didn't\",\n",
    "    \"done\",\n",
    "    \"have\",\n",
    "    \"haven't\",\n",
    "    \"having\",\n",
    "    \"has\",\n",
    "    \"hasn't\",\n",
    "    \"had\",\n",
    "    \"hadn't\",\n",
    "    \"get\",\n",
    "    \"getting\",\n",
    "    \"gets\",\n",
    "    \"got\",\n",
    "    \"gotten\",\n",
    "    \"go\",\n",
    "    \"going\",\n",
    "    \"gonna\",\n",
    "    \"goes\",\n",
    "    \"went\",\n",
    "    \"gone\",\n",
    "    \"make\",\n",
    "    \"making\",\n",
    "    \"makes\",\n",
    "    \"made\",\n",
    "    \"take\",\n",
    "    \"taking\",\n",
    "    \"takes\",\n",
    "    \"took\",\n",
    "    \"taken\",\n",
    "    \"need\",\n",
    "    \"needing\",\n",
    "    \"needs\",\n",
    "    \"needed\",\n",
    "    \"use\",\n",
    "    \"using\",\n",
    "    \"uses\",\n",
    "    \"used\",\n",
    "    \"want\",\n",
    "    \"wanna\",\n",
    "    \"wanting\",\n",
    "    \"wants\",\n",
    "    \"let\",\n",
    "    \"lets\",\n",
    "    \"letting\",\n",
    "    \"let's\",\n",
    "    \"suppose\",\n",
    "    \"supposing\",\n",
    "    \"supposes\",\n",
    "    \"supposed\",\n",
    "    \"seem\",\n",
    "    \"seeming\",\n",
    "    \"seems\",\n",
    "    \"seemed\",\n",
    "    \"say\",\n",
    "    \"saying\",\n",
    "    \"says\",\n",
    "    \"said\",\n",
    "    \"know\",\n",
    "    \"knowing\",\n",
    "    \"knows\",\n",
    "    \"knew\",\n",
    "    \"known\",\n",
    "    \"look\",\n",
    "    \"looking\",\n",
    "    \"looked\",\n",
    "    \"think\",\n",
    "    \"thinking\",\n",
    "    \"thinks\",\n",
    "    \"thought\",\n",
    "    \"feel\",\n",
    "    \"feels\",\n",
    "    \"felt\",\n",
    "    \"based\",\n",
    "    \"put\",\n",
    "    \"puts\",\n",
    "    #\"wanted\"   #Omitted since the advective is relevant\n",
    "    # Question words and associated words\n",
    "    \"who\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"who'd\",\n",
    "    \"whoever\",\n",
    "    \"whoever's\",\n",
    "    \"whom\",\n",
    "    \"whomever\",\n",
    "    \"whomever's\",\n",
    "    \"whose\",\n",
    "    \"whosever\",\n",
    "    \"whosever's\",\n",
    "    \"when\",\n",
    "    \"whenever\",\n",
    "    \"which\",\n",
    "    \"whichever\",\n",
    "    \"where\",\n",
    "    \"where's\",\n",
    "    \"where'd\",\n",
    "    \"wherever\",\n",
    "    \"why\",\n",
    "    \"why's\",\n",
    "    \"why'd\",\n",
    "    \"whyever\",\n",
    "    \"what\",\n",
    "    \"what's\",\n",
    "    \"whatever\",\n",
    "    \"whence\",\n",
    "    \"how\",\n",
    "    \"how's\",\n",
    "    \"how'd\",\n",
    "    \"however\",\n",
    "    \"whether\",\n",
    "    \"whatsoever\",\n",
    "    # Connector words and irrelevant adverbs\n",
    "    \"and\",\n",
    "    \"or\",\n",
    "    \"not\",\n",
    "    \"because\",\n",
    "    \"also\",\n",
    "    \"always\",\n",
    "    \"never\",\n",
    "    \"only\",\n",
    "    \"really\",\n",
    "    \"very\",\n",
    "    \"greatly\",\n",
    "    \"extremely\",\n",
    "    \"somewhat\",\n",
    "    \"no\",\n",
    "    \"nope\",\n",
    "    \"nah\",\n",
    "    \"yes\",\n",
    "    \"yep\",\n",
    "    \"yeh\",\n",
    "    \"yeah\",\n",
    "    \"maybe\",\n",
    "    \"perhaps\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"less\",\n",
    "    \"least\",\n",
    "    \"good\",\n",
    "    \"great\",\n",
    "    \"well\",\n",
    "    \"better\",\n",
    "    \"best\",\n",
    "    \"bad\",\n",
    "    \"worse\",\n",
    "    \"worst\",\n",
    "    \"too\",\n",
    "    \"thru\",\n",
    "    \"though\",\n",
    "    \"although\",\n",
    "    \"yet\",\n",
    "    \"already\",\n",
    "    \"then\",\n",
    "    \"even\",\n",
    "    \"now\",\n",
    "    \"sometimes\",\n",
    "    \"still\",\n",
    "    \"together\",\n",
    "    \"altogether\",\n",
    "    \"entirely\",\n",
    "    \"fully\",\n",
    "    \"entire\",\n",
    "    \"whole\",\n",
    "    \"completely\",\n",
    "    \"utterly\",\n",
    "    \"seemingly\",\n",
    "    \"apparently\",\n",
    "    \"clearly\",\n",
    "    \"obviously\",\n",
    "    \"actually\",\n",
    "    \"actual\",\n",
    "    \"usually\",\n",
    "    \"usual\",\n",
    "    \"literally\",\n",
    "    \"honestly\",\n",
    "    \"absolutely\",\n",
    "    \"definitely\",\n",
    "    \"generally\",\n",
    "    \"totally\",\n",
    "    \"finally\",\n",
    "    \"basically\",\n",
    "    \"essentially\",\n",
    "    \"fundamentally\",\n",
    "    \"automatically\",\n",
    "    \"immediately\",\n",
    "    \"necessarily\",\n",
    "    \"primarily\",\n",
    "    \"normally\",\n",
    "    \"perfectly\",\n",
    "    \"constantly\",\n",
    "    \"particularly\",\n",
    "    \"eventually\",\n",
    "    \"hopefully\",\n",
    "    \"mainly\",\n",
    "    \"typically\",\n",
    "    \"specifically\",\n",
    "    \"differently\",\n",
    "    \"appropriately\",\n",
    "    \"plenty\",\n",
    "    \"certainly\",\n",
    "    \"unfortunately\",\n",
    "    \"ultimately\",\n",
    "    \"unlikely\",\n",
    "    \"likely\",\n",
    "    \"potentially\",\n",
    "    \"fortunately\",\n",
    "    \"personally\",\n",
    "    \"directly\",\n",
    "    \"indirectly\",\n",
    "    \"nearly\",\n",
    "    \"closely\",\n",
    "    \"slightly\",\n",
    "    \"probably\",\n",
    "    \"possibly\",\n",
    "    \"especially\",\n",
    "    \"frequently\",\n",
    "    \"often\",\n",
    "    \"oftentimes\",\n",
    "    \"seldom\",\n",
    "    \"rarely\",\n",
    "    \"sure\",\n",
    "    \"while\",\n",
    "    \"whilst\",\n",
    "    \"able\",\n",
    "    \"unable\",\n",
    "    \"else\",\n",
    "    \"ever\",\n",
    "    \"once\",\n",
    "    \"twice\",\n",
    "    \"thrice\",\n",
    "    \"almost\",\n",
    "    \"again\",\n",
    "    \"instead\",\n",
    "    \"next\",\n",
    "    \"previous\",\n",
    "    \"unless\",\n",
    "    \"somehow\",\n",
    "    \"anyhow\",\n",
    "    \"anywhere\",\n",
    "    \"somewhere\",\n",
    "    \"everywhere\",\n",
    "    \"nowhere\",\n",
    "    \"further\",\n",
    "    \"anymore\",\n",
    "    \"later\",\n",
    "    \"ago\",\n",
    "    \"ahead\",\n",
    "    \"just\",\n",
    "    \"same\",\n",
    "    \"different\",\n",
    "    \"big\",\n",
    "    \"small\",\n",
    "    \"little\",\n",
    "    \"tiny\",\n",
    "    \"large\",\n",
    "    \"huge\",\n",
    "    \"pretty\",\n",
    "    \"mostly\",\n",
    "    \"anyway\",\n",
    "    \"anyways\",\n",
    "    \"otherwise\",\n",
    "    \"regardless\",\n",
    "    \"throughout\",\n",
    "    \"additionally\",\n",
    "    \"moreover\",\n",
    "    \"furthermore\",\n",
    "    \"meanwhile\",\n",
    "    \"afterwards\",\n",
    "    # Irrelevant nouns\n",
    "    \"thing\",\n",
    "    \"thing's\",\n",
    "    \"things\",\n",
    "    \"stuff\",\n",
    "    \"other's\",\n",
    "    \"others\",\n",
    "    \"another's\",\n",
    "    \"total\",\n",
    "    \"\",\n",
    "    \"false\",\n",
    "    \"none\",\n",
    "    \"way\",\n",
    "    \"kind\",\n",
    "    # Lettered numbers and order\n",
    "    \"zero\",\n",
    "    \"zeros\",\n",
    "    \"zeroes\",\n",
    "    \"one\",\n",
    "    \"ones\",\n",
    "    \"two\",\n",
    "    \"three\",\n",
    "    \"four\",\n",
    "    \"five\",\n",
    "    \"six\", \n",
    "    \"seven\",\n",
    "    \"eight\",\n",
    "    \"nine\",\n",
    "    \"ten\",\n",
    "    \"twenty\",\n",
    "    \"thirty\",\n",
    "    \"forty\",\n",
    "    \"fifty\",\n",
    "    \"sixty\",\n",
    "    \"seventy\",\n",
    "    \"eighty\",\n",
    "    \"ninety\",\n",
    "    \"hundred\",\n",
    "    \"hundreds\",\n",
    "    \"thousand\",\n",
    "    \"thousands\",\n",
    "    \"million\",\n",
    "    \"millions\",\n",
    "    \"first\",\n",
    "    \"last\",\n",
    "    \"second\",\n",
    "    \"third\",\n",
    "    \"fourth\",\n",
    "    \"fifth\",\n",
    "    \"sixth\",\n",
    "    \"seventh\",\n",
    "    \"eigth\",\n",
    "    \"ninth\",\n",
    "    \"tenth\",\n",
    "    \"firstly\",\n",
    "    \"secondly\",\n",
    "    \"thirdly\",\n",
    "    \"lastly\",\n",
    "    # Greetings and slang\n",
    "    \"hello\",\n",
    "    \"hi\",\n",
    "    \"hey\",\n",
    "    \"sup\",\n",
    "    \"yo\",\n",
    "    \"greetings\",\n",
    "    \"please\",\n",
    "    \"okay\",\n",
    "    \"ok\",\n",
    "    \"y'all\",\n",
    "    \"lol\",\n",
    "    \"rofl\",\n",
    "    \"thank\",\n",
    "    \"thanks\",\n",
    "    \"alright\",\n",
    "    \"kinda\",\n",
    "    \"dont\",\n",
    "    \"sorry\",\n",
    "    \"idk\",\n",
    "    \"tldr\",\n",
    "    \"tl\",\n",
    "    \"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "    \"tbh\",\n",
    "    \"dude\",\n",
    "    \"tho\",\n",
    "    \"aka\",\n",
    "    \"plz\",\n",
    "    \"pls\",\n",
    "    \"bit\",\n",
    "    \"don\",\n",
    "    # Miscellaneous\n",
    "    \"www\",\n",
    "    \"https\",\n",
    "    \"http\",\n",
    "    \"com\",\n",
    "    \"etc\",\n",
    "    \"html\",\n",
    "    \"reddit\",\n",
    "    \"subreddit\",\n",
    "    \"subreddits\",\n",
    "    \"comments\",\n",
    "    \"reply\",\n",
    "    \"replies\",\n",
    "    \"thread\",\n",
    "    \"threads\",\n",
    "    \"post\",\n",
    "    \"posts\",\n",
    "    \"website\",\n",
    "    \"websites\",\n",
    "    \"web site\",\n",
    "    \"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzmWfRon1TRe",
    "outputId": "9c4597ef-aa78-4cbf-fc84-068e113c41e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation from custom stop words\n",
    "stop_words_custom_punc = stop_words_custom\n",
    "print(len(stop_words_custom_punc))\n",
    "for i in range(len(stop_words_custom_punc)):\n",
    " stop_words_custom_punc[i]= stop_words_custom_punc[i].translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6yLa4-Gze907"
   },
   "outputs": [],
   "source": [
    "#lemmatization \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-wP9I5dWnzs",
    "outputId": "d5b94b5e-b76f-40ff-9c3f-314f1df6fe08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 0.9303321678321679.\n",
      "The winning parameters are {'classify__alpha': 0.02, 'selecter__k': 5000, 'vect__stop_words': ['i', 'ill', 'id', 'im', 'ive', 'ive', 'me', 'myself', 'you', 'youll', 'youd', 'youre', 'youve', 'yourself', 'he', 'hell', 'hed', 'hes', 'him', 'she', 'shell', 'shed', 'shes', 'her', 'it', 'itll', 'itd', 'its', 'itself', 'oneself', 'we', 'well', 'wed', 'were', 'weve', 'us', 'ourselves', 'they', 'theyll', 'theyd', 'theyre', 'theyve', 'them', 'themselves', 'everyone', 'everyones', 'everybody', 'everybodys', 'someone', 'someones', 'somebody', 'somebodys', 'nobody', 'nobodys', 'anyone', 'anyones', 'everything', 'everythings', 'something', 'somethings', 'nothing', 'nothings', 'anything', 'anythings', 'a', 'an', 'the', 'this', 'that', 'thats', 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', 'heres', 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', 'theres', 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', 'wont', 'would', 'wouldnt', 'can', 'cant', 'cannot', 'could', 'couldnt', 'should', 'shouldnt', 'must', 'mustve', 'be', 'being', 'been', 'am', 'are', 'arent', 'aint', 'is', 'isnt', 'was', 'wasnt', 'were', 'werent', 'do', 'doing', 'dont', 'does', 'doesnt', 'did', 'didnt', 'done', 'have', 'havent', 'having', 'has', 'hasnt', 'had', 'hadnt', 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', 'lets', 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', 'whos', 'whove', 'whod', 'whoever', 'whoevers', 'whom', 'whomever', 'whomevers', 'whose', 'whosever', 'whosevers', 'when', 'whenever', 'which', 'whichever', 'where', 'wheres', 'whered', 'wherever', 'why', 'whys', 'whyd', 'whyever', 'what', 'whats', 'whatever', 'whence', 'how', 'hows', 'howd', 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', 'things', 'things', 'stuff', 'others', 'others', 'anothers', 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', 'yall', 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etc', 'html', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n",
      "Run time:  4.818 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['site', 'sites', 'web'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#start testing different modes\n",
    "#select alpha\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5],\n",
    "    \"vect__stop_words\": [stop_words_custom_punc],\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"norm\", normalizer), (\"selecter\", selecter), (\"classify\", CustomNaiveBayes())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANdj7yS4yX7O",
    "outputId": "9e50f84c-6ccc-4552-db54-df3ba6c5c4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 0.8996697746697746.\n",
      "The winning parameters are {'classify__alpha': 0.001, 'selecter__k': 5000}\n"
     ]
    }
   ],
   "source": [
    "#start testing different modes\n",
    "#remove normalize \n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"norm\", normalizer), (\"selecter\", selecter), (\"classify\", CustomNaiveBayes())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C05xS8kuyqac",
    "outputId": "f2abfd97-629c-408a-8d42-40030744fd8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 0.9178321678321678.\n",
      "The winning parameters are {'classify__alpha': 0.1, 'selecter__k': 5000}\n",
      "Run time:  6.200 seconds\n"
     ]
    }
   ],
   "source": [
    "#start testing different modes\n",
    "#stop words - scikitlearn\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(stop_words_sklearn))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter), (\"classify\", CustomNaiveBayes())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fX5swP22zYnR",
    "outputId": "94522cc7-7028-4a7f-d550-78bbbc2bf9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 0.910878010878011.\n",
      "The winning parameters are {'classify__alpha': 0.01, 'selecter__k': 5000}\n",
      "Run time:  4.087 seconds\n"
     ]
    }
   ],
   "source": [
    "#stop words - nltk\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(stop_words_nltk))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter), (\"classify\", CustomNaiveBayes())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNyCYJ5ZzljP",
    "outputId": "1a7911a4-995d-4383-8207-bee864209ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "The best accuracy is 0.9359168609168609.\n",
      "The winning parameters are {'classify__alpha': 0.5, 'selecter__k': 5000}\n",
      "Run time:  3.791 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['site', 'sites', 'web'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#stop words - custom\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(stop_words_custom_punc))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter), (\"classify\", CustomNaiveBayes())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7FlSPBcRylkW"
   },
   "outputs": [],
   "source": [
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n",
    "final_stop_words =  set(stop_words_custom) | stop_words_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgMa6SnW20ym",
    "outputId": "17c412a2-0aa4-4372-ba93-d3571095ac42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best accuracy is 0.9429001554001555.\n",
      "The winning parameters are {'classify__alpha': 0.5, 'selecter__k': 5000}\n",
      "Run time:  7.653 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['site', 'sites', 'web'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#create new stop word dictionary\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n",
    "final_stop_words =  set(stop_words_custom_punc) | stop_words_library\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(final_stop_words))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter), (\"classify\", CustomNaiveBayes())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcZAugg94uIF",
    "outputId": "b25765f5-9dac-42b9-b76f-c217fd4b2532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "#generate test.csv\n",
    "y_pred = grid.predict(test_x)\n",
    "\n",
    "create_test_csv(y_pred,\"customNaiveBayes_12032023_02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mscXIH-F2Tu3",
    "outputId": "067f51f8-cafc-4f74-bccf-add8ec98532d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best accuracy is 0.9429001554001555.\n",
      "The winning parameters are {'classify__alpha': 0.5, 'selecter__k': 5000}\n",
      "Run time:  9.414 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['site', 'sites', 'web'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#final stop words - test normalize\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(final_stop_words))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6xglMuh6BPG",
    "outputId": "b4171e86-281d-44de-d1ad-4cb9426f7eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 862, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 672, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be <= n_features = 4794; got 5000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 862, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 672, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be <= n_features = 4751; got 5000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 862, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 672, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be <= n_features = 4832; got 5000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 862, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 672, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be <= n_features = 4884; got 5000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 862, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py\", line 672, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be <= n_features = 4798; got 5000. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.90113636        nan 0.91085859        nan 0.91920163\n",
      "        nan 0.91363636        nan 0.91640443]\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['base', 'bite', 'comment', 'concern', 'consider', 'exclude', 'follow', 'gon', 'greet', 'leave', 'na', 'regard', 'sha', 'sit', 'site', 'wan', 'web', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 0.9192016317016318.\n",
      "The winning parameters are {'classify__alpha': 0.1, 'selecter__k': 3000}\n",
      "Run time:  43.618 seconds\n"
     ]
    }
   ],
   "source": [
    "#test lemmatizer- LemmaTokenizer_Pos\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(final_stop_words),tokenizer=LemmaTokenizer())\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zaxo-k8JD7kL",
    "outputId": "7229527b-374b-4fcd-eef8-0f0c546be92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'base', 'bite', 'comment', 'concern', 'consider', 'exclude', 'follow', 'gon', 'greet', 'leave', \"n't\", 'na', 'regard', 'sha', 'sit', 'site', 'wan', 'web', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 0.8983100233100233.\n",
      "The winning parameters are {'classify__alpha': 0.01, 'selecter__k': 5000}\n",
      "Run time:  50.826 seconds\n"
     ]
    }
   ],
   "source": [
    "#test lemmatizer- LemmaTokenizer_word\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(final_stop_words),tokenizer=LemmaTokenizer_word())\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYTk6puHEg9X",
    "outputId": "3328d5c2-7f5b-4864-99b4-971a3143d68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "The best accuracy is 0.72004662004662.\n",
      "The winning parameters are {}\n",
      "Run time:  6.797 seconds\n"
     ]
    }
   ],
   "source": [
    "#test stemmizer- StemTokenizer\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \n",
    "}\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=30)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt749vbVF76E"
   },
   "outputs": [],
   "source": [
    "#test stemmizer and lemmatizer\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5],\n",
    "    \"countVectorizer__tokenizer\": [StemTokenizer(), LemmaTokenizer_word()]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(final_stop_words))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=1000)\n",
    "\n",
    "grid.fit(train_x,train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6DyFM1yH6rg",
    "outputId": "593234ff-9386-4e9e-ef8c-ea6cd3fba997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best accuracy is 0.9177641802641803.\n",
      "The winning parameters are {'classify__alpha': 0.01, 'selecter__k': 5000}\n",
      "Run time:  12.622 seconds\n"
     ]
    }
   ],
   "source": [
    "#test tfidf-without lemmarizer\n",
    "import time\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=list(final_stop_words))\n",
    "selecter = SelectKBest(chi2)\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61GAL0wNIPwm",
    "outputId": "ba80ce56-38cb-4d34-ee7e-1f0b4e854d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best accuracy is 0.9177738927738928.\n",
      "The winning parameters are {'classify__alpha': 0.1, 'selecter__k': 5000}\n",
      "Run time:  16.289 seconds\n"
     ]
    }
   ],
   "source": [
    "#test tfidf-without lemmarizer-with normalize\n",
    "import time\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=list(final_stop_words))\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "model = CustomNaiveBayes()\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter), (\"norm\", normalizer) ,(\"classify\", model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(f\"The best accuracy is {grid.best_score_}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "t8qxL_K00I9n"
   },
   "outputs": [],
   "source": [
    "def print_best_params(grid):\n",
    "  bestParameters = grid.best_estimator_.get_params()\n",
    "  # print(bestParameters)\n",
    "  for paramName in sorted(bestParameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (paramName, bestParameters[paramName]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
