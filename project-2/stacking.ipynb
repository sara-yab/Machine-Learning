{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXEl2ep9cMy6",
    "outputId": "2e75b3f9-4bcf-498f-b833-ae8511263b09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh4Rncx_cQho",
    "outputId": "8d34baef-2c30-4bef-fd88-afd22c96853d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el1eg8tdcTVL"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuT3IHQ_cpoG"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZcGyPMzcU1c"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pah6DSKHe7xc"
   },
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "   translator = str.maketrans('', '', string.punctuation)\n",
    "   text = text.translate(translator)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pKmqofAmZUw"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADnjdUOU36o_"
   },
   "outputs": [],
   "source": [
    "def print_best_params(grid):\n",
    "  bestParameters = grid.best_estimator_.get_params()\n",
    "  # print(bestParameters)\n",
    "  for paramName in sorted(bestParameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (paramName, bestParameters[paramName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK-qvvi_Oa26"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MkCaFiKv0ys"
   },
   "outputs": [],
   "source": [
    "#stemmer lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUIZVtDXOHoj"
   },
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOnFbGzniUOH",
    "outputId": "c3e8627d-2945-45c7-87da-fde7804ba372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length custom: 589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words_custom = [\n",
    "# All pronouns and associated words\n",
    "\"i\",\"i'll\",\"i'd\",\"i'm\",\"i've\",\"ive\",\"me\",\"myself\",\"you\",\"you'll\",\"you'd\",\"you're\",\"you've\",\"yourself\",\"he\",\"he'll\",\n",
    "\"he'd\",\n",
    "\"he's\",\n",
    "\"him\",\n",
    "\"she\",\n",
    "\"she'll\",\n",
    "\"she'd\",\n",
    "\"she's\",\n",
    "\"her\",\n",
    "\"it\",\n",
    "\"it'll\",\n",
    "\"it'd\",\n",
    "\"it's\",\n",
    "\"itself\",\n",
    "\"oneself\",\n",
    "\"we\",\n",
    "\"we'll\",\n",
    "\"we'd\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"us\",\n",
    "\"ourselves\",\n",
    "\"they\",\n",
    "\"they'll\",\n",
    "\"they'd\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"them\",\n",
    "\"themselves\",\n",
    "\"everyone\",\n",
    "\"everyone's\",\n",
    "\"everybody\",\n",
    "\"everybody's\",\n",
    "\"someone\",\n",
    "\"someone's\",\n",
    "\"somebody\",\n",
    "\"somebody's\",\n",
    "\"nobody\",\n",
    "\"nobody's\",\n",
    "\"anyone\",\n",
    "\"anyone's\",\n",
    "\"everything\",\n",
    "\"everything's\",\n",
    "\"something\",\n",
    "\"something's\",\n",
    "\"nothing\",\n",
    "\"nothing's\",\n",
    "\"anything\",\n",
    "\"anything's\",\n",
    "# All determiners and associated words\n",
    "\"a\",\n",
    "\"an\",\n",
    "\"the\",\n",
    "\"this\",\n",
    "\"that\",\n",
    "\"that's\",\n",
    "\"these\",\n",
    "\"those\",\n",
    "\"my\",\n",
    "#\"mine\",   #Omitted since mine can refer to something else\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"his\",\n",
    "\"hers\",\n",
    "\"its\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"own\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"few\",\n",
    "\"much\",\n",
    "\"many\",\n",
    "\"lot\",\n",
    "\"lots\",\n",
    "\"some\",\n",
    "\"any\",\n",
    "\"enough\",\n",
    "\"all\",\n",
    "\"both\",\n",
    "\"half\",\n",
    "\"either\",\n",
    "\"neither\",\n",
    "\"each\",\n",
    "\"every\",\n",
    "\"certain\",\n",
    "\"other\",\n",
    "\"another\",\n",
    "\"such\",\n",
    "\"several\",\n",
    "\"multiple\",\n",
    "# \"what\",#Dealt with later on\n",
    "\"rather\",\n",
    "\"quite\",\n",
    "# All prepositions\n",
    "\"aboard\",\n",
    "\"about\",\n",
    "\"above\",\n",
    "\"across\",\n",
    "\"after\",\n",
    "\"against\",\n",
    "\"along\",\n",
    "\"amid\",\n",
    "\"amidst\",\n",
    "\"among\",\n",
    "\"amongst\",\n",
    "\"anti\",\n",
    "\"around\",\n",
    "\"as\",\n",
    "\"at\",\n",
    "\"away\",\n",
    "\"before\",\n",
    "\"behind\",\n",
    "\"below\",\n",
    "\"beneath\",\n",
    "\"beside\",\n",
    "\"besides\",\n",
    "\"between\",\n",
    "\"beyond\",\n",
    "\"but\",\n",
    "\"by\",\n",
    "\"concerning\",\n",
    "\"considering\",\n",
    "\"despite\",\n",
    "\"down\",\n",
    "\"during\",\n",
    "\"except\",\n",
    "\"excepting\",\n",
    "\"excluding\",\n",
    "\"far\",\n",
    "\"following\",\n",
    "\"for\",\n",
    "\"from\",\n",
    "\"here\",\n",
    "\"here's\",\n",
    "\"in\",\n",
    "\"inside\",\n",
    "\"into\",\n",
    "\"left\",\n",
    "\"like\",\n",
    "\"minus\",\n",
    "\"near\",\n",
    "\"of\",\n",
    "\"off\",\n",
    "\"on\",\n",
    "\"onto\",\n",
    "\"opposite\",\n",
    "\"out\",\n",
    "\"outside\",\n",
    "\"over\",\n",
    "\"past\",\n",
    "\"per\",\n",
    "\"plus\",\n",
    "\"regarding\",\n",
    "\"right\",\n",
    "#\"round\",   #Omitted\n",
    "#\"save\",#Omitted\n",
    "\"since\",\n",
    "\"than\",\n",
    "\"there\",\n",
    "\"there's\",\n",
    "\"through\",\n",
    "\"to\",\n",
    "\"toward\",\n",
    "\"towards\",\n",
    "\"under\",\n",
    "\"underneath\",\n",
    "\"unlike\",\n",
    "\"until\",\n",
    "\"up\",\n",
    "\"upon\",\n",
    "\"versus\",\n",
    "\"via\",\n",
    "\"with\",\n",
    "\"within\",\n",
    "\"without\",\n",
    "# Irrelevant verbs\n",
    "\"may\",\n",
    "\"might\",\n",
    "\"will\",\n",
    "\"won't\",\n",
    "\"would\",\n",
    "\"wouldn't\",\n",
    "\"can\",\n",
    "\"can't\",\n",
    "\"cannot\",\n",
    "\"could\",\n",
    "\"couldn't\",\n",
    "\"should\",\n",
    "\"shouldn't\",\n",
    "\"must\",\n",
    "\"must've\",\n",
    "\"be\",\n",
    "\"being\",\n",
    "\"been\",\n",
    "\"am\",\n",
    "\"are\",\n",
    "\"aren't\",\n",
    "\"ain't\",\n",
    "\"is\",\n",
    "\"isn't\",\n",
    "\"was\",\n",
    "\"wasn't\",\n",
    "\"were\",\n",
    "\"weren't\",\n",
    "\"do\",\n",
    "\"doing\",\n",
    "\"don't\",\n",
    "\"does\",\n",
    "\"doesn't\",\n",
    "\"did\",\n",
    "\"didn't\",\n",
    "\"done\",\n",
    "\"have\",\n",
    "\"haven't\",\n",
    "\"having\",\n",
    "\"has\",\n",
    "\"hasn't\",\n",
    "\"had\",\n",
    "\"hadn't\",\n",
    "\"get\",\n",
    "\"getting\",\n",
    "\"gets\",\n",
    "\"got\",\n",
    "\"gotten\",\n",
    "\"go\",\n",
    "\"going\",\n",
    "\"gonna\",\n",
    "\"goes\",\n",
    "\"went\",\n",
    "\"gone\",\n",
    "\"make\",\n",
    "\"making\",\n",
    "\"makes\",\n",
    "\"made\",\n",
    "\"take\",\n",
    "\"taking\",\n",
    "\"takes\",\n",
    "\"took\",\n",
    "\"taken\",\n",
    "\"need\",\n",
    "\"needing\",\n",
    "\"needs\",\n",
    "\"needed\",\n",
    "\"use\",\n",
    "\"using\",\n",
    "\"uses\",\n",
    "\"used\",\n",
    "\"want\",\n",
    "\"wanna\",\n",
    "\"wanting\",\n",
    "\"wants\",\n",
    "\"let\",\n",
    "\"lets\",\n",
    "\"letting\",\n",
    "\"let's\",\n",
    "\"suppose\",\n",
    "\"supposing\",\n",
    "\"supposes\",\n",
    "\"supposed\",\n",
    "\"seem\",\n",
    "\"seeming\",\n",
    "\"seems\",\n",
    "\"seemed\",\n",
    "\"say\",\n",
    "\"saying\",\n",
    "\"says\",\n",
    "\"said\",\n",
    "\"know\",\n",
    "\"knowing\",\n",
    "\"knows\",\n",
    "\"knew\",\n",
    "\"known\",\n",
    "\"look\",\n",
    "\"looking\",\n",
    "\"looked\",\n",
    "\"think\",\n",
    "\"thinking\",\n",
    "\"thinks\",\n",
    "\"thought\",\n",
    "\"feel\",\n",
    "\"feels\",\n",
    "\"felt\",\n",
    "\"based\",\n",
    "\"put\",\n",
    "\"puts\",\n",
    "#\"wanted\"   #Omitted since the advective is relevant\n",
    "# Question words and associated words\n",
    "\"who\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"who'd\",\n",
    "\"whoever\",\n",
    "\"whoever's\",\n",
    "\"whom\",\n",
    "\"whomever\",\n",
    "\"whomever's\",\n",
    "\"whose\",\n",
    "\"whosever\",\n",
    "\"whosever's\",\n",
    "\"when\",\n",
    "\"whenever\",\n",
    "\"which\",\n",
    "\"whichever\",\n",
    "\"where\",\n",
    "\"where's\",\n",
    "\"where'd\",\n",
    "\"wherever\",\n",
    "\"why\",\n",
    "\"why's\",\n",
    "\"why'd\",\n",
    "\"whyever\",\n",
    "\"what\",\n",
    "\"what's\",\n",
    "\"whatever\",\n",
    "\"whence\",\n",
    "\"how\",\n",
    "\"how's\",\n",
    "\"how'd\",\n",
    "\"however\",\n",
    "\"whether\",\n",
    "\"whatsoever\",\n",
    "# Connector words and irrelevant adverbs\n",
    "\"and\",\n",
    "\"or\",\n",
    "\"not\",\n",
    "\"because\",\n",
    "\"also\",\n",
    "\"always\",\n",
    "\"never\",\n",
    "\"only\",\n",
    "\"really\",\n",
    "\"very\",\n",
    "\"greatly\",\n",
    "\"extremely\",\n",
    "\"somewhat\",\n",
    "\"no\",\n",
    "\"nope\",\n",
    "\"nah\",\n",
    "\"yes\",\n",
    "\"yep\",\n",
    "\"yeh\",\n",
    "\"yeah\",\n",
    "\"maybe\",\n",
    "\"perhaps\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"less\",\n",
    "\"least\",\n",
    "\"good\",\n",
    "\"great\",\n",
    "\"well\",\n",
    "\"better\",\n",
    "\"best\",\n",
    "\"bad\",\n",
    "\"worse\",\n",
    "\"worst\",\n",
    "\"too\",\n",
    "\"thru\",\n",
    "\"though\",\n",
    "\"although\",\n",
    "\"yet\",\n",
    "\"already\",\n",
    "\"then\",\n",
    "\"even\",\n",
    "\"now\",\n",
    "\"sometimes\",\n",
    "\"still\",\n",
    "\"together\",\n",
    "\"altogether\",\n",
    "\"entirely\",\n",
    "\"fully\",\n",
    "\"entire\",\n",
    "\"whole\",\n",
    "\"completely\",\n",
    "\"utterly\",\n",
    "\"seemingly\",\n",
    "\"apparently\",\n",
    "\"clearly\",\n",
    "\"obviously\",\n",
    "\"actually\",\n",
    "\"actual\",\n",
    "\"usually\",\n",
    "\"usual\",\n",
    "\"literally\",\n",
    "\"honestly\",\n",
    "\"absolutely\",\n",
    "\"definitely\",\n",
    "\"generally\",\n",
    "\"totally\",\n",
    "\"finally\",\n",
    "\"basically\",\n",
    "\"essentially\",\n",
    "\"fundamentally\",\n",
    "\"automatically\",\n",
    "\"immediately\",\n",
    "\"necessarily\",\n",
    "\"primarily\",\n",
    "\"normally\",\n",
    "\"perfectly\",\n",
    "\"constantly\",\n",
    "\"particularly\",\n",
    "\"eventually\",\n",
    "\"hopefully\",\n",
    "\"mainly\",\n",
    "\"typically\",\n",
    "\"specifically\",\n",
    "\"differently\",\n",
    "\"appropriately\",\n",
    "\"plenty\",\n",
    "\"certainly\",\n",
    "\"unfortunately\",\n",
    "\"ultimately\",\n",
    "\"unlikely\",\n",
    "\"likely\",\n",
    "\"potentially\",\n",
    "\"fortunately\",\n",
    "\"personally\",\n",
    "\"directly\",\n",
    "\"indirectly\",\n",
    "\"nearly\",\n",
    "\"closely\",\n",
    "\"slightly\",\n",
    "\"probably\",\n",
    "\"possibly\",\n",
    "\"especially\",\n",
    "\"frequently\",\n",
    "\"often\",\n",
    "\"oftentimes\",\n",
    "\"seldom\",\n",
    "\"rarely\",\n",
    "\"sure\",\n",
    "\"while\",\n",
    "\"whilst\",\n",
    "\"able\",\n",
    "\"unable\",\n",
    "\"else\",\n",
    "\"ever\",\n",
    "\"once\",\n",
    "\"twice\",\n",
    "\"thrice\",\n",
    "\"almost\",\n",
    "\"again\",\n",
    "\"instead\",\n",
    "\"next\",\n",
    "\"previous\",\n",
    "\"unless\",\n",
    "\"somehow\",\n",
    "\"anyhow\",\n",
    "\"anywhere\",\n",
    "\"somewhere\",\n",
    "\"everywhere\",\n",
    "\"nowhere\",\n",
    "\"further\",\n",
    "\"anymore\",\n",
    "\"later\",\n",
    "\"ago\",\n",
    "\"ahead\",\n",
    "\"just\",\n",
    "\"same\",\n",
    "\"different\",\n",
    "\"big\",\n",
    "\"small\",\n",
    "\"little\",\n",
    "\"tiny\",\n",
    "\"large\",\n",
    "\"huge\",\n",
    "\"pretty\",\n",
    "\"mostly\",\n",
    "\"anyway\",\n",
    "\"anyways\",\n",
    "\"otherwise\",\n",
    "\"regardless\",\n",
    "\"throughout\",\n",
    "\"additionally\",\n",
    "\"moreover\",\n",
    "\"furthermore\",\n",
    "\"meanwhile\",\n",
    "\"afterwards\",\n",
    "# Irrelevant nouns\n",
    "\"thing\",\n",
    "\"thing's\",\n",
    "\"things\",\n",
    "\"stuff\",\n",
    "\"other's\",\n",
    "\"others\",\n",
    "\"another's\",\n",
    "\"total\",\n",
    "\"\",\n",
    "\"false\",\n",
    "\"none\",\n",
    "\"way\",\n",
    "\"kind\",\n",
    "# Lettered numbers and order\n",
    "\"zero\",\n",
    "\"zeros\",\n",
    "\"zeroes\",\n",
    "\"one\",\n",
    "\"ones\",\n",
    "\"two\",\n",
    "\"three\",\n",
    "\"four\",\n",
    "\"five\",\n",
    "\"six\", \n",
    "\"seven\",\n",
    "\"eight\",\n",
    "\"nine\",\n",
    "\"ten\",\n",
    "\"twenty\",\n",
    "\"thirty\",\n",
    "\"forty\",\n",
    "\"fifty\",\n",
    "\"sixty\",\n",
    "\"seventy\",\n",
    "\"eighty\",\n",
    "\"ninety\",\n",
    "\"hundred\",\n",
    "\"hundreds\",\n",
    "\"thousand\",\n",
    "\"thousands\",\n",
    "\"million\",\n",
    "\"millions\",\n",
    "\"first\",\n",
    "\"last\",\n",
    "\"second\",\n",
    "\"third\",\n",
    "\"fourth\",\n",
    "\"fifth\",\n",
    "\"sixth\",\n",
    "\"seventh\",\n",
    "\"eigth\",\n",
    "\"ninth\",\n",
    "\"tenth\",\n",
    "\"firstly\",\n",
    "\"secondly\",\n",
    "\"thirdly\",\n",
    "\"lastly\",\n",
    "# Greetings and slang\n",
    "\"hello\",\n",
    "\"hi\",\n",
    "\"hey\",\n",
    "\"sup\",\n",
    "\"yo\",\n",
    "\"greetings\",\n",
    "\"please\",\n",
    "\"okay\",\n",
    "\"ok\",\n",
    "\"y'all\",\n",
    "\"lol\",\n",
    "\"rofl\",\n",
    "\"thank\",\n",
    "\"thanks\",\n",
    "\"alright\",\n",
    "\"kinda\",\n",
    "\"dont\",\n",
    "\"sorry\",\n",
    "\"idk\",\n",
    "\"tldr\",\n",
    "\"tl\",\n",
    "\"dr\",  #This means that dr (doctor) is a bad feature because of tl;dr\n",
    "\"tbh\",\n",
    "\"dude\",\n",
    "\"tho\",\n",
    "\"aka\",\n",
    "\"plz\",\n",
    "\"pls\",\n",
    "\"bit\",\n",
    "\"don\",\n",
    "# Miscellaneous\n",
    "\"www\",\n",
    "\"https\",\n",
    "\"http\",\n",
    "\"com\",\n",
    "\"etc\"\n",
    "\"html\",\n",
    "\"reddit\",\n",
    "\"subreddit\",\n",
    "\"subreddits\",\n",
    "\"comments\",\n",
    "\"reply\",\n",
    "\"replies\",\n",
    "\"thread\",\n",
    "\"threads\",\n",
    "\"post\",\n",
    "\"posts\",\n",
    "\"website\",\n",
    "\"websites\",\n",
    "\"web site\",\n",
    "\"web sites\"]\n",
    "print('length custom:',len(stop_words_custom))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDJa9UxAHlY8",
    "outputId": "05280817-955f-475e-bcc1-6e68fe59643b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 94.568.\n",
      "The winning parameters are {'tfidf__stop_words': ['even', 'put', 'eleven', 'won', 'didn', 'beforehand', 'toward', 'couldnt', 'mostly', 'eight', 'either', 'enough', 'your', 'while', 'been', 'anyway', 'sincere', 'hasnt', 'others', 'another', 'none', 'itself', 'as', 'formerly', 'often', 'about', 'off', 'just', 'during', 't', 'cannot', 'rather', \"aren't\", 'too', 'ever', 'wasn', 'less', 'yourself', 'myself', 'do', 'hereafter', \"that'll\", 'became', 'will', 'back', \"haven't\", 'seemed', 'name', 'one', 'never', 'so', 'onto', \"wasn't\", 'find', 'until', 'if', \"won't\", 'here', 'elsewhere', 'no', 'those', 'needn', 'hence', 'meanwhile', 'from', 'hereupon', 'for', 'almost', 'did', 'least', 'with', 'she', 'many', 'without', 'noone', 'thereupon', 'not', 'my', 'throughout', 'thick', 'such', 'hadn', 'us', 'all', 'now', 'twenty', 'once', 'at', 'fifty', 'anywhere', 'whereas', 'former', 'else', 'always', 'sometimes', 'please', \"mightn't\", 'mightn', 'd', 'same', 'other', 'few', 'nobody', 'describe', 'sometime', 'somewhere', 'etc', 'seem', 'seems', \"needn't\", 'mill', 'which', 'thereafter', 'sixty', 'together', 'therein', 'two', \"shan't\", 'between', 'he', 'thin', 'already', 'his', 'their', 'hereby', 'doing', 'indeed', 'first', 'latterly', 'still', 'or', 'm', 'nor', 'can', 'neither', \"hasn't\", 'll', 'next', 'when', 'thru', 'over', 'hers', 'mustn', 'besides', 'could', 'side', 'ten', 'yourselves', 'move', 'nevertheless', 'ours', 'this', 'perhaps', 'fifteen', \"it's\", 'well', 'con', 'up', 'un', 'be', 'mine', 'around', 'has', 'whatever', 'wouldn', 'them', 'five', 'last', 'each', \"you're\", 'nowhere', 'shouldn', 'wherever', 'ie', 'anyone', 'again', 'were', 'via', 'theirs', 'being', 'anyhow', 'it', 'more', 'under', 'have', 'since', 'through', 'having', \"you'll\", 'four', 'whereby', 'anything', 'front', 'afterwards', 'a', 'does', 's', 'six', 'somehow', 'should', 'shan', 'would', 'its', 'isn', 'any', 'where', 'keep', 'per', 'also', 'among', 'only', 'except', 'must', 'though', 'take', 'amoungst', 'behind', \"isn't\", 'of', 'done', 'show', 'own', 'by', \"shouldn't\", \"weren't\", 'give', 'after', 'twelve', \"don't\", 'thence', \"wouldn't\", \"you've\", 'then', 'these', 'to', 'everything', 'namely', \"you'd\", 'beside', 'i', 'ltd', 'don', 'me', 'due', \"hadn't\", 'hasn', 'made', 'whoever', 'above', 'forty', 'themselves', 'both', 'hundred', 're', 'our', 'amongst', 'however', 'moreover', 'out', 'fill', \"couldn't\", 'down', 'whom', 'become', 'haven', 'weren', 'thus', 'ma', 'below', 'becomes', 'everywhere', 'interest', 'much', 'herein', 'yours', 'seeming', 'is', 'nine', 'full', 'ourselves', 'ain', 'latter', 'across', 'am', 'call', 'whereupon', 'something', \"doesn't\", 'found', 'why', 'most', 'therefore', 'co', 'thereby', 'someone', 'empty', 'on', 'who', 'towards', 'whereafter', 'go', 'there', 'cry', 'they', 'because', 'beyond', 'bottom', 'that', 'de', 'further', 'y', 'very', 'whole', 'get', 'alone', 'than', 'detail', 'and', 'part', 'whenever', 'top', 'every', 'him', 'but', 'amount', 'everyone', 'herself', 'aren', 'along', 'three', 'fire', 'against', 'we', \"she's\", 'becoming', 've', 'are', 'bill', 'before', \"mustn't\", 'within', 'wherein', 'doesn', 'was', 'nothing', 'himself', 'the', 'whence', 'whither', 'otherwise', 'serious', 'eg', 'in', 'inc', 'into', 'o', 'some', 'upon', 'whether', 'yet', 'cant', 'several', 'how', 'had', 'may', 'whose', \"should've\", 'system', \"didn't\", 'an', 'third', 'her', 'see', 'couldn', 'although', 'you', 'might', 'what']}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "   # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "  \"tfidf__stop_words\": [list(stop_words_library)],\n",
    "   # 'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "   # 'stacking__final_estimator__penalty': ['l1', 'l2'],\n",
    "   # 'stacking__final_estimator__C': [0.1, 1.0, 10.0],\n",
    "   # 'stacking__final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "#accuracy = round(grid.best_score_ * 100,3)\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "#print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOWEaOW6ikhp",
    "outputId": "3cffc0f9-b1af-48b8-c2a2-102f3c42585d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 94.846.\n",
      "The winning parameters are {'tfidf__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "#=>94.846\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "   # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "  \"tfidf__stop_words\": [list(stop_words_library),list(stop_words_custom)],\n",
    "   # 'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "   # 'stacking__final_estimator__penalty': ['l1', 'l2'],\n",
    "   # 'stacking__final_estimator__C': [0.1, 1.0, 10.0],\n",
    "   # 'stacking__final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "#accuracy = round(grid.best_score_ * 100,3)\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "#print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ky_jkRtjBmR",
    "outputId": "7ba4aa74-da2e-404b-9bcd-3e70ae541ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'stacking__mnb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "   # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "  \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    'stacking__mnb__alpha': [0.0001, 0.001, 0.01,0.5],\n",
    "   # 'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "   #'stacking__final_estimator__penalty': ['l1', 'l2'],\n",
    "   # 'stacking__final_estimator__C': [0.1, 1.0, 10.0],\n",
    "   # 'stacking__final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "#accuracy = round(grid.best_score_ * 100,3)\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "#print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "#print_best_params(grid_search)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jje8Ez-slFAd",
    "outputId": "a85c551e-2299-4a14-8167-c81afc3c1769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'stacking__mnb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #(\"selecter\", selecter),\n",
    "    (\"normalizer\",normalizer),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "   # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'stacking__mnb__alpha': [0.5],\n",
    "       # \"selecter__k\":[5000],\n",
    "          \"cv__stop_words\": [list(stop_words_custom)],\n",
    "            \"normalizer__norm\": ['l2','l1']\n",
    "   # 'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "   #'stacking__final_estimator__penalty': ['l1', 'l2'],\n",
    "   # 'stacking__final_estimator__C': [0.1, 1.0, 10.0],\n",
    "   # 'stacking__final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "#accuracy = round(grid.best_score_ * 100,3)\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "#print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "#print_best_params(grid_search)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1Oa1MUbl9cs",
    "outputId": "004cd904-cbf7-4e45-af74-f999abfd2980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'stacking__lr__solver': 'sag', 'stacking__mnb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#    'stacking__lr__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #(\"selecter\", selecter),\n",
    "    (\"normalizer\",normalizer),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "   # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'stacking__mnb__alpha': [0.5],\n",
    "       # \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    \"normalizer__norm\": ['l2','l1'],\n",
    "   # 'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'stacking__lr__solver': ['sag', 'saga'],\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "#accuracy = round(grid.best_score_ * 100,3)\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "#print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "#print_best_params(grid_search)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XM1ug5N-qzhk",
    "outputId": "e910beaa-0e4d-4d1b-ea46-85b42484c330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'stacking__lr__solver': 'sag', 'stacking__mnb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#    'stacking__lr__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #(\"selecter\", selecter),\n",
    "    (\"normalizer\",normalizer),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "   # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'stacking__mnb__alpha': [0.5],\n",
    "       # \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    \"normalizer__norm\": ['l2','l1'],\n",
    "   # 'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'stacking__lr__solver': ['sag', 'lbfgs'],\n",
    "}\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "#accuracy = round(grid.best_score_ * 100,3)\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "#print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "#print_best_params(grid_search)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVxG6p77vLiU",
    "outputId": "8dde5115-295c-4a6d-d88b-2e50f644ceb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__max_df': 0.75, 'cv__ngram_range': (1, 1), 'cv__preprocessor': <function preprocess_text at 0x7f6cc558ea60>, 'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'selecter__k': 5000, 'stacking__lr__solver': 'sag', 'stacking__mnb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#base condition with stacking\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#    'stacking__lr__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    (\"selecter\", selecter),\n",
    "    (\"normalizer\",normalizer),\n",
    "    ('stacking', StackingClassifier(estimators=estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "params = {\n",
    "    'cv__max_df': [0.5, 0.75],\n",
    "    'stacking__mnb__alpha': [0.5],\n",
    "    \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    'cv__preprocessor': [preprocess_text],\n",
    "    \"normalizer__norm\": ['l2'],\n",
    "   'cv__ngram_range': [(1,1)],\n",
    "    'stacking__lr__solver': ['sag'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(stacking_pipeline, params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(grid_search.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid_search.best_params_}\")\n",
    "\n",
    "#print_best_params(grid_search)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChhMjtlXwFGE",
    "outputId": "04488276-e988-4976-96fb-88edbcf7f668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__max_df': 0.75, 'cv__ngram_range': (1, 1), 'cv__preprocessor': <function preprocess_text at 0x7f6cc558ea60>, 'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'selecter__k': 5000, 'stacking__lr__solver': 'sag', 'stacking__mnb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#final\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "final_estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "]\n",
    "\n",
    "final_selecter = SelectKBest(chi2)\n",
    "final_normalizer = Normalizer()\n",
    "\n",
    "\n",
    "# Define the stacking classifier pipeline\n",
    "final_stacking_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    (\"selecter\", final_selecter),\n",
    "    (\"normalizer\",final_normalizer),\n",
    "    ('stacking', StackingClassifier(estimators=final_estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "final_params = {\n",
    "    'cv__max_df': [0.5, 0.75],\n",
    "    'stacking__mnb__alpha': [0.5],\n",
    "    \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    'cv__preprocessor': [preprocess_text],\n",
    "    \"normalizer__norm\": ['l2'],\n",
    "   'cv__ngram_range': [(1,1)],\n",
    "    'stacking__lr__solver': ['sag'],\n",
    "}\n",
    "\n",
    "final_grid = GridSearchCV(final_stacking_pipeline, final_params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "final_grid.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(final_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {final_grid.best_params_}\")\n",
    "\n",
    "#print_best_params(grid_search)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDh1sb3-wj11",
    "outputId": "3688b08e-5702-4249-daf9-baf3be8b80e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "y_pred_new = final_grid.predict(test_x)\n",
    "create_test_csv(y_pred_new,\"Stacking_MultiNB-Logistic-05032023_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMLed6w2qk9Z"
   },
   "outputs": [],
   "source": [
    "############################################################## CNN & Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lINKxRhkhEf"
   },
   "outputs": [],
   "source": [
    "def print_best_params(grid):\n",
    "  bestParameters = grid.best_estimator_.get_params()\n",
    "  # print(bestParameters)\n",
    "  for paramName in sorted(bestParameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (paramName, bestParameters[paramName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIYwDuLEz-To",
    "outputId": "2e9a7387-b29e-45ac-dc7e-f381fb2b32de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 93.729.\n",
      "The winning parameters are {'stacking__mlp__alpha': 0.1}\n",
      "\tcv: TfidfVectorizer()\n",
      "\tcv__analyzer: 'word'\n",
      "\tcv__binary: False\n",
      "\tcv__decode_error: 'strict'\n",
      "\tcv__dtype: <class 'numpy.float64'>\n",
      "\tcv__encoding: 'utf-8'\n",
      "\tcv__input: 'content'\n",
      "\tcv__lowercase: True\n",
      "\tcv__max_df: 1.0\n",
      "\tcv__max_features: None\n",
      "\tcv__min_df: 1\n",
      "\tcv__ngram_range: (1, 1)\n",
      "\tcv__norm: 'l2'\n",
      "\tcv__preprocessor: None\n",
      "\tcv__smooth_idf: True\n",
      "\tcv__stop_words: None\n",
      "\tcv__strip_accents: None\n",
      "\tcv__sublinear_tf: False\n",
      "\tcv__token_pattern: '(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tcv__tokenizer: None\n",
      "\tcv__use_idf: True\n",
      "\tcv__vocabulary: None\n",
      "\tmemory: None\n",
      "\tstacking: StackingClassifier(estimators=[('nb', MultinomialNB()),\n",
      "                               ('mlp', MLPClassifier(alpha=0.1))])\n",
      "\tstacking__cv: None\n",
      "\tstacking__estimators: [('nb', MultinomialNB()), ('mlp', MLPClassifier(alpha=0.1))]\n",
      "\tstacking__final_estimator: None\n",
      "\tstacking__mlp: MLPClassifier(alpha=0.1)\n",
      "\tstacking__mlp__activation: 'relu'\n",
      "\tstacking__mlp__alpha: 0.1\n",
      "\tstacking__mlp__batch_size: 'auto'\n",
      "\tstacking__mlp__beta_1: 0.9\n",
      "\tstacking__mlp__beta_2: 0.999\n",
      "\tstacking__mlp__early_stopping: False\n",
      "\tstacking__mlp__epsilon: 1e-08\n",
      "\tstacking__mlp__hidden_layer_sizes: (100,)\n",
      "\tstacking__mlp__learning_rate: 'constant'\n",
      "\tstacking__mlp__learning_rate_init: 0.001\n",
      "\tstacking__mlp__max_fun: 15000\n",
      "\tstacking__mlp__max_iter: 200\n",
      "\tstacking__mlp__momentum: 0.9\n",
      "\tstacking__mlp__n_iter_no_change: 10\n",
      "\tstacking__mlp__nesterovs_momentum: True\n",
      "\tstacking__mlp__power_t: 0.5\n",
      "\tstacking__mlp__random_state: None\n",
      "\tstacking__mlp__shuffle: True\n",
      "\tstacking__mlp__solver: 'adam'\n",
      "\tstacking__mlp__tol: 0.0001\n",
      "\tstacking__mlp__validation_fraction: 0.1\n",
      "\tstacking__mlp__verbose: False\n",
      "\tstacking__mlp__warm_start: False\n",
      "\tstacking__n_jobs: None\n",
      "\tstacking__nb: MultinomialNB()\n",
      "\tstacking__nb__alpha: 1.0\n",
      "\tstacking__nb__class_prior: None\n",
      "\tstacking__nb__fit_prior: True\n",
      "\tstacking__nb__force_alpha: 'warn'\n",
      "\tstacking__passthrough: False\n",
      "\tstacking__stack_method: 'auto'\n",
      "\tstacking__verbose: 0\n",
      "\tsteps: [('cv', TfidfVectorizer()), ('stacking', StackingClassifier(estimators=[('nb', MultinomialNB()),\n",
      "                               ('mlp', MLPClassifier(alpha=0.1))]))]\n",
      "\tverbose: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#new ensemble\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "ensemble_estimators = [('nb', MultinomialNB()), ('mlp', MLPClassifier())]\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('stacking', StackingClassifier(estimators=ensemble_estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "ensemble_params = {\n",
    "    #'cv__max_df': [0.5, 0.75, 1.0],\n",
    "   # 'nb__alpha': [0.1, 0.5, 1.0],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "   # 'mlp__hidden_layer_sizes': [(50,), (100,), (200,)],\n",
    "    'stacking__mlp__alpha': [0.1],\n",
    "}\n",
    "\n",
    "ensemble_grid = GridSearchCV(ensemble_pipeline, ensemble_params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "ensemble_grid.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(ensemble_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {ensemble_grid.best_params_}\")\n",
    "\n",
    "print_best_params(ensemble_grid)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJUf1duH6XFg",
    "outputId": "44d78b04-9bfe-42ce-d528-841ce6544484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 94.984.\n",
      "The winning parameters are {'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'stacking__mlp__alpha': 0.1, 'stacking__mlp__hidden_layer_sizes': (32,), 'stacking__mlp__solver': 'lbfgs'}\n",
      "\tcv: TfidfVectorizer(stop_words=['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me',\n",
      "                            'myself', 'you', \"you'll\", \"you'd\", \"you're\",\n",
      "                            \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\",\n",
      "                            'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her',\n",
      "                            'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', ...])\n",
      "\tcv__analyzer: 'word'\n",
      "\tcv__binary: False\n",
      "\tcv__decode_error: 'strict'\n",
      "\tcv__dtype: <class 'numpy.float64'>\n",
      "\tcv__encoding: 'utf-8'\n",
      "\tcv__input: 'content'\n",
      "\tcv__lowercase: True\n",
      "\tcv__max_df: 1.0\n",
      "\tcv__max_features: None\n",
      "\tcv__min_df: 1\n",
      "\tcv__ngram_range: (1, 1)\n",
      "\tcv__norm: 'l2'\n",
      "\tcv__preprocessor: None\n",
      "\tcv__smooth_idf: True\n",
      "\tcv__stop_words: ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites']\n",
      "\tcv__strip_accents: None\n",
      "\tcv__sublinear_tf: False\n",
      "\tcv__token_pattern: '(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tcv__tokenizer: None\n",
      "\tcv__use_idf: True\n",
      "\tcv__vocabulary: None\n",
      "\tmemory: None\n",
      "\tstacking: StackingClassifier(estimators=[('nb', MultinomialNB()),\n",
      "                               ('mlp',\n",
      "                                MLPClassifier(alpha=0.1,\n",
      "                                              hidden_layer_sizes=(32,),\n",
      "                                              solver='lbfgs'))])\n",
      "\tstacking__cv: None\n",
      "\tstacking__estimators: [('nb', MultinomialNB()), ('mlp', MLPClassifier(alpha=0.1, hidden_layer_sizes=(32,), solver='lbfgs'))]\n",
      "\tstacking__final_estimator: None\n",
      "\tstacking__mlp: MLPClassifier(alpha=0.1, hidden_layer_sizes=(32,), solver='lbfgs')\n",
      "\tstacking__mlp__activation: 'relu'\n",
      "\tstacking__mlp__alpha: 0.1\n",
      "\tstacking__mlp__batch_size: 'auto'\n",
      "\tstacking__mlp__beta_1: 0.9\n",
      "\tstacking__mlp__beta_2: 0.999\n",
      "\tstacking__mlp__early_stopping: False\n",
      "\tstacking__mlp__epsilon: 1e-08\n",
      "\tstacking__mlp__hidden_layer_sizes: (32,)\n",
      "\tstacking__mlp__learning_rate: 'constant'\n",
      "\tstacking__mlp__learning_rate_init: 0.001\n",
      "\tstacking__mlp__max_fun: 15000\n",
      "\tstacking__mlp__max_iter: 200\n",
      "\tstacking__mlp__momentum: 0.9\n",
      "\tstacking__mlp__n_iter_no_change: 10\n",
      "\tstacking__mlp__nesterovs_momentum: True\n",
      "\tstacking__mlp__power_t: 0.5\n",
      "\tstacking__mlp__random_state: None\n",
      "\tstacking__mlp__shuffle: True\n",
      "\tstacking__mlp__solver: 'lbfgs'\n",
      "\tstacking__mlp__tol: 0.0001\n",
      "\tstacking__mlp__validation_fraction: 0.1\n",
      "\tstacking__mlp__verbose: False\n",
      "\tstacking__mlp__warm_start: False\n",
      "\tstacking__n_jobs: None\n",
      "\tstacking__nb: MultinomialNB()\n",
      "\tstacking__nb__alpha: 1.0\n",
      "\tstacking__nb__class_prior: None\n",
      "\tstacking__nb__fit_prior: True\n",
      "\tstacking__nb__force_alpha: 'warn'\n",
      "\tstacking__passthrough: False\n",
      "\tstacking__stack_method: 'auto'\n",
      "\tstacking__verbose: 0\n",
      "\tsteps: [('cv', TfidfVectorizer(stop_words=['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me',\n",
      "                            'myself', 'you', \"you'll\", \"you'd\", \"you're\",\n",
      "                            \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\",\n",
      "                            'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her',\n",
      "                            'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', ...])), ('stacking', StackingClassifier(estimators=[('nb', MultinomialNB()),\n",
      "                               ('mlp',\n",
      "                                MLPClassifier(alpha=0.1,\n",
      "                                              hidden_layer_sizes=(32,),\n",
      "                                              solver='lbfgs'))]))]\n",
      "\tverbose: False\n"
     ]
    }
   ],
   "source": [
    "#new ensemble\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "ensemble_estimators = [('nb', MultinomialNB()), ('mlp', MLPClassifier())]\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('stacking', StackingClassifier(estimators=ensemble_estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "ensemble_params = {\n",
    "    #'cv__max_df': [0.5, 0.75, 1.0],\n",
    "   # 'nb__alpha': [0.1, 0.5, 1.0],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    'stacking__mlp__solver':[\"lbfgs\"],\n",
    "    'stacking__mlp__hidden_layer_sizes': [(32,)],\n",
    "   # 'mlp__hidden_layer_sizes': [(50,), (100,), (200,)],\n",
    "    'stacking__mlp__alpha': [0.1],\n",
    "}\n",
    "\n",
    "ensemble_grid = GridSearchCV(ensemble_pipeline, ensemble_params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "ensemble_grid.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(ensemble_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {ensemble_grid.best_params_}\")\n",
    "\n",
    "print_best_params(ensemble_grid)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZVOOwm6-T6a",
    "outputId": "dd568f94-09af-4cb7-bc96-3d740b728f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__max_df': 0.75, 'cv__ngram_range': (1, 1), 'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'selecter__k': 5000, 'stacking__mlp__alpha': 0.1, 'stacking__mlp__hidden_layer_sizes': (32,), 'stacking__mlp__solver': 'lbfgs', 'stacking__nb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#new ensemble\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "ensemble_estimators = [('nb', MultinomialNB()), ('mlp', MLPClassifier())]\n",
    "\n",
    "ensemble_selector = SelectKBest(chi2)\n",
    "ensemble_normalizer = Normalizer()\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    (\"normalizer\",ensemble_normalizer),\n",
    "    (\"selecter\", ensemble_selector),\n",
    "    ('stacking', StackingClassifier(estimators=ensemble_estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "ensemble_params = {\n",
    "    'cv__max_df': [0.75],\n",
    "    \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    \"normalizer__norm\": ['l2'],\n",
    "    'cv__ngram_range': [(1,1)],\n",
    "    'stacking__mlp__solver':[\"lbfgs\"],\n",
    "    'stacking__mlp__hidden_layer_sizes': [(32,)],\n",
    "    'stacking__mlp__alpha': [0.1],    \n",
    "    'stacking__nb__alpha': [0.5],\n",
    "}\n",
    "\n",
    "ensemble_grid = GridSearchCV(ensemble_pipeline, ensemble_params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "ensemble_grid.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(ensemble_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {ensemble_grid.best_params_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBJ9Oc7qHsl_",
    "outputId": "1a7cc25a-e5ca-4b17-fb67-ead898a0f156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "The best accuracy is 95.123.\n",
      "The winning parameters are {'cv__max_df': 0.75, 'cv__ngram_range': (1, 1), 'cv__stop_words': ['even', 'put', 'eleven', 'won', 'didn', 'beforehand', 'toward', 'couldnt', 'mostly', 'eight', 'either', 'enough', 'your', 'while', 'been', 'anyway', 'sincere', 'hasnt', 'others', 'another', 'none', 'itself', 'as', 'formerly', 'often', 'about', 'off', 'just', 'during', 't', 'cannot', 'rather', \"aren't\", 'too', 'ever', 'wasn', 'less', 'yourself', 'myself', 'do', 'hereafter', \"that'll\", 'became', 'will', 'back', \"haven't\", 'seemed', 'name', 'one', 'never', 'so', 'onto', \"wasn't\", 'find', 'until', 'if', \"won't\", 'here', 'elsewhere', 'no', 'those', 'needn', 'hence', 'meanwhile', 'from', 'hereupon', 'for', 'almost', 'did', 'least', 'with', 'she', 'many', 'without', 'noone', 'thereupon', 'not', 'my', 'throughout', 'thick', 'such', 'hadn', 'us', 'all', 'now', 'twenty', 'once', 'at', 'fifty', 'anywhere', 'whereas', 'former', 'else', 'always', 'sometimes', 'please', \"mightn't\", 'mightn', 'd', 'same', 'other', 'few', 'nobody', 'describe', 'sometime', 'somewhere', 'etc', 'seem', 'seems', \"needn't\", 'mill', 'which', 'thereafter', 'sixty', 'together', 'therein', 'two', \"shan't\", 'between', 'he', 'thin', 'already', 'his', 'their', 'hereby', 'doing', 'indeed', 'first', 'latterly', 'still', 'or', 'm', 'nor', 'can', 'neither', \"hasn't\", 'll', 'next', 'when', 'thru', 'over', 'hers', 'mustn', 'besides', 'could', 'side', 'ten', 'yourselves', 'move', 'nevertheless', 'ours', 'this', 'perhaps', 'fifteen', \"it's\", 'well', 'con', 'up', 'un', 'be', 'mine', 'around', 'has', 'whatever', 'wouldn', 'them', 'five', 'last', 'each', \"you're\", 'nowhere', 'shouldn', 'wherever', 'ie', 'anyone', 'again', 'were', 'via', 'theirs', 'being', 'anyhow', 'it', 'more', 'under', 'have', 'since', 'through', 'having', \"you'll\", 'four', 'whereby', 'anything', 'front', 'afterwards', 'a', 'does', 's', 'six', 'somehow', 'should', 'shan', 'would', 'its', 'isn', 'any', 'where', 'keep', 'per', 'also', 'among', 'only', 'except', 'must', 'though', 'take', 'amoungst', 'behind', \"isn't\", 'of', 'done', 'show', 'own', 'by', \"shouldn't\", \"weren't\", 'give', 'after', 'twelve', \"don't\", 'thence', \"wouldn't\", \"you've\", 'then', 'these', 'to', 'everything', 'namely', \"you'd\", 'beside', 'i', 'ltd', 'don', 'me', 'due', \"hadn't\", 'hasn', 'made', 'whoever', 'above', 'forty', 'themselves', 'both', 'hundred', 're', 'our', 'amongst', 'however', 'moreover', 'out', 'fill', \"couldn't\", 'down', 'whom', 'become', 'haven', 'weren', 'thus', 'ma', 'below', 'becomes', 'everywhere', 'interest', 'much', 'herein', 'yours', 'seeming', 'is', 'nine', 'full', 'ourselves', 'ain', 'latter', 'across', 'am', 'call', 'whereupon', 'something', \"doesn't\", 'found', 'why', 'most', 'therefore', 'co', 'thereby', 'someone', 'empty', 'on', 'who', 'towards', 'whereafter', 'go', 'there', 'cry', 'they', 'because', 'beyond', 'bottom', 'that', 'de', 'further', 'y', 'very', 'whole', 'get', 'alone', 'than', 'detail', 'and', 'part', 'whenever', 'top', 'every', 'him', 'but', 'amount', 'everyone', 'herself', 'aren', 'along', 'three', 'fire', 'against', 'we', \"she's\", 'becoming', 've', 'are', 'bill', 'before', \"mustn't\", 'within', 'wherein', 'doesn', 'was', 'nothing', 'himself', 'the', 'whence', 'whither', 'otherwise', 'serious', 'eg', 'in', 'inc', 'into', 'o', 'some', 'upon', 'whether', 'yet', 'cant', 'several', 'how', 'had', 'may', 'whose', \"should've\", 'system', \"didn't\", 'an', 'third', 'her', 'see', 'couldn', 'although', 'you', 'might', 'what'], 'normalizer__norm': 'l2', 'selecter__k': 5000, 'stacking__mlp__alpha': 0.1, 'stacking__mlp__hidden_layer_sizes': (32,), 'stacking__mlp__solver': 'lbfgs', 'stacking__nb__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#new ensemble\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "ensemble_estimators = [('nb', MultinomialNB()), ('mlp', MLPClassifier())]\n",
    "\n",
    "ensemble_selector = SelectKBest(chi2)\n",
    "ensemble_normalizer = Normalizer()\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    (\"normalizer\",ensemble_normalizer),\n",
    "    (\"selecter\", ensemble_selector),\n",
    "    ('stacking', StackingClassifier(estimators=ensemble_estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "ensemble_params = {\n",
    "    'cv__max_df': [0.75],\n",
    "    \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_library)],\n",
    "    \"normalizer__norm\": ['l2'],\n",
    "    'cv__ngram_range': [(1,1)],\n",
    "    'stacking__mlp__solver':[\"lbfgs\"],\n",
    "    'stacking__mlp__hidden_layer_sizes': [(32,)],\n",
    "    'stacking__mlp__alpha': [0.1],    \n",
    "    'stacking__nb__alpha': [0.5],\n",
    "}\n",
    "\n",
    "ensemble_grid = GridSearchCV(ensemble_pipeline, ensemble_params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "ensemble_grid.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(ensemble_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {ensemble_grid.best_params_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqh5cGxoIQ7z",
    "outputId": "92632da0-9632-4429-e087-9080fbbf94d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'll', 're', 'shouldn', 'site', 'sites', 've', 'wasn', 'web', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.402.\n",
      "The winning parameters are {'cv__binary': False, 'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__preprocessor': <function preprocess_text at 0x7f6cc558ea60>, 'cv__stop_words': ['i', \"i'll\", \"i'd\", \"i'm\", \"i've\", 'ive', 'me', 'myself', 'you', \"you'll\", \"you'd\", \"you're\", \"you've\", 'yourself', 'he', \"he'll\", \"he'd\", \"he's\", 'him', 'she', \"she'll\", \"she'd\", \"she's\", 'her', 'it', \"it'll\", \"it'd\", \"it's\", 'itself', 'oneself', 'we', \"we'll\", \"we'd\", \"we're\", \"we've\", 'us', 'ourselves', 'they', \"they'll\", \"they'd\", \"they're\", \"they've\", 'them', 'themselves', 'everyone', \"everyone's\", 'everybody', \"everybody's\", 'someone', \"someone's\", 'somebody', \"somebody's\", 'nobody', \"nobody's\", 'anyone', \"anyone's\", 'everything', \"everything's\", 'something', \"something's\", 'nothing', \"nothing's\", 'anything', \"anything's\", 'a', 'an', 'the', 'this', 'that', \"that's\", 'these', 'those', 'my', 'your', 'yours', 'his', 'hers', 'its', 'our', 'ours', 'own', 'their', 'theirs', 'few', 'much', 'many', 'lot', 'lots', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'certain', 'other', 'another', 'such', 'several', 'multiple', 'rather', 'quite', 'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'anti', 'around', 'as', 'at', 'away', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'far', 'following', 'for', 'from', 'here', \"here's\", 'in', 'inside', 'into', 'left', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'right', 'since', 'than', 'there', \"there's\", 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without', 'may', 'might', 'will', \"won't\", 'would', \"wouldn't\", 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'should', \"shouldn't\", 'must', \"must've\", 'be', 'being', 'been', 'am', 'are', \"aren't\", \"ain't\", 'is', \"isn't\", 'was', \"wasn't\", 'were', \"weren't\", 'do', 'doing', \"don't\", 'does', \"doesn't\", 'did', \"didn't\", 'done', 'have', \"haven't\", 'having', 'has', \"hasn't\", 'had', \"hadn't\", 'get', 'getting', 'gets', 'got', 'gotten', 'go', 'going', 'gonna', 'goes', 'went', 'gone', 'make', 'making', 'makes', 'made', 'take', 'taking', 'takes', 'took', 'taken', 'need', 'needing', 'needs', 'needed', 'use', 'using', 'uses', 'used', 'want', 'wanna', 'wanting', 'wants', 'let', 'lets', 'letting', \"let's\", 'suppose', 'supposing', 'supposes', 'supposed', 'seem', 'seeming', 'seems', 'seemed', 'say', 'saying', 'says', 'said', 'know', 'knowing', 'knows', 'knew', 'known', 'look', 'looking', 'looked', 'think', 'thinking', 'thinks', 'thought', 'feel', 'feels', 'felt', 'based', 'put', 'puts', 'who', \"who's\", \"who've\", \"who'd\", 'whoever', \"whoever's\", 'whom', 'whomever', \"whomever's\", 'whose', 'whosever', \"whosever's\", 'when', 'whenever', 'which', 'whichever', 'where', \"where's\", \"where'd\", 'wherever', 'why', \"why's\", \"why'd\", 'whyever', 'what', \"what's\", 'whatever', 'whence', 'how', \"how's\", \"how'd\", 'however', 'whether', 'whatsoever', 'and', 'or', 'not', 'because', 'also', 'always', 'never', 'only', 'really', 'very', 'greatly', 'extremely', 'somewhat', 'no', 'nope', 'nah', 'yes', 'yep', 'yeh', 'yeah', 'maybe', 'perhaps', 'more', 'most', 'less', 'least', 'good', 'great', 'well', 'better', 'best', 'bad', 'worse', 'worst', 'too', 'thru', 'though', 'although', 'yet', 'already', 'then', 'even', 'now', 'sometimes', 'still', 'together', 'altogether', 'entirely', 'fully', 'entire', 'whole', 'completely', 'utterly', 'seemingly', 'apparently', 'clearly', 'obviously', 'actually', 'actual', 'usually', 'usual', 'literally', 'honestly', 'absolutely', 'definitely', 'generally', 'totally', 'finally', 'basically', 'essentially', 'fundamentally', 'automatically', 'immediately', 'necessarily', 'primarily', 'normally', 'perfectly', 'constantly', 'particularly', 'eventually', 'hopefully', 'mainly', 'typically', 'specifically', 'differently', 'appropriately', 'plenty', 'certainly', 'unfortunately', 'ultimately', 'unlikely', 'likely', 'potentially', 'fortunately', 'personally', 'directly', 'indirectly', 'nearly', 'closely', 'slightly', 'probably', 'possibly', 'especially', 'frequently', 'often', 'oftentimes', 'seldom', 'rarely', 'sure', 'while', 'whilst', 'able', 'unable', 'else', 'ever', 'once', 'twice', 'thrice', 'almost', 'again', 'instead', 'next', 'previous', 'unless', 'somehow', 'anyhow', 'anywhere', 'somewhere', 'everywhere', 'nowhere', 'further', 'anymore', 'later', 'ago', 'ahead', 'just', 'same', 'different', 'big', 'small', 'little', 'tiny', 'large', 'huge', 'pretty', 'mostly', 'anyway', 'anyways', 'otherwise', 'regardless', 'throughout', 'additionally', 'moreover', 'furthermore', 'meanwhile', 'afterwards', 'thing', \"thing's\", 'things', 'stuff', \"other's\", 'others', \"another's\", 'total', '', 'false', 'none', 'way', 'kind', 'zero', 'zeros', 'zeroes', 'one', 'ones', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'hundreds', 'thousand', 'thousands', 'million', 'millions', 'first', 'last', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eigth', 'ninth', 'tenth', 'firstly', 'secondly', 'thirdly', 'lastly', 'hello', 'hi', 'hey', 'sup', 'yo', 'greetings', 'please', 'okay', 'ok', \"y'all\", 'lol', 'rofl', 'thank', 'thanks', 'alright', 'kinda', 'dont', 'sorry', 'idk', 'tldr', 'tl', 'dr', 'tbh', 'dude', 'tho', 'aka', 'plz', 'pls', 'bit', 'don', 'www', 'https', 'http', 'com', 'etchtml', 'reddit', 'subreddit', 'subreddits', 'comments', 'reply', 'replies', 'thread', 'threads', 'post', 'posts', 'website', 'websites', 'web site', 'web sites'], 'normalizer__norm': 'l2', 'selecter__k': 5000, 'stacking__mlp__alpha': 0.1, 'stacking__mlp__hidden_layer_sizes': (32,), 'stacking__mlp__solver': 'lbfgs', 'stacking__nb__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#new ensemble\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the base estimators for the stacking classifier\n",
    "ensemble_estimators = [('nb', MultinomialNB()), ('mlp', MLPClassifier())]\n",
    "\n",
    "ensemble_selector = SelectKBest(chi2)\n",
    "ensemble_normalizer = Normalizer()\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    (\"normalizer\",ensemble_normalizer),\n",
    "    (\"selecter\", ensemble_selector),\n",
    "    ('stacking', StackingClassifier(estimators=ensemble_estimators))\n",
    "])\n",
    "\n",
    "# Define the grid search parameters\n",
    "ensemble_params = {\n",
    "    'cv__max_df': [1.0],\n",
    "    \"selecter__k\":[5000],\n",
    "    \"cv__stop_words\": [list(stop_words_custom)],\n",
    "    'cv__preprocessor': [preprocess_text],\n",
    "    #    'cv__preprocessor': [preprocess_text,remove_punctuation,None],\n",
    "\n",
    "    \"cv__binary\": [False],\n",
    "    \"normalizer__norm\": ['l2'],\n",
    "    'cv__ngram_range': [(1,1)],\n",
    "    'stacking__mlp__solver':[\"lbfgs\"],\n",
    "    'stacking__mlp__hidden_layer_sizes': [(32,)],\n",
    "    'stacking__mlp__alpha': [0.1],    \n",
    "    'stacking__nb__alpha': [0.1],\n",
    "}\n",
    "\n",
    "ensemble_grid = GridSearchCV(ensemble_pipeline, ensemble_params, cv=5,scoring='accuracy' ,verbose=1, n_jobs=-1)\n",
    "\n",
    "ensemble_grid.fit(train_x, train_y)\n",
    "\n",
    "accuracy = round(ensemble_grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {ensemble_grid.best_params_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynABMtARKfzd",
    "outputId": "da6ff5a1-c0cd-4212-a5fd-808f9842cfa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 95.402.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best accuracy is {accuracy}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcLeXGVCIDyM",
    "outputId": "4d164641-afd2-4942-f376-d66d0bdbe86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "y_pred_new = ensemble_grid.predict(test_x)\n",
    "create_test_csv(y_pred_new,\"Stacking_MultiNB-MLP-05032023_02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvY1Dm14OKwi"
   },
   "outputs": [],
   "source": [
    "######################################################### final"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
