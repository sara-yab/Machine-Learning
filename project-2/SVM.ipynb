{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXEl2ep9cMy6",
    "outputId": "bc354c17-e532-45df-b948-82da7e491d52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from google.colab import drive\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif,mutual_info_classif,f_regression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh4Rncx_cQho",
    "outputId": "ead7f14e-5989-4fe4-db8e-3d78e87a13e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n",
      "shape train: (718, 2)\n",
      "shape test: (279, 2)\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "drive.mount('/content/gdrive/', force_remount=True)\n",
    "\n",
    "train_data_initial = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/train.csv')\n",
    "test_data = pd.read_csv('/content/gdrive/MyDrive/ecse551-mp2/test.csv')\n",
    "\n",
    "print('shape train:',train_data_initial.shape)\n",
    "print('shape test:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "el1eg8tdcTVL"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(df):\n",
    "    random.seed(0)  # Use a fixed seed for the random number generator\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JuT3IHQ_cpoG"
   },
   "outputs": [],
   "source": [
    "#function for creating the test csv file to upload to kaggle\n",
    "def create_test_csv(data, outfile_name):\n",
    "  rawdata= {'subreddit':data}\n",
    "  csv = pd.DataFrame(rawdata, columns = ['subreddit'])\n",
    "  csv.to_csv(outfile_name,index=True, header=True)\n",
    "  print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nZcGyPMzcU1c"
   },
   "outputs": [],
   "source": [
    "#shuffle the data and split the features from the label\n",
    "train_data = shuffle_data(train_data_initial)\n",
    "\n",
    "train_x = train_data[\"body\"]\n",
    "train_y = train_data[\"subreddit\"]\n",
    "test_x = test_data[\"body\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2pKmqofAmZUw"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dK-qvvi_Oa26"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "stop_words_library = stop_words_sklearn.union(stop_words_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUIZVtDXOHoj"
   },
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec3yRFyGOpZo",
    "outputId": "881aee48-9d7a-49ab-c422-007dfae521f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The best accuracy is 90.251.\n",
      "The winning parameters are {'classify__C': 10, 'classify__kernel': 'rbf'}\n",
      "Run time: 15.285731792449951 seconds\n"
     ]
    }
   ],
   "source": [
    "#initial training without removing parameters\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    'classify__C': [0.1, 1, 10],\n",
    "    'classify__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "model = SVC()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),(\"classify\",model)]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4w1BXsuwOJ7X",
    "outputId": "55e91da1-6117-4599-e503-2626959bf7b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "The best accuracy is 92.198.\n",
      "The winning parameters are {'classify__alpha': 0.1, 'selecter__k': 5000, 'vect__binary': False, 'vect__stop_words': ['when', 'few', 'very', 'between', 'nine', 'd', 'elsewhere', 'ourselves', 'wherein', 'several', 'still', 'even', 'seeming', 'an', 'becoming', 'below', 'give', 'nobody', 'behind', 'thru', 'mustn', 'ma', 'about', 'if', 'must', 'toward', 'what', 'on', 'through', 'ever', 'anyhow', 'there', 'fill', 'empty', 'by', 'these', 'co', 'full', 'therefore', \"didn't\", 'won', 'you', 'another', 'within', 'seemed', 'sometimes', 'doesn', 'meanwhile', 'becomes', 'thence', 'fifteen', 'take', 'to', 'will', 'hadn', 'found', 'have', 'four', 'them', 'whereby', 'were', 'theirs', 'be', \"wasn't\", 'six', 'nevertheless', 'formerly', 'are', 'although', 'cry', 'sometime', 'eg', 'further', 'perhaps', 'again', 'under', 'this', 'alone', 'us', 'might', 'see', 'do', 'both', 'against', 'con', 'having', 'since', 'around', 'needn', 'himself', 'system', 'among', 'eleven', 'for', 'former', 'it', 'onto', 'interest', 'anyway', 'hereby', 'out', 'after', 'itself', \"don't\", \"isn't\", 'before', 'besides', \"should've\", 'nothing', 'can', \"you'll\", 'or', 'get', 'anyone', 'move', 'whence', 'mine', 'a', 'nor', 'other', 'per', 'back', 'last', 'everywhere', 'm', 'part', 'own', 'name', 'should', 'y', 'was', 'didn', 'whereupon', 'mightn', 'over', 'haven', 'bottom', \"hadn't\", 'thereafter', 'anything', 'inc', 'above', 'de', 'noone', 'don', \"it's\", 'one', 'from', 'someone', 'during', 'therein', 'everyone', 'well', 'his', 'i', 'being', 'without', 'while', 'o', 'done', 'whatever', 'yet', \"shouldn't\", 'hence', 'go', 'hasn', 'afterwards', 'seems', 'as', \"that'll\", 'may', 'though', 'hereafter', 'however', 'made', 'seem', 'him', 'amoungst', 'somehow', 'mostly', 'whither', 'none', 'then', 'could', 'also', 'how', 't', 'off', 'others', 'ie', 'latter', 'serious', 'describe', 'everything', 'across', 'll', 'yourself', 'front', 'same', 'yours', 'next', 'no', 'else', 'via', 'thin', \"wouldn't\", 'side', 'up', 'every', 'two', 'mill', 'something', 'already', 'together', 'many', 'thus', 'but', 'that', 'rather', 'neither', 'nowhere', 'your', 'its', 'except', 'ten', 'keep', 'show', 'yourselves', 'my', \"couldn't\", 'where', 'much', 'he', 'herein', 'down', 'wherever', 'with', 'due', 'namely', 'please', 'always', 'did', \"you've\", \"shan't\", 'into', 'cant', 'less', 'five', 'had', 'twelve', 'and', 'along', 'almost', \"haven't\", 'most', \"aren't\", 'third', 'some', 'hundred', 'they', 'such', 'been', 're', 'indeed', 'often', 'would', \"mightn't\", 'just', 'me', 'call', 'weren', 'now', 'of', 'throughout', 'thick', 'whenever', 'until', 'cannot', 'least', 'thereupon', 'beside', 'hers', \"doesn't\", 'beyond', 'thereby', 'towards', 'couldn', 'top', 'once', 'whole', 'three', 'couldnt', 'ours', 'has', 'more', 'forty', 'whereafter', 'amongst', 'beforehand', 's', 'became', 'fifty', 'wasn', \"you'd\", 'am', 'twenty', \"needn't\", 'each', 'does', 'in', 'otherwise', 'ain', 'bill', 'become', 'than', 'detail', 'at', 'put', 'themselves', 'because', 'shan', 'latterly', 'sixty', 'our', 'we', 'eight', 'not', 'amount', 'too', 'fire', 'whereas', 'who', 'doing', 'isn', 'whom', 'any', 'whether', \"won't\", 'un', 'etc', 'so', 'her', 'shouldn', \"weren't\", 'myself', 'upon', 'somewhere', 'never', 'which', 'aren', \"you're\", 'why', 'ltd', \"mustn't\", 'hereupon', 'herself', 've', 'whose', 'is', \"hasn't\", 'enough', 'all', 'only', 'those', 'whoever', 'wouldn', 'anywhere', 'hasnt', 'the', 'their', 'sincere', \"she's\", 'here', 'either', 'first', 'find', 'moreover', 'she']}\n",
      "Run time: 74.24770450592041 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing stop words\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False,True],\n",
    "    \"vect__stop_words\": [list(stop_words_nltk),list(stop_words_sklearn),list(stop_words_library)],\n",
    "    \"selecter__k\":[5000,6000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5]\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter),(\"classify\", SVC())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gzydUK4afym",
    "outputId": "e8674960-96d7-4139-de0c-54156cc20537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "The best accuracy is 90.53.\n",
      "The winning parameters are {'classify__alpha': 0.02, 'normalizer__norm': 'l1', 'selecter__k': 5000, 'vect__binary': False, 'vect__stop_words': frozenset({'only', 'whereby', 'thereby', 'within', 'that', 'top', 'bill', 'here', 'ain', 'anyway', 'himself', 'full', 'there', 'nine', 'well', 'couldn', 'would', 'they', \"hadn't\", 'along', 'whether', 'more', 'around', 'an', 'hasnt', 'she', 'never', 'be', 'already', 'de', 'else', 'whose', 'anyone', 'wasn', 'without', 'whole', 'thru', 'even', \"it's\", \"doesn't\", 'none', 'made', 'to', 'not', 'still', 'sometimes', 'my', 'yours', 'from', 'keep', 'who', \"you'd\", 'further', 'his', 'might', 'whoever', 'through', 'formerly', 'describe', 'ltd', 'whereafter', 'whenever', 'being', 'us', 'upon', 'ourselves', 'show', 'against', 'we', 'cannot', 'anyhow', 'doing', 'get', 'has', 'with', 'by', 'just', 'seeming', 'whatever', 'although', 'most', 'when', 'thereafter', 'below', 'and', 'yourself', 'how', 'down', 'back', 'five', 'four', \"you'll\", 'until', 'what', 'while', 'as', 'fill', 'those', \"aren't\", 'but', 'call', 'could', 'their', 'then', 'noone', 'six', 'which', 'ma', 'throughout', 'anything', 'part', 'itself', 'again', 'twelve', \"won't\", 'last', 'whereas', 'up', 'perhaps', 'ours', 'all', \"needn't\", 'been', 'eight', 'etc', 'via', 'amoungst', 'o', 'either', 'least', 'three', \"couldn't\", 'didn', 'beforehand', 'latter', 'thence', 'amongst', 'hereby', 'whither', 'became', 'couldnt', \"wouldn't\", 'onto', 'seem', 'm', 'almost', 'fifty', 'nowhere', 'anywhere', 'therein', \"didn't\", 'under', 'was', 'others', \"haven't\", 'thin', 'behind', 'too', 'done', 'after', 'should', 'third', 'per', 'across', \"you're\", 'becoming', 'its', 'these', 'them', 've', 'enough', 'if', 'seems', 'beyond', 'fire', 'front', 'somehow', 'however', 'everything', 'a', 'empty', 'y', 'together', 'shouldn', 'for', 'hereupon', 'hadn', 'same', 'hence', 'indeed', 'over', 'no', 'doesn', 'have', 'forty', 'ten', 'amount', 'having', 'hasn', 'any', 'off', 'such', 'first', 'themselves', \"don't\", 'bottom', 'rather', \"mustn't\", 'do', 'are', 'can', 'besides', 'somewhere', 'fifteen', \"weren't\", 'since', 'also', 'mill', 'often', 'nobody', 'due', 'wherever', 'did', 'always', 'thereupon', \"hasn't\", 'name', 'therefore', 'un', 'go', 'aren', \"isn't\", 'were', 'out', 'sincere', \"mightn't\", 'thick', 'inc', 'become', 'alone', 'several', 'this', 'he', 'among', 'll', 'detail', 'during', 'mostly', 'you', 'won', 'namely', 'our', 'yourselves', 'in', 'why', 'herein', 'wherein', 'serious', 'both', 'the', 'toward', \"shouldn't\", 'on', 'another', 'because', 'haven', 'needn', 'please', 'next', 'find', 'your', 'moreover', \"should've\", 'though', 's', \"wasn't\", 'nothing', 'less', 'system', 'twenty', 'now', 'about', 'mustn', 'herself', 'hers', 'or', 'every', 'than', 'everywhere', \"you've\", 'latterly', \"she's\", 'afterwards', 'weren', 'above', 'side', 'everyone', 'eg', 'elsewhere', 're', 'hereafter', 'where', 'see', 'very', 'yet', 'myself', 'two', 'former', 'cry', 'towards', 'thus', 'i', 'd', 'ie', 'whence', 'con', 'move', 'mightn', 'am', 'don', 'hundred', 'of', 'whereupon', 'other', 'once', 'me', 'her', 'wouldn', 'otherwise', 'found', 'seemed', 'give', 'becomes', 'it', 'at', 'between', 'something', 'so', 'him', 'into', 'neither', 't', 'put', 'except', 'few', 'beside', 'whom', 'meanwhile', 'nevertheless', 'mine', 'isn', 'does', 'before', 'may', 'ever', 'theirs', 'will', 'eleven', \"that'll\", 'one', \"shan't\", 'take', 'sixty', 'sometime', 'each', 'had', 'interest', 'own', 'is', 'much', 'shan', 'cant', 'nor', 'co', 'many', 'must', 'some', 'someone'})}\n",
      "Run time: 26.246994018554688 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing normalizer\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False,True],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"selecter__k\":[5000,3000],\n",
    "    \"classify__alpha\" : [0.001, 0.01, 0.1,0.02,0.5],\n",
    "    \"normalizer__norm\": ['l2','l1','max']\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter),(\"normalizer\",normalizer),(\"classify\", SVC())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Yv5fjkCyba1G"
   },
   "outputs": [],
   "source": [
    "#stem lemmatizer \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer_Pos:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
    "\n",
    "class LemmaTokenizer_word:\n",
    "     def __init__(self):\n",
    "       self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) ]\n",
    "\n",
    "\n",
    "class StemTokenizer:\n",
    "     def __init__(self):\n",
    "       self.wnl =PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiMh3bszbewm",
    "outputId": "216e1f58-d50f-4d12-e056-bfd1de47ba5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'make', \"n't\", 'need', 'sha', 'win', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is 89.833.\n",
      "The winning parameters are {'classify__alpha': 0.1, 'selecter__k': 5000, 'vect__binary': False, 'vect__stop_words': frozenset({'only', 'whereby', 'thereby', 'within', 'that', 'top', 'bill', 'here', 'ain', 'anyway', 'himself', 'full', 'there', 'nine', 'well', 'couldn', 'would', 'they', \"hadn't\", 'along', 'whether', 'more', 'around', 'an', 'hasnt', 'she', 'never', 'be', 'already', 'de', 'else', 'whose', 'anyone', 'wasn', 'without', 'whole', 'thru', 'even', \"it's\", \"doesn't\", 'none', 'made', 'to', 'not', 'still', 'sometimes', 'my', 'yours', 'from', 'keep', 'who', \"you'd\", 'further', 'his', 'might', 'whoever', 'through', 'formerly', 'describe', 'ltd', 'whereafter', 'whenever', 'being', 'us', 'upon', 'ourselves', 'show', 'against', 'we', 'cannot', 'anyhow', 'doing', 'get', 'has', 'with', 'by', 'just', 'seeming', 'whatever', 'although', 'most', 'when', 'thereafter', 'below', 'and', 'yourself', 'how', 'down', 'back', 'five', 'four', \"you'll\", 'until', 'what', 'while', 'as', 'fill', 'those', \"aren't\", 'but', 'call', 'could', 'their', 'then', 'noone', 'six', 'which', 'ma', 'throughout', 'anything', 'part', 'itself', 'again', 'twelve', \"won't\", 'last', 'whereas', 'up', 'perhaps', 'ours', 'all', \"needn't\", 'been', 'eight', 'etc', 'via', 'amoungst', 'o', 'either', 'least', 'three', \"couldn't\", 'didn', 'beforehand', 'latter', 'thence', 'amongst', 'hereby', 'whither', 'became', 'couldnt', \"wouldn't\", 'onto', 'seem', 'm', 'almost', 'fifty', 'nowhere', 'anywhere', 'therein', \"didn't\", 'under', 'was', 'others', \"haven't\", 'thin', 'behind', 'too', 'done', 'after', 'should', 'third', 'per', 'across', \"you're\", 'becoming', 'its', 'these', 'them', 've', 'enough', 'if', 'seems', 'beyond', 'fire', 'front', 'somehow', 'however', 'everything', 'a', 'empty', 'y', 'together', 'shouldn', 'for', 'hereupon', 'hadn', 'same', 'hence', 'indeed', 'over', 'no', 'doesn', 'have', 'forty', 'ten', 'amount', 'having', 'hasn', 'any', 'off', 'such', 'first', 'themselves', \"don't\", 'bottom', 'rather', \"mustn't\", 'do', 'are', 'can', 'besides', 'somewhere', 'fifteen', \"weren't\", 'since', 'also', 'mill', 'often', 'nobody', 'due', 'wherever', 'did', 'always', 'thereupon', \"hasn't\", 'name', 'therefore', 'un', 'go', 'aren', \"isn't\", 'were', 'out', 'sincere', \"mightn't\", 'thick', 'inc', 'become', 'alone', 'several', 'this', 'he', 'among', 'll', 'detail', 'during', 'mostly', 'you', 'won', 'namely', 'our', 'yourselves', 'in', 'why', 'herein', 'wherein', 'serious', 'both', 'the', 'toward', \"shouldn't\", 'on', 'another', 'because', 'haven', 'needn', 'please', 'next', 'find', 'your', 'moreover', \"should've\", 'though', 's', \"wasn't\", 'nothing', 'less', 'system', 'twenty', 'now', 'about', 'mustn', 'herself', 'hers', 'or', 'every', 'than', 'everywhere', \"you've\", 'latterly', \"she's\", 'afterwards', 'weren', 'above', 'side', 'everyone', 'eg', 'elsewhere', 're', 'hereafter', 'where', 'see', 'very', 'yet', 'myself', 'two', 'former', 'cry', 'towards', 'thus', 'i', 'd', 'ie', 'whence', 'con', 'move', 'mightn', 'am', 'don', 'hundred', 'of', 'whereupon', 'other', 'once', 'me', 'her', 'wouldn', 'otherwise', 'found', 'seemed', 'give', 'becomes', 'it', 'at', 'between', 'something', 'so', 'him', 'into', 'neither', 't', 'put', 'except', 'few', 'beside', 'whom', 'meanwhile', 'nevertheless', 'mine', 'isn', 'does', 'before', 'may', 'ever', 'theirs', 'will', 'eleven', \"that'll\", 'one', \"shan't\", 'take', 'sixty', 'sometime', 'each', 'had', 'interest', 'own', 'is', 'much', 'shan', 'cant', 'nor', 'co', 'many', 'must', 'some', 'someone'}), 'vect__tokenizer': <__main__.LemmaTokenizer_word object at 0x7f4b6a36c940>}\n",
      "Run time: 74.08984041213989 seconds\n"
     ]
    }
   ],
   "source": [
    "#testing lemma\n",
    "t_start = time.time()\n",
    "\n",
    "pipe_params = {\n",
    "    \"vect__binary\": [False,True],\n",
    "    \"vect__stop_words\": [list(stop_words_library)],\n",
    "    \"vect__tokenizer\": [LemmaTokenizer_word()],\n",
    "    \"selecter__k\":[5000,3000]\n",
    "    }\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "selecter = SelectKBest(chi2)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer), (\"selecter\", selecter),(\"normalizer\",normalizer),(\"classify\", SVC())]\n",
    ")\n",
    "\n",
    "grid = model_selection.GridSearchCV(pipe, pipe_params, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "elapsed_time = t_end-t_start\n",
    "accuracy = round(grid.best_score_ * 100,3)\n",
    "\n",
    "print(f\"The best accuracy is {accuracy}.\")\n",
    "print(f\"The winning parameters are {grid.best_params_}\")\n",
    "print(f\"Run time: {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3TjAAZcppJT"
   },
   "outputs": [],
   "source": [
    "# Step 5: Make predictions on test data using the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvY1Dm14OKwi"
   },
   "outputs": [],
   "source": [
    "######################################################### final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYybx8kZcVQT",
    "outputId": "b593fb20-e4b2-4fab-db45-b82b2e686cee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score-> 0.8551379176379175\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorize', CountVectorizer(stop_words=list(stop_words), binary=True,lowercase = False,preprocessor=preprocess_text)),\n",
    "    ('selector', SelectKBest(chi2, k=3000)),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "cross_val_score = np.mean(model_selection.cross_val_score(pipeline, train_x, train_y, cv=5,n_jobs=-1,verbose=1))\n",
    "print('cross_val_score->',cross_val_score)\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "\n",
    "test_x_processed = pipeline.named_steps['vectorize'].transform(test_x)\n",
    "test_x_selected = pipeline.named_steps['selector'].transform(test_x_processed)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(test_x)\n",
    "\n",
    "create_test_csv(y_pred,\"SVM.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7VWdFEzwwm8",
    "outputId": "54894895-1ab9-43cb-f050-18bc71dacd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
